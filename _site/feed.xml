<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>newbiettn</title>
    <description>Standing on the shoulders of giants</description>
    <link>http://127.0.0.1:4000</link>
    <atom:link href="http://127.0.0.1:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Perceptron</title>
        <description>&lt;p&gt;This is a perceptron&lt;/p&gt;
</description>
        <pubDate>Thu, 01 Dec 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/12/01/perceptron/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/12/01/perceptron/</guid>
      </item>
    
      <item>
        <title>Unconstrained optimization methods</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent&quot;&gt;1. Gradient descent&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;1.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;1.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#newtons-method&quot;&gt;2. Newton’s method&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;2.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#minimization-and-maximization-problem&quot;&gt;2.2. Minimization and maximization problem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;2.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gauss-newton-method&quot;&gt;3. Gauss-Newton method&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;3.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;3.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;In this post, we are going to review classical unconstrained optimization algorithms, including gradient descent, Newton’s and Gauss-Newton. Those three algorithms have been heavily used as optimization methods of different machine learning algorithms, such as linear regression, logistic regression, neural network, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; All R codes of experiments can be found &lt;a href=&quot;https://github.com/newbiettn/R/tree/master/optimization_algorithms&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;1. Gradient descent&lt;/h3&gt;

&lt;h4 id=&quot;concept&quot;&gt;1.1. Concept&lt;/h4&gt;

&lt;p&gt;Gradient descent (or steepest descent) is the most intuitively simple among three algorithms. The underlying mechanism of the algorithm is the concept of the gradient $\nabla$ of the function $f$. Intuitively speaking, $\nabla f$ indicates the direction by which if we traverse along the graph curve of $f$, we are able to find the increasing function values (e.g., $f(x_{n+1}) \gt f(x)$). Conversely, if we traverse the opposite direction of $\nabla f$, we can obtain decreasing values of $f$ such that $f(x_{n+1}) \lt f(x)$. And that’s how gradient descent works!&lt;/p&gt;

&lt;p&gt;Mathematically speaking, given a differential function $f(x)$, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{n+1} = x_n - \eta \nabla f(x)&lt;/script&gt;

&lt;p&gt;where $\eta$ ($0 \lt \eta &amp;lt; 1$) is &lt;strong&gt;the learning rate&lt;/strong&gt;, $\nabla f(x)$ is the gradient of $f$ at $x_n$.&lt;/p&gt;

&lt;p&gt;then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{n+1}) \geq f(x_n)&lt;/script&gt;

&lt;p&gt;This process stops either after $m$ predefined iterations or until $f(x_{n+1}) \approx f(x_n)$.&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;1.2. Experiments&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      

Consider $f(x)$ as a quadratic function. Suppose we have $f(x) = ax^2 + bx + c$ with $a = 1, b = 2, c = 3$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.


  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;At the beginning, we blindly set $x_0 = 45$, and sequentially change $\eta = (0.1, 0.3, 0.5, 0.7)$ to observe how the learning rate affects the convergence of the algorithm. Look at Figure 1, at $\eta = 0.1$ (the top left), gradient descent can find the minima of $f(x)$ after roughly 10 iterations. However, from the figure, we can also see that the rate of convergence of the algorithm significantly reduces as soon as it is close to the minima. When $\eta = 0.3$ (the top right), we need roughly 5 iterations, and it seems like at $\eta = 0.5$ (the bottom left), we only need 2 iterations, which is really computationally effective. The worst case is when $\eta = 0.7$ (the bottom right), we “jump” over the minima several times before we approach it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/bfb06b4f890c538b0e5da98474904a61.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc_quad_func.gif&quot; alt=&quot;Fig 1. Find the local  minima of a quadratic function&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. Find the local  minima of a quadratic function
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 2:&lt;/p&gt;
      

Consider $f(x)$ as a polynomial function. Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.


  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This example demonstrates that &lt;strong&gt;gradient descent cannot guarantee finding global minima of the function&lt;/strong&gt; although we keep changing $\eta$. The worst case happens when $\eta = 0.02$ (the top right), gradient descent can only find the sub minimal value of the function.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/3a3ad725cf87f03d1cca10c39a6cf77d.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc_poly_func.gif&quot; alt=&quot;Fig 2. Find the local minima of a polynomial function&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 2. Find the local minima of a polynomial function
  &lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;newtons-method&quot;&gt;2. Newton’s method&lt;/h3&gt;

&lt;h4 id=&quot;concept-1&quot;&gt;2.1. Concept&lt;/h4&gt;

&lt;p&gt;In my opinion, Newton’s method (or Newton-Raphson method) can be intuitively comprehended as &lt;strong&gt;a method to approximate an approximation&lt;/strong&gt;. It means that if we already have a reasonably good approximation of a function $f$ at a certain value $x_0$, we still can have a better approximation by estimating an approximation of the approximation itself.&lt;/p&gt;

&lt;p&gt;I found &lt;a href=&quot;http://tutorial.math.lamar.edu/Classes/CalcI/NewtonsMethod.aspx&quot;&gt;this page&lt;/a&gt; has a brilliant explanation for the algorithm and thus I will reiterate that explanation by my own words.&lt;/p&gt;

&lt;p&gt;Suppose we have a function $f(x) \in \mathbb{R^2}$, and we want to find the value of $f$ at a certain value $x_0$. One can say that we can simply plug $x_0$ into $f$ to get the searching value. However, there exist many functions, such as $f = \sqrt{x}$, where we cannot straightforwardly get the value. One can also say that computers can do that. But unlike simple equation like $f(x) = 2x$, where we can compute manually, we &lt;strong&gt;do not have any exact expression to describe that root function&lt;/strong&gt;. In fact, computers use approximation methods behind the scene (like Newton’s method) to compute root value of numbers.&lt;/p&gt;

&lt;p&gt;Suppose we want to find an approximation of $f(x^*) = 0$. We do that as follows.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/newton_exp.gif&quot; alt=&quot;Fig 3. Using 2nd tangent line to have a better approximation&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 3. Using 2nd tangent line to have a better approximation
  &lt;/p&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;First, we construct &lt;strong&gt;a tangent line&lt;/strong&gt; (the blue line) at a point $x_0$ where $x_0$ is a guessing point and is reasonably close to the target point where $y = 0$. We can construct the tangent line by using &lt;strong&gt;slope-point form&lt;/strong&gt;. We can see that $x_1$ is a better approximation than $x_0$. However, we are still able to get a better one.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y - f(x_0) = f&#39;(x_0)(x-x_0)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Second, we construct a second tangent line at $x_1$ (the green line). Again, by using the slope-point form of the tangent line, we have,&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 = f(x_0) + f&#39;(x_0)(x_1 - x_0)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \Longleftrightarrow x_1 - x_0 = - \frac{f(x_0)}{f&#39;{x_0}} \\
&amp; \Longleftrightarrow x_1 = x_0 - \frac{f(x_0)}{f&#39;{x_0}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;As a result, we can find a closer point $x_1$ by using the formula $x_1 = x_0 - \frac{f(x_0)}{f’{x_0}}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;minimization-and-maximization-problem&quot;&gt;2.2. Minimization and maximization problem&lt;/h4&gt;
&lt;p&gt;Similar to gradient descent, Newton’s method can be used to find optimize the function by setting the derivative to 0 like we explained above.&lt;/p&gt;

&lt;h4 id=&quot;experiments-1&quot;&gt;2.2. Experiments&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      
Consider $f(x)$ as a polynomial function. Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this example, we are going to find the local minima of $f(x)$ by using both gradient descent and Newton’s method. We can see that the rate of convergence of Newton’s method is higher than gradient descent. This happens due to the fact the Newton’s method makes use of the 2nd-order derivative of $f$.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/76430b8d6e124f994f56f47d3116b921.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc__and_newton_poly_func.gif&quot; alt=&quot;Fig 4. Find the local minima of a polynomial function using both gradient descent and Newton&#39;s method&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 4. Find the local minima of a polynomial function using both gradient descent and Newton&#39;s method
  &lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;gauss-newton-method&quot;&gt;3. Gauss-Newton method&lt;/h3&gt;

&lt;h4 id=&quot;concept-2&quot;&gt;3.1. Concept&lt;/h4&gt;

&lt;p&gt;Unlike gradient descent and Newton method, Gauss-Newton method can only be used to minimize the sum of squares, such as $C = \frac{1}{2} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$ where $\hat{y}$ is the predicted output, $y$ is the actual output and $n$ is the total number of observations in the dataset.&lt;/p&gt;

&lt;p&gt;Let’s set $e = (\hat{y}_i - y_i)$ , and suppose &lt;strong&gt;${\theta}_j$ is the coefficient vector of the hypothesis function &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x)&lt;/script&gt; in the $j^{th}$ iteration&lt;/strong&gt;. For the &lt;script type=&quot;math/tex&quot;&gt;(j+1)^{th}&lt;/script&gt; iteration, the updated error term &lt;script type=&quot;math/tex&quot;&gt;e_{j+1}&lt;/script&gt; would be,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_{j+1} = e_j + \begin{bmatrix}\frac{\partial e}{\partial \theta}\end{bmatrix}_{\theta = \theta_j}(\theta_{j+1} - \theta_j) \qquad (I)&lt;/script&gt;

&lt;p&gt;We can make &lt;strong&gt;(I)&lt;/strong&gt; more beautiful by employing the concept of &lt;strong&gt;Jacobian matrix&lt;/strong&gt; $J$ such that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
J = \begin{bmatrix}\frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_2} &amp; \dots &amp; \frac{\partial f}{\partial x_n} \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, we have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(I) \Longleftrightarrow e_{j+1} = e_j + J(\theta_{j+1} - \theta_j) \qquad (II)&lt;/script&gt;

&lt;p&gt;Our goal is finding $\theta_{j+1}$ such that &lt;strong&gt;the error term &lt;script type=&quot;math/tex&quot;&gt;e_{j+1}&lt;/script&gt; is minimal&lt;/strong&gt;. Mathematically, we have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j+1} = \underset{\theta}argmin(\frac{1}{2}\lVert e_{j+1} \rVert_2^2) \qquad (III)&lt;/script&gt;

&lt;p&gt;Remember that &lt;script type=&quot;math/tex&quot;&gt;\lVert v \rVert_2&lt;/script&gt; is the &lt;em&gt;$l_2$-norm&lt;/em&gt; or Euclidiean norm of the vector $v$.&lt;/p&gt;

&lt;p&gt;We now need to elaborate &lt;script type=&quot;math/tex&quot;&gt;\lVert e_{j+1} \rVert^2&lt;/script&gt;. First, remember that the norm of a vector $v$ can be defined as &lt;script type=&quot;math/tex&quot;&gt;\lVert v \rVert^2 = v^Tv&lt;/script&gt;. Second, from the equation &lt;strong&gt;(II)&lt;/strong&gt;, suppose that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; A = e_j \\
&amp; B = J(\theta_{j+1} - \theta_j) \\
&amp; C = e_{j+1}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, &lt;script type=&quot;math/tex&quot;&gt;(II) \Longleftrightarrow C = A + B&lt;/script&gt;. From that, we also have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\lVert C \rVert^2 &amp; = C^TC \\
&amp; = (A + B)^T(A+B) \\
&amp; = (A^T + B^T)(A+B) \\
&amp; = A^TA + A^TB + B^TA + B^TB
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because &lt;script type=&quot;math/tex&quot;&gt;A^TB = \sum a_jb_j = \sum b_ja_j = B^TA&lt;/script&gt;, we can rewrite,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \lVert A \rVert^2 + 2A^TB + \lVert B \rVert^2 \qquad (IV)&lt;/script&gt;

&lt;p&gt;From &lt;strong&gt;(II)&lt;/strong&gt; and &lt;strong&gt;(IV)&lt;/strong&gt;, we can have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert e_{j+1} \rVert^2 = \lVert e_{j} \rVert^2 + 2e^TJ(\theta_{j+1} - \theta_j) + (\theta_{j+1} - \theta_j)^TJ^TJ(\theta_{j+1} - \theta_j) \qquad (V)&lt;/script&gt;

&lt;p&gt;Now, we are going to minimize &lt;strong&gt;(V)&lt;/strong&gt;. First, we need to take the derivative of &lt;strong&gt;(V)&lt;/strong&gt; w.r.t. $\theta$ and set it equal to 0, and then find $\theta$. We have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \frac{\partial V}{\partial \theta} = J^Te + J^TJ(\theta_{j+1} - \theta_j) = 0 \\
\Longleftrightarrow &amp; J^TJ(\theta_{j+1} - \theta_j) = -J^Te \\
\Longleftrightarrow &amp; \theta_{j+1} - \theta_j = -(J^TJ)^{-1}J^Te \\
\Longleftrightarrow &amp; \theta_{j+1} = \theta - (J^TJ)^{-1}J^Te \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The formula &lt;script type=&quot;math/tex&quot;&gt;\theta_{j+1} = \theta - (J^TJ)^{-1}J^Te&lt;/script&gt; is &lt;strong&gt;the pure form of Gauss-Newton method&lt;/strong&gt;. By using this equation, we can update the coefficient &lt;script type=&quot;math/tex&quot;&gt;\theta_{j+1}&lt;/script&gt;. To do that, all we need to do is to find the Jacobian matrix. Note that the Gass-Newton method always converge because the Gauss-Newton methods assumes that &lt;script type=&quot;math/tex&quot;&gt;J^TJ&lt;/script&gt; is &lt;strong&gt;non-singular matrix&lt;/strong&gt;, making it possible to find the inverse matrix.&lt;/p&gt;

&lt;h4 id=&quot;experiments-2&quot;&gt;3.2. Experiments&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      
Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We first will use $f(x)$ to generate sample data points and second, we will use Gauss-Newton method to fit a model to the generated data points.

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In Figure 5, we can see that at the beginning, we naively set $\theta =$ [1 1 1 1 1 1 1 1]. In the second iteration, $\theta$ is updated by Gauss-Newton method, allowing the model to fit to the generated data points.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/969cf66470de9ce193e51992f4b691e2.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/gauss_newton_poly_func.gif&quot; alt=&quot;Fig 5. Use Gauss-Newton method to fit generated data points of $f(x)$&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 5. Use Gauss-Newton method to fit generated data points of $f(x)$
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 2:&lt;/p&gt;
      
Using the dataset Speed and Stopping Distances of Cars, we will find a model that find the relationship between speed and distance

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/d04f39453c563c86a1fe2ca3ce68be95.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/gauss_newton_mtcars.gif&quot; alt=&quot;Fig 6. Use Gauss-Newton method to fit a curve line to the dataset&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 6. Use Gauss-Newton method to fit a curve line to the dataset
  &lt;/p&gt;
&lt;/div&gt;

</description>
        <pubDate>Wed, 23 Nov 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/11/23/uncontrained-optimization-methods/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/11/23/uncontrained-optimization-methods/</guid>
      </item>
    
      <item>
        <title>Classification using Rough Set theory</title>
        <description>&lt;h4 id=&quot;i-introduction&quot;&gt;I. Introduction&lt;/h4&gt;
&lt;p&gt;Rough Set (RS) algorithm targets the &lt;strong&gt;vagueness&lt;/strong&gt; of the knowledge, e.g., the boundary between observations is not strong enough to set them apart. Look at Figure 1, we can see that we have no clue how to correctly classify items $x_3$ and $x_4$ when the attribute values of those items are identical.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 1: it is impractical to classify x3 and x4&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1: it is impractical to classify x3 and x4
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ii-primary-concepts-of-rs&quot;&gt;II. Primary concepts of RS&lt;/h4&gt;
&lt;p&gt;RS theory defines a number of concepts:&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/headache-muscle-temp-dataset.png&quot; alt=&quot;Figure 2: Sample dataset&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2: Sample dataset
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;information-system-i--u-a&quot;&gt;1) Information system: $I = (U, A)$&lt;/h4&gt;
&lt;p&gt;Information system can be understood as a body of information/knowledge that we have. Mathematically the information system is denoted as a pair $(U, A)$ where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$U = {x_1, x_2, …, x_n}$: universally represents a non-empty $(U \neq \emptyset)$, finite set of observations $x_i$.&lt;/li&gt;
  &lt;li&gt;$A = {a_1, a_2, …, a_n}$: represent a non-empty $(A \neq \emptyset)$, finite set of attributes $a_j$ such that $a \in A$, $a: U \rightarrow V_a$, where $V_a$ is the value set of $a$ (which means every observation in $U$ can be set to attributes that belong to $V_a$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In ML, we usually call $x_i$ observations, objects, rows; and $a_j$ predictors, features, attributes.&lt;/p&gt;

&lt;h4 id=&quot;decision-system-ds--u-a-cup-d--ds-subseteq-i&quot;&gt;2) Decision system: $DS = (U, A \cup {d})$ | $DS \subseteq I$&lt;/h4&gt;
&lt;p&gt;Decision system $DS$ is a special type of information system $I$ that can be used in classification. $d$ is the &lt;em&gt;decision&lt;/em&gt; values (i.e., Yes/No, 1/0, and so on). In ML, we call $d_k \in {d}$ response, or outcome.&lt;/p&gt;

&lt;h4 id=&quot;indiscernibility-relation&quot;&gt;3) Indiscernibility relation&lt;/h4&gt;
&lt;p&gt;Given an information system $I = (U, A)$, for any subset $B \subseteq A$, the indiscernibility or equivalence relation is defined by:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{(x, y) \in U^2 | \forall a \in B, a(x) = a(y)\}&lt;/script&gt;
where $IND(B)$ is called the &lt;strong&gt;B-indiscernibility relation&lt;/strong&gt;. The equivalence classes of $IND(B)$ are denoted as $[x]_B$.&lt;/p&gt;

&lt;p&gt;Take an example from Figure 2. Suppose we have $B= { Headache, Musclepain }$. Thus, we can have:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{\{e1, e2, e3\}, \{e4, e6\}, \{e5\}\}&lt;/script&gt;
We can also say that $e1, e2, e3$ are indiscernible from each other.&lt;/p&gt;

&lt;h4 id=&quot;approximation&quot;&gt;4) Approximation&lt;/h4&gt;
&lt;p&gt;The concept of approximation regards the problem of &lt;strong&gt;data reduction&lt;/strong&gt; in ML. For that, we can define a approximation set $X (X \subseteq U)$ by using the information from the set $B (B \subseteq A)$. To do that, we have to construct $B-lower$ $(\underline{B}X)$ and $B-upper$ $(\overline{B}X)$ approximations of $X$:
&lt;script type=&quot;math/tex&quot;&gt;\underline{B}X = \{x | [x]_B \subseteq X\}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\overline{B}X = \{x | [x]_B \cap X \neq \emptyset\}&lt;/script&gt;
Whereas &lt;em&gt;lower-approximation&lt;/em&gt; consists of all observations which &lt;strong&gt;certainly&lt;/strong&gt; belong to the class, &lt;em&gt;upper-approximation&lt;/em&gt; contains all observations which &lt;strong&gt;likely&lt;/strong&gt; belong to the class.&lt;/p&gt;

&lt;p&gt;From that we have $B-boundary$:
&lt;script type=&quot;math/tex&quot;&gt;BN_B(X) = \overline{B}X - \underline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and $B-outside$:
&lt;script type=&quot;math/tex&quot;&gt;BO_B = U - \overline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 3&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 3
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For example, in Figure 3, let
&lt;script type=&quot;math/tex&quot;&gt;W = \{x | Walk(x) = yes\}&lt;/script&gt;
we can have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\underline{B}W = {x1, x6}$&lt;/li&gt;
  &lt;li&gt;$\overline{B}W = {x1, x3, x4, x6}$&lt;/li&gt;
  &lt;li&gt;$BN_B(W) = {x3, x4}$&lt;/li&gt;
  &lt;li&gt;$BO_B = {x2, x5, x7}$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 06 Sep 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</guid>
      </item>
    
      <item>
        <title>Precision, Recall, Sensitivity and Specificity</title>
        <description>&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;I recently discovered that the concepts of precision, recall, sensitivity and specificity are very much important to what I am currently working on. Those are equally important but not for a single domain. Instead we have different indicators for different domains. For example, in information retrieval, precision and recall are two important measures that need to be taken into account when evaluate a search system. In pattern recognition and machine learning, precision and recall are important as well but there is a slight difference when interpret them. On the other hand, sensitivity and specificity are alternations to precision and recall when it comes to medical application. In this writing, we aim to explain why there exist those differences in the first place.&lt;/p&gt;

&lt;p&gt;In order to begin, first we look at &lt;strong&gt;the confusion matrix&lt;/strong&gt; to understand where terms TP, FP, TN, and FN come from, and second, look at the Venn digram that perfectly portraits the territories of TP, FP, TN, and FN within the search space.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/confusion-matrix-noted.jpg&quot; alt=&quot;The confusion matrix of the search space&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The confusion matrix of the search space
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/precision-recall.jpg&quot; alt=&quot;The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;And mathematically, precision, recall, sensitivity and specificity are denoted as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{TP}{TP + FP} \\
\\
Recall = Sensitivity = \frac{TP}{TP + FN} \\
\\
Specificity = \frac{TN}{TN + FP}&lt;/script&gt;

&lt;h4 id=&quot;precision&quot;&gt;Precision&lt;/h4&gt;
&lt;p&gt;I found that we can easily understand the intuitive concept of precision by simply looking at the Venn diagram. Simply speaking, precision is the ratio between the documents that match the user expectation and the total number of documents returned by the system. One cay say that the higher the precision, the better. But it is always true because a system can &lt;em&gt;cheat&lt;/em&gt; the precision score up to 100% by only returning documents about which it is extremely confident. By doing this, the amount of returned documents is severely reduced, making it so resistant to FP.Look at the figure below, because the space of returned documents is so small, it can fall within the space of the expected documents easily. What is the problem with that? Because the amount of search result is extremely small, the information is likely inadequate to support the user tasks. Mathematically, we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt;, thus &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-precision-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;recall-and-sensitivity&quot;&gt;Recall and Sensitivity&lt;/h4&gt;
&lt;p&gt;We can also grasp the concept of recall and sensitivity smoothly through the Venn diagram observation. Both recall and sensitivity can be understand as the ratio between the documents that match the user expectation and the total number of expected documents from the user. Intuitively speaking, recall and sensitivity are big indicators for how much good information that a system really misses, whereas precision is an indicator for how much good information in the search result the user can use. We can also cheat the recall/sensitivity score by getting the system to add up the number of returned documents, making the search result overwhelmingly abundant. Then guess who is going have a hard time to filtrate such load of information. Look at the figure below, we can see that by returning an overwhelming amount of information, the system can ensure that its recall/sensitivity is maximized. Mathematically, by doing this we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FN \rightarrow 0&lt;/script&gt;, therefore &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FN} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-recall-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;specificity&quot;&gt;Specificity&lt;/h4&gt;
&lt;p&gt;To understand the concept of specificity, as oppose to observing the Venn diagram, I found it much more easier to understand by checking the confusion matrix. First of all, one has to understand that by using specificity, he wants to steer the focus of his evaluation towards how good the system can predict negative outcomes( or how good the system can avoid returning irrelevant documents,. That’s the reason why TN is the numerator of the equation in the first place. One can also maximize the specificity score of the system by getting the system return no information at all (or predict all observations are negative). Mathematically, when return no information or predict all are negative, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;TN \rightarrow 1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\frac{FP}{TN + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;which-one-i-should-use&quot;&gt;Which one I should use?&lt;/h4&gt;
&lt;p&gt;We have four metrics to choose, so the question is which one I should. Well I can say that it depends on the problem domain. For example, if we have built a search system, in order to evaluate it, firstly we have to answer some questions: Which criteria we care about most? Do we care about the vast amount of information we ignore (which is TN)? Or do we care about the information we can return (FP + TP + FN)? My answer is I care about the relevant of information my system gives the user rather than the information I can’t return (there is countless amount of information, who cares in the first place). Thus, to evaluate the search system, I would use precision, recall and ignore specificity.&lt;/p&gt;

&lt;p&gt;In contrast, suppose we have  a diabetes predictive model to evaluate, which metrics should we care about? Do we care if a patient doesn’t have diabetes, but the system predicts positive (TN + FP)? Do we care about if a patient has diabetes, but the system predicts negative (TP + FP)? and so on. My answer is I would use all of three indicators (e.g., precision, recall, specificity) because I guess we have to take into account all of indicators TP, FP, TN and FN. Everything is crucial in this case.&lt;/p&gt;

&lt;p&gt;Lastly and interestingly, imagine if we had a system that can exploit all evidences and judge if a suspect guilty. In this case, what we care about most? Do we care if the suspect doesn’t commit the crime but the system judges him guilty (FP)? Or do we care if the suspect does commit the crime but the system judges him not guilty (FN)? Yeah, I guess in this case, we should put more emphasis on FP rather than FN, because putting an innocent person to the jail is also a crime, and letting a guilty suspect go is not as dire as putting innocent person to jail.&lt;/p&gt;

&lt;p&gt;** In statistic and relevant fields, usually we put more emphasis on Type I error (e.g., reject &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; incorrectly (FP)), rather than Type 2 error (e.g., retain a false &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; (FN)).&lt;/p&gt;

&lt;p&gt;The end.&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Aug 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</guid>
      </item>
    
      <item>
        <title>Likelihood ratio test</title>
        <description>&lt;p&gt;The likelihood-ratio test is an alternative to $\chi^2$ test in order to assess the **goodness of fit**. It is straight-forward and easy to use.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Calculate $G = -2ln\bigg[\frac{likeli \, hood \, of \, the \, replacing \, model \, h_\theta^{(1)}}{likelihood \, of \, the \, existing \, model \, h_\theta^{(0)}}\bigg] = 2[loglikelihood \, of \, h_\theta^{(0)} – loglikelihood \, of \, h_\theta^{(1)}]$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– Notice that $ln$ is removed by performing logarithm.&lt;/p&gt;

&lt;p&gt;– The log-likelihood is usually outputted by R-language.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Because the distribution model of likelihood ratio test is nearly the same as $\chi^2$, it allows us to assign $G$-score to $\chi^2$-score.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– Mathematically we have $G$-score = $\chi^2$-score&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Find $p$ value based on $\chi^2$ statistic obtained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Determine if $h_\theta^{(1)}$ is more statistically significant than $h_\theta^{(0)}$ based on obtained $p$ value and the statistical significance level $\alpha$ (usually $\alpha = 0.05$)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;example&quot;&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/h3&gt;

&lt;p&gt;Suppose we try to fit a given dataset with two alternative models $h_\theta^{(1)}$ and $h_\theta^{(0)}$ where in $h_\theta^{(1)}$ we eliminated some predictors we reckon that they are not sensitive enough. Now we try to assess if the model $h_\theta^{(1)}$ is more statistically significant than the original model by using the likelihood ratio test.&lt;/p&gt;

&lt;p&gt;Suppose $df = 1$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Log-likelihood of&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– $h_\theta^{(1)}$ is 53.6&lt;/p&gt;

&lt;p&gt;– $h_\theta^{(0)}$ is 68.3&lt;/p&gt;

&lt;p&gt;Therefore, we have $G = 2[68.3 – 53.6] = 29.31$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\chi^2 = G = 29.31$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find $p = P[\chi^2(df = 1) = 29.31]$ by using R&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pchisq(29.31, 1, lower.tail = FALSE)
[1] 6.167658e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Because $p$-value « $\alpha = 0.05$, we can reject the null hypothesis $H_0$ that two models are not significant different, and accept the alternative hypothesis $H_a$ that $h_\theta^{(1)}$ fits better than $h_\theta^{(0)}$.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 09 Aug 2016 21:34:05 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/08/09/likelihood-ratio-test/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/08/09/likelihood-ratio-test/</guid>
      </item>
    
      <item>
        <title>Maximum Likelihood Estimation, Maximum a Posteriori Estimation and Naive Bayes (part 2)</title>
        <description>&lt;h3 id=&quot;maximum-a-posteriori-estimation-map&quot;&gt;Maximum a Posteriori Estimation (MAP)&lt;/h3&gt;

&lt;p&gt;Generally, MAP is a particular extension of MLE where we take in account a biased assumption of data, called &lt;strong&gt;&lt;em&gt;prior knowledge&lt;/em&gt;&lt;/strong&gt;. Specifically Bayes’ theorem (or Bayes’ rule) allows us to incorporate prior probability as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}&lt;/script&gt;

&lt;p&gt;In which, $P(D|\theta)$ is the likelihood, $P(\theta)$ is the prior probability, $P(D)$ is the marginal likelihood, and $P(\theta|D)$ is the posteriori likelihood.&lt;/p&gt;

&lt;p&gt;The principle of MAP is to estimate $\theta$ that maximise the posteriori likelihood $P(\theta)$. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MAP} = argmax\ \frac{P(D|\theta)P(\theta)}{P(D)}&lt;/script&gt;

&lt;p&gt;Notice that the denominator $P(D)$ is independent of $\theta$. Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D) \propto {P(D|\theta)P(\theta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \hat{\theta}_{MAP} = argmax\ {P(D|\theta)P(\theta)}&lt;/script&gt;

&lt;p&gt;By assuming attributes $X_{1}, X_{2}, …, X_{n}$ are independent of each other, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MAP} = argmax\ P(\theta) \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;
</description>
        <pubDate>Sat, 05 Mar 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/03/05/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes-part-2/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/03/05/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes-part-2/</guid>
      </item>
    
      <item>
        <title>Maximum Likelihood Estimation, Maximum a Posteriori Estimation and Naive Bayes (part 1)</title>
        <description>&lt;p&gt;There are some notes with regards to three important concepts – Maximum Likelihood Estimation (MLE), Maximum a Posterior Estimation (MAP), and Naive Bayes (NB) – that I would like to put here in order to remind me in case necessary. I also can see that my note can be fruitful to anyone having a hard time in grasping these concepts, because we’re gonna unfold plot twists of them that confused me while learning in this writing.&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood-estimation-mle&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;

&lt;h4 id=&quot;probability-vs-likelihood&quot;&gt;Probability vs Likelihood&lt;/h4&gt;

&lt;p&gt;First of all, before diving deep, it is essentially important to understand differences between two terms “likelihood” and “probability”, at least for the given concept. Thank to &lt;a href=&quot;http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_3.html&quot; target=&quot;_blank&quot;&gt;this article&lt;/a&gt;, we can have a good explanation as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: Observation of data -&amp;gt; Estimation of parameters &lt;strong&gt;(I)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Probability&lt;/strong&gt;: Knowing parameters  -&amp;gt; Prediction of outcome &lt;strong&gt;(II)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it’s still unclear, take another orthogonal but obvious example that involves another estimation method, called Least Squares Estimation (LSE), which is more preferable than MLE in case the data is linear. The way LSE works is straightforward: to estimate unknown parameters $\theta_{j}$ of fitting hypothesis functions $ h_{\theta_{j}} (x)$ for given a set of data by minimising the sum of squared errors. In &lt;strong&gt;Figure 1&lt;/strong&gt; we have a good illustration of this idea. In this example, by using linear regression, which is the method of least squares, we are able to &lt;em&gt;&lt;strong&gt;approximate&lt;/strong&gt;&lt;/em&gt; locally optimal values of parameters $\theta_{0} = 3.5$ and $\theta_{1} = 1.4$ for a linear function $ h_{\theta_{j}} (x) = \theta_{0} + \theta_{1}x$. By substituting values of $\theta_{j}$ we would have a particular hypothesis function $ h_{\theta_{j}} (x) = 3.5 + 1.4x$ corresponding to the blue line in the figure. Now we can use the hypothesis function we have found to &lt;strong&gt;&lt;em&gt;predict&lt;/em&gt;&lt;/strong&gt; a new value $y$ for a given $x$. Briefly we have two distinct steps as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Approximation&lt;/strong&gt;: Observation of data -&amp;gt; Estimation of parameters &lt;strong&gt;(I’)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Knowing parameters  -&amp;gt; Prediction of value &lt;strong&gt;(II’)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now if we roughly put steps &lt;strong&gt;I&lt;/strong&gt; and &lt;strong&gt;I’&lt;/strong&gt;, &lt;strong&gt;II&lt;/strong&gt; and &lt;strong&gt;II’&lt;/strong&gt; into comparison, we can see that they share common traits. Particularly the goal of steps &lt;strong&gt;I &lt;/strong&gt;and &lt;strong&gt;I’ &lt;/strong&gt;is to estimate $\theta_{j}$ that either best explains or best fits the data. In contrast, in both steps &lt;strong&gt;II &lt;/strong&gt;and &lt;strong&gt;II’, &lt;/strong&gt;the target is to predict either value or probability by using given parameters $\theta_{j}$.&lt;/p&gt;

&lt;div id=&quot;attachment_334&quot; style=&quot;width: 310px&quot; class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img class=&quot;wp-image-334 size-medium&quot; src=&quot;http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-300x290.png&quot; alt=&quot;Figure 1 &quot; width=&quot;300&quot; height=&quot;290&quot; srcset=&quot;http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-300x290.png 300w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-768x742.png 768w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-1024x989.png 1024w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM.png 1474w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot; /&gt;

  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1 [Photo courtesy of Wikipedia]
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;I know I was messing up a bit above when using an example of LSE to illustrate the differences between Probability and Likelihood. But that’s the thing I had not realised at first when I started to learn MLE. I was wondering why the hell we need to estimate a parameter $\theta$ for a given data $D$. It did not make sense because I assumed we already have a data, we can estimate any probability with ease… It was only when I started to approach MLE from a perspective of LSE as I illustrated above, things turn to be more clear to me.&lt;/p&gt;

&lt;h4 id=&quot;definition&quot;&gt;Definition&lt;/h4&gt;

&lt;p&gt;As said, the ultimate purpose of MLE is to choose parameter $\theta$ that best explains observed data $D$. In other words, MLE is a method to search in a space $\Omega$ to find a $\theta$ that maximise the probability of data $D$. We can formulated MLE mathematically as follows.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_{MLE}} = argmax \ P(D|\theta) = argmax \ P(X_{1}, X_{2}, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;Given that $D$ has $n$ attributes, we can obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}, X_{2}, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;By using &lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot; target=&quot;_blank&quot;&gt;chain rule&lt;/a&gt;, we can also have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_{1}, X_{2}, ..., X_{n}|\theta) = P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_2, ..., X_{n} |\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_{2}|X_{3}, ..., X_{n}, \theta) P(X_3, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;If repeat this process, we would obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_{2}|X_{3}, ..., X_{n},\theta)\ ...\ P(X_{n-1} | X_{n}, \theta) P(X_n|\theta)&lt;/script&gt;

&lt;p&gt;According to this equation, we can see there is an extremely big problem of computation complexity.&lt;/p&gt;

&lt;p&gt;Given $P(X_{1}, X_{2}, X_{n}|\theta)$, we would need $2(2^{n}-1) + 1$ parameters to describe. In practice, it is common to work with a dataset of more than 100 attributes, if not thousands. To reduce the number of parameters, MLE, MAP as well as NB fundamentally rely on an assumption that attributes $X_{1}, X_{2}, …, X_{n}$ are independent of each other. By doing this, we can reduce a number of parameters to $2n + 1$. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}|\theta) P(X_{2}|\theta)\ ...\ P(X_{n-1} |\theta) P(X_n|\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;

&lt;p&gt;Finally we have
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta_{MLE}} = argmax \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 03 Mar 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/03/03/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/03/03/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes/</guid>
      </item>
    
      <item>
        <title>Conquer unhappiness</title>
        <description>&lt;p&gt;Though the primitive purpose of my blog is to discuss issues in Computer Science, it’s always tempting to write something a bit off-the-topic. The subject matter I want to put in writing this time is about conquering happiness. Whilst the topic is very clique when we can easily stumble upon such topic in countless number of articles, in both tabloid and non-tabloid newspaper, I still think it is always of concern, particularly to programmers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/psyduck.jpg&quot; alt=&quot;You can&#39;t confuse me&quot; class=&quot;img-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As often, when being interviewed, people tend to portray a programmer or a computer scientist as a weird, non-sporty, untidy and super fatty/skinny person who is heavily addicted to computers, not to mention sugar drinks. They may also wonder how such sort of people can overcome depressions, or tragically, whether programmers are always in state of bad mood, and that’s why they eventually become so weird ?!&lt;/p&gt;

&lt;p&gt;I cannot answer those questions but there is a story I want to tell. In my first year at VUB, besides being intrigued by new programming stuffs, it was both appealing and entertaining to me to meet up a group of second year Master students, called InfoGroup. They own a separate and ample room in a building. In that room, besides desktops and books, I could see a lot of snacks, beers, wines and of course sugar drinks. I still remember a skinny, curly hair guy who put a monitor on top of many Cokes cans, surrounding his working cubic was a mountain of snacks. Many of his friends, both sitting next and in front of him, also seemed so nerdy with countless lines of codes running on their screens. It was actually an interesting experience to me because it was the first time, I understood why people being so serious about programmers. It did not take me long to get used to those kinds of people, but until now, whenever I recall those moments, I always feel very funny.&lt;/p&gt;

&lt;p&gt;Non-sporty, odd, snack addictive, etc. There are maybe more adjectives to depict a software programmers, but the truth is I never see myself being such kind of people, except the part that I am truly addictive to computers. However, the critical part I would like to emphasise is I always have weekly plans to clean up myself, both mentally and physically, through varied kinds of exercises. I also have to say that it is very hard to stick to the plan when there are thousands of bugs can silently sneak into any line of codes for which a developer has to sweatily burn his ass to write. None of developers want bugs, at least to me, but when bugs occur, it is often that I have to skip meals to fix. In such case, snacks and sugar drinks become more tempting, and sometimes unavoided.&lt;/p&gt;

&lt;p&gt;Nonetheless, in my experience, using sugar and being active likely get me depressed and foggy. When I was in that state of mood, I can write no line of codes. I get easily get struck by minor bugs. My creativity is dead and more importantly, insomnia occurs more often. It is extremely difficult to stay upbeat during those moments. Unhappiness and loneliness creep into my heart, getting me feel bad at everything, including myself.&lt;/p&gt;

&lt;p&gt;In order to stay upbeat, swimming, running and weight lifting has become the norm in my life style for approximately 5 years. Whilst running and weight lifting are suitable for gloomy days, swimming is always my first choice when days become warm and sunny. I always have positive feelings after every swimming session. My mind is refreshed and my flesh is toned up. Come back to nerdy guys in InfoGroup at VUB. In my second year when going to gym, I surprisingly saw a nerdy and skinny guy of InfoGroup signed up for a gym class. Many days later, I saw more members of that group went to the gym. It was once again surprisingly and entertaining to me when I saw such change. I have no clue about it but I was more happy to chit chat with them in gym class rather than in computer room.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Feb 2016 09:52:14 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/02/25/conquer-unhappiness/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/02/25/conquer-unhappiness/</guid>
      </item>
    
      <item>
        <title>Full-text search with Apache Lucene</title>
        <description>&lt;p&gt;Apache Lucene is a de facto open source library targeting search and search- related tasks such as indexing or querying.&lt;/p&gt;

&lt;h3 id=&quot;how-lucene-indexes-documents-algorithmically&quot;&gt;1. How Lucene indexes documents algorithmically&lt;/h3&gt;

&lt;p&gt;Lucene uses the &lt;em&gt;inverted index data structure&lt;/em&gt; to index documents. The indexing strategy of Lucene is different from others when it creates segments and merges them, rather than uses a single index. Typically, an index segment consists of a dictionary index, a term dictionary and a posting dictionary.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/indexing-lucene.png&quot; alt=&quot;The index diagram with the merge factor b is 3 &quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The index diagram with the merge factor b is 3 
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt; illustrates an example of an index, where the merge factor equals three. In the example, there are 14 documents in the collection. Lucene repeatedly creates a segment for a document and periodically merges a group of 3 documents. The process is similar when Lucene keeps merging a group of 3 segments until there is no more segment to merge. After merging, all greyed segments will be removed and in total, Lucene merged 5 times after the indexing finishes. The approach of merging and deleting segments is considerably useful as the document collection does not change frequently. More importantly, the segment will never be modified. Instead, Lucene creates new segments when the document collection changes and later, it merges segments into new ones and deletes the old segments. This strategy ensures there is no conflict between reading writing indexes. Furthermore, this strategy also allows Lucene to avoid complex B-trees to store segments. Instead, all segments will be stored in flat files.&lt;/p&gt;

&lt;h3 id=&quot;extract-text-andindex&quot;&gt;2. Extract text and index&lt;/h3&gt;

&lt;p&gt;Typically we can divide indexing documents into two distinct procedures, extracting text and creating index &lt;strong&gt;(Figure 2)&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/analyze-lucene.png&quot; alt=&quot;Figure 2. Indexing with Lucene breaks down into three main operations: extracting text from source documents, analyzing it, and saving it to the index [Photo courtesy of Lucene in Action]&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2. Indexing with Lucene breaks down into three main operations: extracting text from source documents, analyzing it, and saving it to the index [Photo courtesy of Lucene in Action]
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In extracting procedure, it is common to use a versatile parser that can extract textual contents from documents. One of widely known tools we can use to parse documents is &lt;a href=&quot;https://tika.apache.org&quot; target=&quot;_blank&quot;&gt;Apache Tika&lt;/a&gt;. When parsing completes, we will have an input stream that needs to be indexed. It is the second procedure of our index process – creating an index.&lt;/p&gt;

&lt;p&gt;More often than not, we do not index everything we retrieve from the input stream. We instead need to apply &lt;strong&gt;analysis process&lt;/strong&gt; where we will discard unimportant terms from the textual content (e.g., punctuations, stopping words). In Lucene, the analysis process is handled by &lt;code class=&quot;highlighter-rouge&quot;&gt;Analyzer&lt;/code&gt;. The framework already provides handy and powerful analysers that we can use without customising in a majority of problems.&lt;/p&gt;

&lt;p&gt;After analysing, the textual contents are ready to be indexed.&lt;/p&gt;

&lt;h3 id=&quot;building-an-index&quot;&gt;&lt;strong&gt;3. Building an index&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In this section, we will look at how we can create a new index and add documents to it.&lt;/p&gt;

&lt;h4 id=&quot;constructan-index-writer&quot;&gt;3.1 Construct an index writer&lt;/h4&gt;

&lt;p&gt;In Lucene, the index is created and maintained by &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriter&lt;/code&gt;. The IndexWriter constructors can be defined in various ways but most often we simply need to supply&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Directory d&lt;/code&gt; where the index will be stored&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriterConfig conf&lt;/code&gt; holds all the configuration that is used to construct an &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriter&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the code below, we can see that I have define an instance of &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriterConfig&lt;/code&gt; with a specialised Analyzer`. The purpose is I want to analyse documents in a particular way that suits my personal needs (e.g., I want to keep specific words that built-in analysers will certainly remove).&lt;/p&gt;

&lt;p&gt;In the given code sample, we use &lt;code class=&quot;highlighter-rouge&quot;&gt;FSDirectory&lt;/code&gt; . There are other alternative solutions, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;SimpleFSDirectory&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;NIOFSDirectory&lt;/code&gt; .&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;cm&quot;&gt;/**
 * Create an Indexer using the given directory to store index files, and the
 * directory need to be indexed.
 *
 * */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Indexer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anIndexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aDataDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// set place to store indexes&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;indexDir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anIndexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;// set directory has to be indexed&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;dataDir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aDataDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// create Directory to store indexes, use FSDirectory.open&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// to automatically pick the most suitable directory implementation&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Directory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create an instance of Analyzer with my own custom analyzer&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomAnalyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create configuration for new IndexWriter&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;IndexWriterConfig&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexWriterConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;LUCENE_46&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create IndexWriter, which can create new index, or&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// adds, removes, or updates documents in the index&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// however, it can not read or search&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexWriter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IOException&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;modelling-document&quot;&gt;3.2 Modelling document&lt;/h4&gt;

&lt;p&gt;In the framework textual content is modelled using the class &lt;code class=&quot;highlighter-rouge&quot;&gt;Document&lt;/code&gt; . This class allows to model a document instance with numerous attributes where for every attribute we can also define using the class &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;In the sample code below, I have define an &lt;code class=&quot;highlighter-rouge&quot;&gt;Document&lt;/code&gt; instance with a number of attributes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Document ID&lt;/strong&gt;: because in practice we may have thousands of documents, I have used base64 to generate unique IDs for documents. Notice that we supply the &lt;code class=&quot;highlighter-rouge&quot;&gt;StringField&lt;/code&gt; constructor with the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;Field.Store.YES&lt;/code&gt; , which means the values of this attributes will be stored in index files. More importantly, we &lt;strong&gt;do not index &lt;/strong&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;StringField&lt;/code&gt; by default.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;File path&lt;/strong&gt;: will be stored&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content field&lt;/strong&gt;: as you may think, we need to index the content field for further full-text search. However, we may or may not store it in the index files. It is a matter of choice. In this particular case, I choose to store the field.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// docId&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// generated by base64 of filepath&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseEncoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;base64&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getCanonicalPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Charsets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;US_ASCII&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;ID_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Store&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;YES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// filepath&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;FILE_PATH_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getCanonicalPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Store&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;YES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// content field&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TokenStream&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tokenStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;CONTENT_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contentField&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;CONTENT_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;AnalyzerUtils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tokenStreamToString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TYPE_STORED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contentField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;index-document&quot;&gt;3.3 Index document&lt;/h4&gt;

&lt;p&gt;While Lucene consists many components, it, at the same time, uses such extremely simple and intuitive methods to abstract the complex structures that it is fascinating seeing how it is simple to perform indexing.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//modelling document in the same manner as section 3.2&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//add document to the index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;addDocument&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//close the index when finish&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//which also means we commit the index to be ready to store&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;full-text-search&quot;&gt;4. Full-text search&lt;/h3&gt;

&lt;p&gt;Until now we have an index that can be used to search over. To initialise a search, we must input a search query, which can be a keyword or even a wild card. In Lucene, the search query will firstly be handled by &lt;code class=&quot;highlighter-rouge&quot;&gt;QueryParser&lt;/code&gt; where the query will be parsed into meaningful structure, called &lt;code class=&quot;highlighter-rouge&quot;&gt;Query&lt;/code&gt; . After that, the &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexSearcher&lt;/code&gt; will consume &lt;code class=&quot;highlighter-rouge&quot;&gt;Query&lt;/code&gt; instance in order to look up over the index.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// searcher&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IndexSearcher&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searcher&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexSearcher&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;indexReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// query parser&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;QueryParser&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QueryParser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;LUCENE_46&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleAnalyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// parsing query&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Query&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;....&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// look up and return top 10 relevant documents&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Topdocs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;docs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searcher&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;bottom-line&quot;&gt;5. Bottom line&lt;/h3&gt;

&lt;p&gt;So far we have seen a “hello world” example of using Lucene to perform full-text search. In real project, we can expect there are a number of possibilities where we need to tweak Lucene so that it can serve our search purpose at a higher level. An example scenario is when we need to support &lt;strong&gt;like-this search&lt;/strong&gt; where the system allows the user to search textual contents that are similar to the keywords. Theoretically we can measure &lt;strong&gt;the similarity &lt;/strong&gt;by using distance measure methods, such as Euclidean distance measure, cosine distance measure, or Manhattan distance measure, etc. Another scenario is when we can create a system that supports &lt;strong&gt;synonym search &lt;/strong&gt;where we retrieve not only document contains the search keywords but also documents that has synonyms of keywords.&lt;/p&gt;

&lt;p&gt;As said, in spite of the simplicity, Lucene adds a tremendous amount of support in building a powerful search tool. In the future post, I may discuss how we can combine both Apache Lucene and Apache Mahout (a machine learning framework) in order to index a corpus and use that index to perform clustering. Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Sat, 16 Jan 2016 21:36:49 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/01/16/full-text-search-with-apache-lucene/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/01/16/full-text-search-with-apache-lucene/</guid>
      </item>
    
      <item>
        <title>Google Reader replacement</title>
        <description>&lt;p&gt;Lately I am not feeling like I am using time wisely when it is quite often I can see myself wasting a huge amount of time on consuming a large amount of information, among which were mostly garbage. I have done several contemplations regarding to this matter to see how I would done to improve such situation. And I come up with an idea of building a hub of filtered news. It would be exactly the same as Google Reader, which is already shut down for a period of time, but the difference is that this hub will be self-hosted.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/tiny-tiny-rss.png&quot; alt=&quot;My Tiny Tiny RSS homepage&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    My Tiny Tiny RSS homepage
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;At the moment there are some open source applications that can be alternatives to Google Reader, among them I think Tiny Tiny RSS would be a top candidate. TTS is lightweight, and easy-to-set-up. I have started to use it for a couple of hours, and I couldn’t say how satisfied I am.&lt;/p&gt;

&lt;p&gt;It can be downloaded &lt;a href=&quot;https://tt-rss.org/gitlab/fox/tt-rss/wikis/home&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2016 05:21:57 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/01/13/google-reader-replacement/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/01/13/google-reader-replacement/</guid>
      </item>
    
  </channel>
</rss>

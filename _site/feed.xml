<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>newbiettn</title>
    <description>Standing on the shoulders of giants</description>
    <link>http://127.0.0.1:4000</link>
    <atom:link href="http://127.0.0.1:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Classification using Rough Set theory</title>
        <description>&lt;h4 id=&quot;i-introduction&quot;&gt;I. Introduction&lt;/h4&gt;
&lt;p&gt;Rough Set (RS) algorithm targets the &lt;strong&gt;vagueness&lt;/strong&gt; of the knowledge, e.g., the boundary between observations is not strong enough to set them apart. Look at Figure 1, we can see that we have no clue how to correctly classify items $x_3$ and $x_4$ when the attribute values of those items are identical.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 1: it is impractical to classify x3 and x4&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1: it is impractical to classify x3 and x4
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ii-primary-concepts-of-rs&quot;&gt;II. Primary concepts of RS&lt;/h4&gt;
&lt;p&gt;RS theory defines a number of concepts:&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/headache-muscle-temp-dataset.png&quot; alt=&quot;Figure 2: Sample dataset&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2: Sample dataset
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;information-system-i--u-a&quot;&gt;1) Information system: $I = (U, A)$&lt;/h4&gt;
&lt;p&gt;Information system can be understood as a body of information/knowledge that we have. Mathematically the information system is denoted as a pair $(U, A)$ where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$U = {x_1, x_2, …, x_n}$: universally represents a non-empty $(U \neq \emptyset)$, finite set of observations $x_i$.&lt;/li&gt;
  &lt;li&gt;$A = {a_1, a_2, …, a_n}$: represent a non-empty $(A \neq \emptyset)$, finite set of attributes $a_j$ such that $a \in A$, $a: U \rightarrow V_a$, where $V_a$ is the value set of $a$ (which means every observation in $U$ can be set to attributes that belong to $V_a$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In ML, we usually call $x_i$ observations, objects, rows; and $a_j$ predictors, features, attributes.&lt;/p&gt;

&lt;h4 id=&quot;decision-system-ds--u-a-cup-d--ds-subseteq-i&quot;&gt;2) Decision system: $DS = (U, A \cup {d})$ | $DS \subseteq I$&lt;/h4&gt;
&lt;p&gt;Decision system $DS$ is a special type of information system $I$ that can be used in classification. $d$ is the &lt;em&gt;decision&lt;/em&gt; values (i.e., Yes/No, 1/0, and so on). In ML, we call $d_k \in {d}$ response, or outcome.&lt;/p&gt;

&lt;h4 id=&quot;indiscernibility-relation&quot;&gt;3) Indiscernibility relation&lt;/h4&gt;
&lt;p&gt;Given an information system $I = (U, A)$, for any subset $B \subseteq A$, the indiscernibility or equivalence relation is defined by:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{(x, y) \in U^2 | \forall a \in B, a(x) = a(y)\}&lt;/script&gt;
where $IND(B)$ is called the &lt;strong&gt;B-indiscernibility relation&lt;/strong&gt;. The equivalence classes of $IND(B)$ are denoted as $[x]_B$.&lt;/p&gt;

&lt;p&gt;Take an example from Figure 2. Suppose we have $B= { Headache, Musclepain }$. Thus, we can have:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{\{e1, e2, e3\}, \{e4, e6\}, \{e5\}\}&lt;/script&gt;
We can also say that $e1, e2, e3$ are indiscernible from each other.&lt;/p&gt;

&lt;h4 id=&quot;approximation&quot;&gt;4) Approximation&lt;/h4&gt;
&lt;p&gt;The concept of approximation regards the problem of &lt;strong&gt;data reduction&lt;/strong&gt; in ML. For that, we can define a approximation set $X (X \subseteq U)$ by using the information from the set $B (B \subseteq A)$. To do that, we have to construct $B-lower$ $(\underline{B}X)$ and $B-upper$ $(\overline{B}X)$ approximations of $X$:
&lt;script type=&quot;math/tex&quot;&gt;\underline{B}X = \{x | [x]_B \subseteq X\}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\overline{B}X = \{x | [x]_B \cap X \neq \emptyset\}&lt;/script&gt;
Whereas &lt;em&gt;lower-approximation&lt;/em&gt; consists of all observations which &lt;strong&gt;certainly&lt;/strong&gt; belong to the class, &lt;em&gt;upper-approximation&lt;/em&gt; contains all observations which &lt;strong&gt;likely&lt;/strong&gt; belong to the class.&lt;/p&gt;

&lt;p&gt;From that we have $B-boundary$:
&lt;script type=&quot;math/tex&quot;&gt;BN_B(X) = \overline{B}X - \underline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and $B-outside$:
&lt;script type=&quot;math/tex&quot;&gt;BO_B = U - \overline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 3&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 3
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For example, in Figure 3, let
&lt;script type=&quot;math/tex&quot;&gt;W = \{x | Walk(x) = yes\}&lt;/script&gt;
we can have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\underline{B}W = {x1, x6}$&lt;/li&gt;
  &lt;li&gt;$\overline{B}W = {x1, x3, x4, x6}$&lt;/li&gt;
  &lt;li&gt;$BN_B(W) = {x3, x4}$&lt;/li&gt;
  &lt;li&gt;$BO_B = {x2, x5, x7}$&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;iii-roughtsets-package-in-r&quot;&gt;III. “RoughtSets” package in R&lt;/h4&gt;
</description>
        <pubDate>Tue, 06 Sep 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</guid>
      </item>
    
      <item>
        <title>Precision, Recall, Sensitivity and Specificity</title>
        <description>&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;I recently discovered that the concepts of precision, recall, sensitivity and specificity are very much important to what I am currently working on. Those are equally important but not for a single domain. Instead we have different indicators for different domains. For example, in information retrieval, precision and recall are two important measures that need to be taken into account when evaluate a search system. In pattern recognition and machine learning, precision and recall are important as well but there is a slight difference when interpret them. On the other hand, sensitivity and specificity are alternations to precision and recall when it comes to medical application. In this writing, we aim to explain why there exist those differences in the first place.&lt;/p&gt;

&lt;p&gt;In order to begin, first we look at &lt;strong&gt;the confusion matrix&lt;/strong&gt; to understand where terms TP, FP, TN, and FN come from, and second, look at the Venn digram that perfectly portraits the territories of TP, FP, TN, and FN within the search space.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/confusion-matrix-noted.jpg&quot; alt=&quot;The confusion matrix of the search space&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The confusion matrix of the search space
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/precision-recall.jpg&quot; alt=&quot;The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;And mathematically, precision, recall, sensitivity and specificity are denoted as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{TP}{TP + FP} \\
\\
Recall = Sensitivity = \frac{TP}{TP + FN} \\
\\
Specificity = \frac{TN}{TN + FP}&lt;/script&gt;

&lt;h4 id=&quot;precision&quot;&gt;Precision&lt;/h4&gt;
&lt;p&gt;I found that we can easily understand the intuitive concept of precision by simply looking at the Venn diagram. Simply speaking, precision is the ratio between the documents that match the user expectation and the total number of documents returned by the system. One cay say that the higher the precision, the better. But it is always true because a system can &lt;em&gt;cheat&lt;/em&gt; the precision score up to 100% by only returning documents about which it is extremely confident. By doing this, the amount of returned documents is severely reduced, making it so resistant to FP.Look at the figure below, because the space of returned documents is so small, it can fall within the space of the expected documents easily. What is the problem with that? Because the amount of search result is extremely small, the information is likely inadequate to support the user tasks. Mathematically, we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt;, thus &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-precision-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;recall-and-sensitivity&quot;&gt;Recall and Sensitivity&lt;/h4&gt;
&lt;p&gt;We can also grasp the concept of recall and sensitivity smoothly through the Venn diagram observation. Both recall and sensitivity can be understand as the ratio between the documents that match the user expectation and the total number of expected documents from the user. Intuitively speaking, recall and sensitivity are big indicators for how much good information that a system really misses, whereas precision is an indicator for how much good information in the search result the user can use. We can also cheat the recall/sensitivity score by getting the system to add up the number of returned documents, making the search result overwhelmingly abundant. Then guess who is going have a hard time to filtrate such load of information. Look at the figure below, we can see that by returning an overwhelming amount of information, the system can ensure that its recall/sensitivity is maximized. Mathematically, by doing this we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FN \rightarrow 0&lt;/script&gt;, therefore &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FN} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-recall-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;specificity&quot;&gt;Specificity&lt;/h4&gt;
&lt;p&gt;To understand the concept of specificity, as oppose to observing the Venn diagram, I found it much more easier to understand by checking the confusion matrix. First of all, one has to understand that by using specificity, he wants to steer the focus of his evaluation towards how good the system can predict negative outcomes( or how good the system can avoid returning irrelevant documents,. That’s the reason why TN is the numerator of the equation in the first place. One can also maximize the specificity score of the system by getting the system return no information at all (or predict all observations are negative). Mathematically, when return no information or predict all are negative, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;TN \rightarrow 1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\frac{FP}{TN + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;which-one-i-should-use&quot;&gt;Which one I should use?&lt;/h4&gt;
&lt;p&gt;We have four metrics to choose, so the question is which one I should. Well I can say that it depends on the problem domain. For example, if we have built a search system, in order to evaluate it, firstly we have to answer some questions: Which criteria we care about most? Do we care about the vast amount of information we ignore (which is TN)? Or do we care about the information we can return (FP + TP + FN)? My answer is I care about the relevant of information my system gives the user rather than the information I can’t return (there is countless amount of information, who cares in the first place). Thus, to evaluate the search system, I would use precision, recall and ignore specificity.&lt;/p&gt;

&lt;p&gt;In contrast, suppose we have  a diabetes predictive model to evaluate, which metrics should we care about? Do we care if a patient doesn’t have diabetes, but the system predicts positive (TN + FP)? Do we care about if a patient has diabetes, but the system predicts negative (TP + FP)? and so on. My answer is I would use all of three indicators (e.g., precision, recall, specificity) because I guess we have to take into account all of indicators TP, FP, TN and FN. Everything is crucial in this case.&lt;/p&gt;

&lt;p&gt;Lastly and interestingly, imagine if we had a system that can exploit all evidences and judge if a suspect guilty. In this case, what we care about most? Do we care if the suspect doesn’t commit the crime but the system judges him guilty (FP)? Or do we care if the suspect does commit the crime but the system judges him not guilty (FN)? Yeah, I guess in this case, we should put more emphasis on FP rather than FN, because putting an innocent person to the jail is also a crime, and letting a guilty suspect go is not as dire as putting innocent person to jail.&lt;/p&gt;

&lt;p&gt;** In statistic and relevant fields, usually we put more emphasis on Type I error (e.g., reject &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; incorrectly (FP)), rather than Type 2 error (e.g., retain a false &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; (FN)).&lt;/p&gt;

&lt;p&gt;The end.&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Aug 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</guid>
      </item>
    
      <item>
        <title>Likelihood ratio test</title>
        <description>&lt;p&gt;The likelihood-ratio test is an alternative to $\chi^2$ test in order to assess the **goodness of fit**. It is straight-forward and easy to use.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Calculate $G = -2ln\bigg[\frac{likeli \, hood \, of \, the \, replacing \, model \, h_\theta^{(1)}}{likelihood \, of \, the \, existing \, model \, h_\theta^{(0)}}\bigg] = 2[loglikelihood \, of \, h_\theta^{(0)} – loglikelihood \, of \, h_\theta^{(1)}]$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– Notice that $ln$ is removed by performing logarithm.&lt;/p&gt;

&lt;p&gt;– The log-likelihood is usually outputted by R-language.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Because the distribution model of likelihood ratio test is nearly the same as $\chi^2$, it allows us to assign $G$-score to $\chi^2$-score.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– Mathematically we have $G$-score = $\chi^2$-score&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Find $p$ value based on $\chi^2$ statistic obtained.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Determine if $h_\theta^{(1)}$ is more statistically significant than $h_\theta^{(0)}$ based on obtained $p$ value and the statistical significance level $\alpha$ (usually $\alpha = 0.05$)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;example&quot;&gt;&lt;strong&gt;Example&lt;/strong&gt;:&lt;/h3&gt;

&lt;p&gt;Suppose we try to fit a given dataset with two alternative models $h_\theta^{(1)}$ and $h_\theta^{(0)}$ where in $h_\theta^{(1)}$ we eliminated some predictors we reckon that they are not sensitive enough. Now we try to assess if the model $h_\theta^{(1)}$ is more statistically significant than the original model by using the likelihood ratio test.&lt;/p&gt;

&lt;p&gt;Suppose $df = 1$.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Log-likelihood of&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;– $h_\theta^{(1)}$ is 53.6&lt;/p&gt;

&lt;p&gt;– $h_\theta^{(0)}$ is 68.3&lt;/p&gt;

&lt;p&gt;Therefore, we have $G = 2[68.3 – 53.6] = 29.31$&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;$\chi^2 = G = 29.31$&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Find $p = P[\chi^2(df = 1) = 29.31]$ by using R&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pchisq(29.31, 1, lower.tail = FALSE)
[1] 6.167658e-08
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;ol&gt;
  &lt;li&gt;Because $p$-value « $\alpha = 0.05$, we can reject the null hypothesis $H_0$ that two models are not significant different, and accept the alternative hypothesis $H_a$ that $h_\theta^{(1)}$ fits better than $h_\theta^{(0)}$.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Tue, 09 Aug 2016 21:34:05 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/08/09/likelihood-ratio-test/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/08/09/likelihood-ratio-test/</guid>
      </item>
    
      <item>
        <title>Maximum Likelihood Estimation, Maximum a Posteriori Estimation and Naive Bayes (part 2)</title>
        <description>&lt;h3 id=&quot;maximum-a-posteriori-estimation-map&quot;&gt;Maximum a Posteriori Estimation (MAP)&lt;/h3&gt;

&lt;p&gt;Generally, MAP is a particular extension of MLE where we take in account a biased assumption of data, called &lt;strong&gt;&lt;em&gt;prior knowledge&lt;/em&gt;&lt;/strong&gt;. Specifically Bayes’ theorem (or Bayes’ rule) allows us to incorporate prior probability as follows&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}&lt;/script&gt;

&lt;p&gt;In which, $P(D|\theta)$ is the likelihood, $P(\theta)$ is the prior probability, $P(D)$ is the marginal likelihood, and $P(\theta|D)$ is the posteriori likelihood.&lt;/p&gt;

&lt;p&gt;The principle of MAP is to estimate $\theta$ that maximise the posteriori likelihood $P(\theta)$. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MAP} = argmax\ \frac{P(D|\theta)P(\theta)}{P(D)}&lt;/script&gt;

&lt;p&gt;Notice that the denominator $P(D)$ is independent of $\theta$. Therefore&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(\theta|D) \propto {P(D|\theta)P(\theta)}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Rightarrow \hat{\theta}_{MAP} = argmax\ {P(D|\theta)P(\theta)}&lt;/script&gt;

&lt;p&gt;By assuming attributes $X_{1}, X_{2}, …, X_{n}$ are independent of each other, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta}_{MAP} = argmax\ P(\theta) \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;
</description>
        <pubDate>Sat, 05 Mar 2016 22:23:49 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/03/05/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes-part-2/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/03/05/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes-part-2/</guid>
      </item>
    
      <item>
        <title>Maximum Likelihood Estimation, Maximum a Posteriori Estimation and Naive Bayes (part 1)</title>
        <description>&lt;p&gt;There are some notes with regards to three important concepts – Maximum Likelihood Estimation (MLE), Maximum a Posterior Estimation (MAP), and Naive Bayes (NB) – that I would like to put here in order to remind me in case necessary. I also can see that my note can be fruitful to anyone having a hard time in grasping these concepts, because we’re gonna unfold plot twists of them that confused me while learning in this writing.&lt;/p&gt;

&lt;h3 id=&quot;maximum-likelihood-estimation-mle&quot;&gt;Maximum Likelihood Estimation (MLE)&lt;/h3&gt;

&lt;h4 id=&quot;probability-vs-likelihood&quot;&gt;Probability vs Likelihood&lt;/h4&gt;

&lt;p&gt;First of all, before diving deep, it is essentially important to understand differences between two terms “likelihood” and “probability”, at least for the given concept. Thank to &lt;a href=&quot;http://statgen.iop.kcl.ac.uk/bgim/mle/sslike_3.html&quot; target=&quot;_blank&quot;&gt;this article&lt;/a&gt;, we can have a good explanation as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Likelihood&lt;/strong&gt;: Observation of data -&amp;gt; Estimation of parameters &lt;strong&gt;(I)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Probability&lt;/strong&gt;: Knowing parameters  -&amp;gt; Prediction of outcome &lt;strong&gt;(II)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If it’s still unclear, take another orthogonal but obvious example that involves another estimation method, called Least Squares Estimation (LSE), which is more preferable than MLE in case the data is linear. The way LSE works is straightforward: to estimate unknown parameters $\theta_{j}$ of fitting hypothesis functions $ h_{\theta_{j}} (x)$ for given a set of data by minimising the sum of squared errors. In &lt;strong&gt;Figure 1&lt;/strong&gt; we have a good illustration of this idea. In this example, by using linear regression, which is the method of least squares, we are able to &lt;em&gt;&lt;strong&gt;approximate&lt;/strong&gt;&lt;/em&gt; locally optimal values of parameters $\theta_{0} = 3.5$ and $\theta_{1} = 1.4$ for a linear function $ h_{\theta_{j}} (x) = \theta_{0} + \theta_{1}x$. By substituting values of $\theta_{j}$ we would have a particular hypothesis function $ h_{\theta_{j}} (x) = 3.5 + 1.4x$ corresponding to the blue line in the figure. Now we can use the hypothesis function we have found to &lt;strong&gt;&lt;em&gt;predict&lt;/em&gt;&lt;/strong&gt; a new value $y$ for a given $x$. Briefly we have two distinct steps as follows.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Approximation&lt;/strong&gt;: Observation of data -&amp;gt; Estimation of parameters &lt;strong&gt;(I’)&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Prediction&lt;/strong&gt;: Knowing parameters  -&amp;gt; Prediction of value &lt;strong&gt;(II’)&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now if we roughly put steps &lt;strong&gt;I&lt;/strong&gt; and &lt;strong&gt;I’&lt;/strong&gt;, &lt;strong&gt;II&lt;/strong&gt; and &lt;strong&gt;II’&lt;/strong&gt; into comparison, we can see that they share common traits. Particularly the goal of steps &lt;strong&gt;I &lt;/strong&gt;and &lt;strong&gt;I’ &lt;/strong&gt;is to estimate $\theta_{j}$ that either best explains or best fits the data. In contrast, in both steps &lt;strong&gt;II &lt;/strong&gt;and &lt;strong&gt;II’, &lt;/strong&gt;the target is to predict either value or probability by using given parameters $\theta_{j}$.&lt;/p&gt;

&lt;div id=&quot;attachment_334&quot; style=&quot;width: 310px&quot; class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img class=&quot;wp-image-334 size-medium&quot; src=&quot;http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-300x290.png&quot; alt=&quot;Figure 1 &quot; width=&quot;300&quot; height=&quot;290&quot; srcset=&quot;http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-300x290.png 300w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-768x742.png 768w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM-1024x989.png 1024w, http://newbiettn.com/wp-content/uploads/2016/03/Screen-Shot-2016-03-03-at-4.29.41-PM.png 1474w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot; /&gt;

  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1 [Photo courtesy of Wikipedia]
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;I know I was messing up a bit above when using an example of LSE to illustrate the differences between Probability and Likelihood. But that’s the thing I had not realised at first when I started to learn MLE. I was wondering why the hell we need to estimate a parameter $\theta$ for a given data $D$. It did not make sense because I assumed we already have a data, we can estimate any probability with ease… It was only when I started to approach MLE from a perspective of LSE as I illustrated above, things turn to be more clear to me.&lt;/p&gt;

&lt;h4 id=&quot;definition&quot;&gt;Definition&lt;/h4&gt;

&lt;p&gt;As said, the ultimate purpose of MLE is to choose parameter $\theta$ that best explains observed data $D$. In other words, MLE is a method to search in a space $\Omega$ to find a $\theta$ that maximise the probability of data $D$. We can formulated MLE mathematically as follows.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\hat{\theta_{MLE}} = argmax \ P(D|\theta) = argmax \ P(X_{1}, X_{2}, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;Given that $D$ has $n$ attributes, we can obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}, X_{2}, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;By using &lt;a href=&quot;https://en.wikipedia.org/wiki/Chain_rule_(probability)&quot; target=&quot;_blank&quot;&gt;chain rule&lt;/a&gt;, we can also have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(X_{1}, X_{2}, ..., X_{n}|\theta) = P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_2, ..., X_{n} |\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_{2}|X_{3}, ..., X_{n}, \theta) P(X_3, ..., X_{n}|\theta)&lt;/script&gt;

&lt;p&gt;If repeat this process, we would obtain&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}|X_{2}, ..., X_{n}, \theta) P(X_{2}|X_{3}, ..., X_{n},\theta)\ ...\ P(X_{n-1} | X_{n}, \theta) P(X_n|\theta)&lt;/script&gt;

&lt;p&gt;According to this equation, we can see there is an extremely big problem of computation complexity.&lt;/p&gt;

&lt;p&gt;Given $P(X_{1}, X_{2}, X_{n}|\theta)$, we would need $2(2^{n}-1) + 1$ parameters to describe. In practice, it is common to work with a dataset of more than 100 attributes, if not thousands. To reduce the number of parameters, MLE, MAP as well as NB fundamentally rely on an assumption that attributes $X_{1}, X_{2}, …, X_{n}$ are independent of each other. By doing this, we can reduce a number of parameters to $2n + 1$. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(D|\theta) = P(X_{1}|\theta) P(X_{2}|\theta)\ ...\ P(X_{n-1} |\theta) P(X_n|\theta)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;= \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;

&lt;p&gt;Finally we have
&lt;script type=&quot;math/tex&quot;&gt;\hat{\theta_{MLE}} = argmax \prod_{i=1}^n P(X_i|\theta)&lt;/script&gt;&lt;/p&gt;
</description>
        <pubDate>Fri, 04 Mar 2016 03:48:25 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/03/04/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/03/04/maximum-likelihood-estimation-maximum-a-posteriori-estimation-and-naive-bayes/</guid>
      </item>
    
      <item>
        <title>Conquer unhappiness</title>
        <description>&lt;p&gt;Though the primitive purpose of my blog is to discuss issues in Computer Science, it’s always tempting to write something a bit off-the-topic. The subject matter I want to put in writing this time is about conquering happiness. Whilst the topic is very clique when we can easily stumble upon such topic in countless number of articles, in both tabloid and non-tabloid newspaper, I still think it is always of concern, particularly to programmers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/images/psyduck.jpg&quot; alt=&quot;You can&#39;t confuse me&quot; class=&quot;img-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As often, when being interviewed, people tend to portray a programmer or a computer scientist as a weird, non-sporty, untidy and super fatty/skinny person who is heavily addicted to computers, not to mention sugar drinks. They may also wonder how such sort of people can overcome depressions, or tragically, whether programmers are always in state of bad mood, and that’s why they eventually become so weird ?!&lt;/p&gt;

&lt;p&gt;I cannot answer those questions but there is a story I want to tell. In my first year at VUB, besides being intrigued by new programming stuffs, it was both appealing and entertaining to me to meet up a group of second year Master students, called InfoGroup. They own a separate and ample room in a building. In that room, besides desktops and books, I could see a lot of snacks, beers, wines and of course sugar drinks. I still remember a skinny, curly hair guy who put a monitor on top of many Cokes cans, surrounding his working cubic was a mountain of snacks. Many of his friends, both sitting next and in front of him, also seemed so nerdy with countless lines of codes running on their screens. It was actually an interesting experience to me because it was the first time, I understood why people being so serious about programmers. It did not take me long to get used to those kinds of people, but until now, whenever I recall those moments, I always feel very funny.&lt;/p&gt;

&lt;p&gt;Non-sporty, odd, snack addictive, etc. There are maybe more adjectives to depict a software programmers, but the truth is I never see myself being such kind of people, except the part that I am truly addictive to computers. However, the critical part I would like to emphasise is I always have weekly plans to clean up myself, both mentally and physically, through varied kinds of exercises. I also have to say that it is very hard to stick to the plan when there are thousands of bugs can silently sneak into any line of codes for which a developer has to sweatily burn his ass to write. None of developers want bugs, at least to me, but when bugs occur, it is often that I have to skip meals to fix. In such case, snacks and sugar drinks become more tempting, and sometimes unavoided.&lt;/p&gt;

&lt;p&gt;Nonetheless, in my experience, using sugar and being active likely get me depressed and foggy. When I was in that state of mood, I can write no line of codes. I get easily get struck by minor bugs. My creativity is dead and more importantly, insomnia occurs more often. It is extremely difficult to stay upbeat during those moments. Unhappiness and loneliness creep into my heart, getting me feel bad at everything, including myself.&lt;/p&gt;

&lt;p&gt;In order to stay upbeat, swimming, running and weight lifting has become the norm in my life style for approximately 5 years. Whilst running and weight lifting are suitable for gloomy days, swimming is always my first choice when days become warm and sunny. I always have positive feelings after every swimming session. My mind is refreshed and my flesh is toned up. Come back to nerdy guys in InfoGroup at VUB. In my second year when going to gym, I surprisingly saw a nerdy and skinny guy of InfoGroup signed up for a gym class. Many days later, I saw more members of that group went to the gym. It was once again surprisingly and entertaining to me when I saw such change. I have no clue about it but I was more happy to chit chat with them in gym class rather than in computer room.&lt;/p&gt;
</description>
        <pubDate>Thu, 25 Feb 2016 09:52:14 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/02/25/conquer-unhappiness/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/02/25/conquer-unhappiness/</guid>
      </item>
    
      <item>
        <title>Full-text search with Apache Lucene</title>
        <description>&lt;p&gt;Apache Lucene is a de facto open source library targeting search and search- related tasks such as indexing or querying.&lt;/p&gt;

&lt;h3 id=&quot;how-lucene-indexes-documents-algorithmically&quot;&gt;1. How Lucene indexes documents algorithmically&lt;/h3&gt;

&lt;p&gt;Lucene uses the &lt;em&gt;inverted index data structure&lt;/em&gt; to index documents. The indexing strategy of Lucene is different from others when it creates segments and merges them, rather than uses a single index. Typically, an index segment consists of a dictionary index, a term dictionary and a posting dictionary.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/indexing-lucene.png&quot; alt=&quot;The index diagram with the merge factor b is 3 &quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The index diagram with the merge factor b is 3 
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Figure 1&lt;/strong&gt; illustrates an example of an index, where the merge factor equals three. In the example, there are 14 documents in the collection. Lucene repeatedly creates a segment for a document and periodically merges a group of 3 documents. The process is similar when Lucene keeps merging a group of 3 segments until there is no more segment to merge. After merging, all greyed segments will be removed and in total, Lucene merged 5 times after the indexing finishes. The approach of merging and deleting segments is considerably useful as the document collection does not change frequently. More importantly, the segment will never be modified. Instead, Lucene creates new segments when the document collection changes and later, it merges segments into new ones and deletes the old segments. This strategy ensures there is no conflict between reading writing indexes. Furthermore, this strategy also allows Lucene to avoid complex B-trees to store segments. Instead, all segments will be stored in flat files.&lt;/p&gt;

&lt;h3 id=&quot;extract-text-andindex&quot;&gt;2. Extract text and index&lt;/h3&gt;

&lt;p&gt;Typically we can divide indexing documents into two distinct procedures, extracting text and creating index &lt;strong&gt;(Figure 2)&lt;/strong&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/analyze-lucene.png&quot; alt=&quot;Figure 2. Indexing with Lucene breaks down into three main operations: extracting text from source documents, analyzing it, and saving it to the index [Photo courtesy of Lucene in Action]&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2. Indexing with Lucene breaks down into three main operations: extracting text from source documents, analyzing it, and saving it to the index [Photo courtesy of Lucene in Action]
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In extracting procedure, it is common to use a versatile parser that can extract textual contents from documents. One of widely known tools we can use to parse documents is &lt;a href=&quot;https://tika.apache.org&quot; target=&quot;_blank&quot;&gt;Apache Tika&lt;/a&gt;. When parsing completes, we will have an input stream that needs to be indexed. It is the second procedure of our index process – creating an index.&lt;/p&gt;

&lt;p&gt;More often than not, we do not index everything we retrieve from the input stream. We instead need to apply &lt;strong&gt;analysis process&lt;/strong&gt; where we will discard unimportant terms from the textual content (e.g., punctuations, stopping words). In Lucene, the analysis process is handled by &lt;code class=&quot;highlighter-rouge&quot;&gt;Analyzer&lt;/code&gt;. The framework already provides handy and powerful analysers that we can use without customising in a majority of problems.&lt;/p&gt;

&lt;p&gt;After analysing, the textual contents are ready to be indexed.&lt;/p&gt;

&lt;h3 id=&quot;building-an-index&quot;&gt;&lt;strong&gt;3. Building an index&lt;/strong&gt;&lt;/h3&gt;

&lt;p&gt;In this section, we will look at how we can create a new index and add documents to it.&lt;/p&gt;

&lt;h4 id=&quot;constructan-index-writer&quot;&gt;3.1 Construct an index writer&lt;/h4&gt;

&lt;p&gt;In Lucene, the index is created and maintained by &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriter&lt;/code&gt;. The IndexWriter constructors can be defined in various ways but most often we simply need to supply&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Directory d&lt;/code&gt; where the index will be stored&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriterConfig conf&lt;/code&gt; holds all the configuration that is used to construct an &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriter&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the code below, we can see that I have define an instance of &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexWriterConfig&lt;/code&gt; with a specialised Analyzer`. The purpose is I want to analyse documents in a particular way that suits my personal needs (e.g., I want to keep specific words that built-in analysers will certainly remove).&lt;/p&gt;

&lt;p&gt;In the given code sample, we use &lt;code class=&quot;highlighter-rouge&quot;&gt;FSDirectory&lt;/code&gt; . There are other alternative solutions, such as &lt;code class=&quot;highlighter-rouge&quot;&gt;SimpleFSDirectory&lt;/code&gt; , &lt;code class=&quot;highlighter-rouge&quot;&gt;NIOFSDirectory&lt;/code&gt; .&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;cm&quot;&gt;/**
 * Create an Indexer using the given directory to store index files, and the
 * directory need to be indexed.
 *
 * */&lt;/span&gt;
&lt;span class=&quot;kd&quot;&gt;public&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;Indexer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anIndexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aDataDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
	&lt;span class=&quot;c1&quot;&gt;// set place to store indexes&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;indexDir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;anIndexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

	&lt;span class=&quot;c1&quot;&gt;// set directory has to be indexed&lt;/span&gt;
	&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;dataDir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aDataDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;;&lt;/span&gt;

	&lt;span class=&quot;k&quot;&gt;try&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// create Directory to store indexes, use FSDirectory.open&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// to automatically pick the most suitable directory implementation&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Directory&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FSDirectory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;File&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indexDir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create an instance of Analyzer with my own custom analyzer&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyCustomAnalyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create configuration for new IndexWriter&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;IndexWriterConfig&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexWriterConfig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;LUCENE_46&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

		&lt;span class=&quot;c1&quot;&gt;// create IndexWriter, which can create new index, or&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// adds, removes, or updates documents in the index&lt;/span&gt;
		&lt;span class=&quot;c1&quot;&gt;// however, it can not read or search&lt;/span&gt;
		&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;writer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexWriter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dir&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;conf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;catch&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;IOException&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;e&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;printStackTrace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
	&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;modelling-document&quot;&gt;3.2 Modelling document&lt;/h4&gt;

&lt;p&gt;In the framework textual content is modelled using the class &lt;code class=&quot;highlighter-rouge&quot;&gt;Document&lt;/code&gt; . This class allows to model a document instance with numerous attributes where for every attribute we can also define using the class &lt;code class=&quot;highlighter-rouge&quot;&gt;Field&lt;/code&gt; .&lt;/p&gt;

&lt;p&gt;In the sample code below, I have define an &lt;code class=&quot;highlighter-rouge&quot;&gt;Document&lt;/code&gt; instance with a number of attributes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Document ID&lt;/strong&gt;: because in practice we may have thousands of documents, I have used base64 to generate unique IDs for documents. Notice that we supply the &lt;code class=&quot;highlighter-rouge&quot;&gt;StringField&lt;/code&gt; constructor with the variable &lt;code class=&quot;highlighter-rouge&quot;&gt;Field.Store.YES&lt;/code&gt; , which means the values of this attributes will be stored in index files. More importantly, we &lt;strong&gt;do not index &lt;/strong&gt;the &lt;code class=&quot;highlighter-rouge&quot;&gt;StringField&lt;/code&gt; by default.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;File path&lt;/strong&gt;: will be stored&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Content field&lt;/strong&gt;: as you may think, we need to index the content field for further full-text search. However, we may or may not store it in the index files. It is a matter of choice. In this particular case, I choose to store the field.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// docId&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;// generated by base64 of filepath&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;String&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BaseEncoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;base64&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getCanonicalPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;().&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getBytes&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Charsets&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;US_ASCII&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;ID_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;id&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Store&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;YES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// filepath&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;FILE_PATH_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;
		&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;getCanonicalPath&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;Store&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;YES&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;));&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// content field&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;TokenStream&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;analyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tokenStream&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;CONTENT_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;StringReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
				&lt;span class=&quot;n&quot;&gt;handler&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;toString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()));&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contentField&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Field&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;MyDocumentIndexedProperties&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;CONTENT_FIELD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt;
		&lt;span class=&quot;n&quot;&gt;AnalyzerUtils&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;tokenStreamToString&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;TYPE_STORED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;ts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;add&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;contentField&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h4 id=&quot;index-document&quot;&gt;3.3 Index document&lt;/h4&gt;

&lt;p&gt;While Lucene consists many components, it, at the same time, uses such extremely simple and intuitive methods to abstract the complex structures that it is fascinating seeing how it is simple to perform indexing.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Document&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;();&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//modelling document in the same manner as section 3.2&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//...&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//add document to the index&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;addDocument&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;doc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;//close the index when finish&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;//which also means we commit the index to be ready to store&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;writer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;close&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;full-text-search&quot;&gt;4. Full-text search&lt;/h3&gt;

&lt;p&gt;Until now we have an index that can be used to search over. To initialise a search, we must input a search query, which can be a keyword or even a wild card. In Lucene, the search query will firstly be handled by &lt;code class=&quot;highlighter-rouge&quot;&gt;QueryParser&lt;/code&gt; where the query will be parsed into meaningful structure, called &lt;code class=&quot;highlighter-rouge&quot;&gt;Query&lt;/code&gt; . After that, the &lt;code class=&quot;highlighter-rouge&quot;&gt;IndexSearcher&lt;/code&gt; will consume &lt;code class=&quot;highlighter-rouge&quot;&gt;Query&lt;/code&gt; instance in order to look up over the index.&lt;/p&gt;

&lt;div class=&quot;language-java highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;// searcher&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;IndexSearcher&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searcher&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;IndexSearcher&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;this&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;indexReader&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// query parser&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;QueryParser&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;QueryParser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;LUCENE_46&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;content&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;new&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;SimpleAnalyzer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;());&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// parsing query&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Query&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;q&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parser&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;parse&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;....&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;// look up and return top 10 relevant documents&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Topdocs&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;docs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;searcher&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;na&quot;&gt;search&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;query&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;);&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h3 id=&quot;bottom-line&quot;&gt;5. Bottom line&lt;/h3&gt;

&lt;p&gt;So far we have seen a “hello world” example of using Lucene to perform full-text search. In real project, we can expect there are a number of possibilities where we need to tweak Lucene so that it can serve our search purpose at a higher level. An example scenario is when we need to support &lt;strong&gt;like-this search&lt;/strong&gt; where the system allows the user to search textual contents that are similar to the keywords. Theoretically we can measure &lt;strong&gt;the similarity &lt;/strong&gt;by using distance measure methods, such as Euclidean distance measure, cosine distance measure, or Manhattan distance measure, etc. Another scenario is when we can create a system that supports &lt;strong&gt;synonym search &lt;/strong&gt;where we retrieve not only document contains the search keywords but also documents that has synonyms of keywords.&lt;/p&gt;

&lt;p&gt;As said, in spite of the simplicity, Lucene adds a tremendous amount of support in building a powerful search tool. In the future post, I may discuss how we can combine both Apache Lucene and Apache Mahout (a machine learning framework) in order to index a corpus and use that index to perform clustering. Stay tuned!&lt;/p&gt;
</description>
        <pubDate>Sat, 16 Jan 2016 21:36:49 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/01/16/full-text-search-with-apache-lucene/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/01/16/full-text-search-with-apache-lucene/</guid>
      </item>
    
      <item>
        <title>Google Reader replacement</title>
        <description>&lt;p&gt;Lately I am not feeling like I am using time wisely when it is quite often I can see myself wasting a huge amount of time on consuming a large amount of information, among which were mostly garbage. I have done several contemplations regarding to this matter to see how I would done to improve such situation. And I come up with an idea of building a hub of filtered news. It would be exactly the same as Google Reader, which is already shut down for a period of time, but the difference is that this hub will be self-hosted.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/tiny-tiny-rss.png&quot; alt=&quot;My Tiny Tiny RSS homepage&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    My Tiny Tiny RSS homepage
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;At the moment there are some open source applications that can be alternatives to Google Reader, among them I think Tiny Tiny RSS would be a top candidate. TTS is lightweight, and easy-to-set-up. I have started to use it for a couple of hours, and I couldn’t say how satisfied I am.&lt;/p&gt;

&lt;p&gt;It can be downloaded &lt;a href=&quot;https://tt-rss.org/gitlab/fox/tt-rss/wikis/home&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Wed, 13 Jan 2016 05:21:57 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/01/13/google-reader-replacement/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/01/13/google-reader-replacement/</guid>
      </item>
    
      <item>
        <title>Invited talk at Swiss Post Solution Vietnam</title>
        <description>&lt;p&gt;Recently, I was invited by a friend of mine, who works at Swiss Post Solution Vietnam, to deliver a training course on the topic of Machine Learning. Despite the fact that the training is very short, approximately 9 hours in total, but it does not prevent me from conveying essential concepts that who is interested in Machine Learning should know. During two training sessions, I’d introduced linear regressions, logistic regression, neural networks as well as clustering (k-means algorithm in specific). It was a pity that people couldn’t have chance to get their hands dirty on the implementation in order to see how those concepts can be easily implemented in Octave.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/hai-swisspost.jpg&quot; alt=&quot;Hai talking about Big Data processing framework&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Hai talking about Big Data processing framework
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/ngoc-swisspost.png&quot; alt=&quot;Just a tour of machine learning&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Just a tour of machine learning
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;After all, what I really concern is not only how people evaluate my presentation but what I have learnt during the time period I had spent effort to prepare. It was also great to talk and have lunches with people at the company, including ones who have very mature experiences in the field.&lt;/p&gt;

&lt;p&gt;I start to miss a very cozy atmosphere of the company.&lt;/p&gt;
</description>
        <pubDate>Tue, 12 Jan 2016 21:26:07 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/01/12/invited-talk-at-swiss-post-solution-vietnam/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/01/12/invited-talk-at-swiss-post-solution-vietnam/</guid>
      </item>
    
      <item>
        <title>To hold the sun</title>
        <description>&lt;p&gt;Previously self-helping books are not of my interests. I rarely paid my attention to any book of such type. I neither had any intention to pour my thoughts and feelings onto a book review. I rather prefer to keep it myself.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/to-hold-the-sun.jpg&quot; alt=&quot;&amp;#8220;To hold the sun&amp;#8221; by Chas Watkins&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    &amp;#8220;To hold the sun&amp;#8221; by Chas Watkins
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;“To hold the sun” is the first self-helping book I’ve ever read. I like neither the book cover nor the title. It would definitely fail me if I were sort of person who judges books by their covers. Though the content is positively worth reading, it might attract even more readers if the book possessed aesthetically pleasing cover that is capable of reflecting inner values it wants to convey.&lt;/p&gt;

&lt;p&gt;In a nutshell, the book is a self-narrative book of a typical uninspiring white-collar worker who is being stuck of reality where he has to to pay bills and house rent on a daily basis, and thus desires an escape to a new place. This unstimulating character got an unexpected trip to Roatan, a largest island of Honduras where he will meet a special person who has given strong impact on his life that he would never think of. It is a clique beginning for a self-helping book but what comes in later make the book worth reading. The book barely portrays the main character throughout the course of further conversations between him and the special person he meets. To resolve life problems of him is the ultimate goal of those conversations. There are many things to be named, including how our brains works scientifically, how to trick the brain in order to live a life to its fullest, or how mediation can help. Those conversations are compelling to follow and I was even able to feel I am the main character at different times during reading the book.&lt;/p&gt;

&lt;p&gt;There is many parts of the book I vote. One of the best parts is when the special person elaborates his idea that the reality ain’t the reality. Indeed the reality is built by innermost feelings and thoughts of individuals by which we can have different opinions towards a same object we observe. We can only live life to its fullest only if we understand how our brains and the reality intersect.&lt;/p&gt;

&lt;p style=&quot;text-align: justify;&quot;&gt;
  I found the book not plainly a self-helping book but also a self-realising. There are many parts of the books I&amp;#8217;ve already known or read beforehand but I rarely notice them. Years ago I have started to learn such things and one of habits that I often do is to remind of myself that happiness is NOW, not tomorrow, and I really enjoy how I&amp;#8217;ve started to change. I highly recommend the book to anyone who is questing for not only motivations but also profound changes to his or her life.
&lt;/p&gt;
</description>
        <pubDate>Sat, 19 Dec 2015 22:16:10 +1100</pubDate>
        <link>http://127.0.0.1:4000/2015/12/19/to-hold-the-sun/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2015/12/19/to-hold-the-sun/</guid>
      </item>
    
  </channel>
</rss>

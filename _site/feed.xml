<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>newbiettn</title>
    <description>Standing on the shoulders of giants</description>
    <link>http://127.0.0.1:4000</link>
    <atom:link href="http://127.0.0.1:4000/feed.xml" rel="self" type="application/rss+xml" />
    
      <item>
        <title>Prelude to feedforward neural networks (part 4)</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#regularisation-in-neural-networks&quot;&gt;III. Regularisation in neural networks&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#benefits-of-regularisation&quot;&gt;2. Benefits of regularisation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#lagrange-multipliers&quot;&gt;2. Lagrange multipliers&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#equality-constraints&quot;&gt;2.1. Equality constraints&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#inequality-constraints&quot;&gt;2.2. Inequality constraints&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#constrained-cost-function-minimization&quot;&gt;2.3. Constrained cost function minimization&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#ell-1-and-ell-2-regularisations&quot;&gt;3. $\ell$-$1$ and $\ell$-$2$ regularisations&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiment&quot;&gt;4. Experiment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iii-regularisation-in-neural-networks&quot;&gt;III. Regularisation in neural networks&lt;/h4&gt;

&lt;h5 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h5&gt;

&lt;p&gt;&lt;em&gt;Overfitting&lt;/em&gt; prefers to the problem, where a machine learning model is able to perform (very) well on the training data, but (very) poorly on the unseen data. There are 3 typical methods to conquer overfitting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Increasing the amount of the data&lt;/strong&gt; This method is considered as the most effective technique to prevent overfitting. Unfortunately, data is not always available, and often expensive to acquire.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Reducing the dimension of the data&lt;/strong&gt; This method is technically preferred as _feature selection} in machine learning, where we only use a subset of features to build the model.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Adding regularisation term&lt;/strong&gt; We can use &lt;em&gt;regularisation&lt;/em&gt; to add additional complexity to the cost function, which can help to prevent overfitting.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Suppose $J_0$ is the original cost function of the model, we can have the modified cost function $J$ of the model by adding the regularisation component. The new cost function is given by&lt;/p&gt;

&lt;p&gt;\begin{equation}
J = J_0  + \lambda R(\theta)
\end{equation}&lt;/p&gt;

&lt;p&gt;where $R(\theta)$ is the &lt;em&gt;regularisation term&lt;/em&gt;, whose strength can be governed by $\lambda$.&lt;/p&gt;

&lt;p&gt;There are many regularisation techniques, however, within the scope of this writing, we will primarily focus on two particular techniques, namely $\ell$-$1$ and $\ell$-$2$ regularisation.&lt;/p&gt;

&lt;h5 id=&quot;benefits-of-regularisation&quot;&gt;2. Benefits of regularisation&lt;/h5&gt;

&lt;p&gt;It is already known that the purpose of the minimisation of the cost function is to find the set of coefficients $\theta$ that produce the cheaper cost function. However, it is often that our optimisation could result in an ``exploded’’ set of coefficients with high variance, making our model poorly fit to the unseen data. In other words, we would say our model is overfitted.&lt;/p&gt;

&lt;p&gt;In our case, our model demonstrates traits of overfitting because its coefficients have high variance. This symptom exists because we impose no restriction on coefficients, allowing them to freely obtain any possible values that simply will give minimal value to the cost function. Hence, to reduce overfitting, we might want to &lt;em&gt;regularise&lt;/em&gt; coefficients while learning. Our idea is mathematically demonstrated by&lt;/p&gt;

&lt;p&gt;\begin{align}
\operatorname*{arg\,min}_\theta J(\theta) \quad s.t. \quad R(\theta) \leq 0
\end{align}&lt;/p&gt;

&lt;p&gt;The expression $(2)$ remarks the idea of finding possible values of $\theta$ at which the cost function $J(\theta)$ is minimal while satisfying $g(\theta) \leq 0$.&lt;/p&gt;

&lt;p&gt;Although $(1)$ and $(2)$ seems different, they are equivalent to each other. We can show that by rewriting $(2)$ using &lt;em&gt;Lagrange multipliers&lt;/em&gt; in the following section.&lt;/p&gt;

&lt;h5 id=&quot;lagrange-multipliers&quot;&gt;2. Lagrange multipliers&lt;/h5&gt;
&lt;p&gt;Before we show how to rewrite and optimise $(2)$ using Lagrange multipliers, we will briefly discuss it. In a nutshell, Lagrange multipliers is a mathematical method used to solve equality/inequality constrained optimisation of differentiable functions. In this section, we will discuss the naive intuition behind Lagrange multipliers and how it can be used to solve optimisation problems. For more explanations focusing on intuition, it is advised to read (Klein, 2004; Smith, 2004).&lt;/p&gt;

&lt;h6 id=&quot;equality-constraints&quot;&gt;2.1. Equality constraints&lt;/h6&gt;
&lt;p&gt;Suppose we have two distinct functions $f(\theta)$ and $g(\theta)$ and we want to find extreme values of $f(\theta)$ while satisfying $g(\theta) = 0$. The geometric intuition behind Lagrange multipliers is that we can only find extreme points that satisfy conditions when two contours of $f$ and $g$ are &lt;em&gt;tangent (not intersection)&lt;/em&gt;. When they are tangent, the gradient vectors of $f$ and $g$ are &lt;em&gt;parallel&lt;/em&gt;, which can be expressed by&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(\theta) &amp;amp;= \lambda \nabla g(x)
\end{align}&lt;/p&gt;

&lt;p&gt;where $\lambda$ is the Lagrangian multipliers.&lt;/p&gt;

&lt;p&gt;The equation $(3)$ is the centre idea. The &lt;em&gt;Lagrangian&lt;/em&gt; equation can be written based on that&lt;/p&gt;

&lt;p&gt;\begin{align}
\mathcal{L}(\theta, \lambda) = f(\theta) - \lambda g(\theta)
\end{align}&lt;/p&gt;

&lt;p&gt;From $(4)$ we can find extreme points by solving the following equation&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla \mathcal{L}(\theta, \lambda) = 0
\end{align}&lt;/p&gt;

&lt;h6 id=&quot;inequality-constraints&quot;&gt;2.2. Inequality constraints&lt;/h6&gt;

&lt;p&gt;In the previous section, we have the condition that $g(\theta) = 0$. When we are no longer interested in the usual constraint $g(\theta) = 0$, but $g(\theta) \leq 0$ or $g(\theta) \geq 0$, we need to apply Lagrange multipliers in a slightly different way.&lt;/p&gt;

&lt;p&gt;Similar to $(3)$, we still have&lt;/p&gt;

&lt;p&gt;\begin{align}
\nabla f(\theta) &amp;amp;= \lambda \nabla g(\theta)
\end{align}&lt;/p&gt;

&lt;p&gt;But now we have additional constraints for $\lambda$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
g(\theta) \geq 0 \Rightarrow \lambda \geq 0 \\
g(\theta) \leq 0 \Rightarrow \lambda \leq 0
\end{align}&lt;/script&gt;

&lt;p&gt;Given $(7)$ and $(8)$, we can find the extreme points that satisfy the inequality constraints by solving the following equations&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\nabla \mathcal{L}(\theta, \lambda) = 0 \\
\text{if} \quad g(\theta) \geq 0, \lambda \geq 0 \\
\text{if} \quad g(\theta) \leq 0, \lambda \leq 0
\end{align}&lt;/script&gt;

&lt;h6 id=&quot;constrained-cost-function-minimization&quot;&gt;2.3. Constrained cost function minimization&lt;/h6&gt;
&lt;p&gt;Until now we already see how Lagrangian multipliers can be used to solve optimisation problems of differential functions with either equality or inequality constraints. Now we will see how Lagrangian can be applied to a particular case, where we want to optimise the cost function $J(\theta)$ while constraining coefficients $\theta$ by certain conditions. We can elaborate our idea by the expression&lt;/p&gt;

&lt;p&gt;\begin{align}
\operatorname*{arg\,min}_\theta J(\theta) \quad &amp;amp;s.t. \quad R(\theta) \leq c
\end{align}&lt;/p&gt;

&lt;p&gt;where $c$ is a constant. By moving everything to one side, we can have&lt;/p&gt;

&lt;p&gt;\begin{align}
(10) \Leftrightarrow \operatorname*{arg\,min}_\theta J(\theta) \quad &amp;amp;s.t. \quad c - R(\theta) \geq 0
\end{align}&lt;/p&gt;

&lt;p&gt;Now we can write a Lagrangian of $(11)$. Note that $\lambda$ is now constrained by $\lambda \geq 0$ in the same manner as $(8)$.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\mathcal{L}(\theta, \lambda) &amp;= J(\theta) - \lambda (c - R(\theta)) \\
\Leftrightarrow \mathcal{L}(\theta, \lambda) &amp;= J(\theta) + \lambda \big[R(\theta) - c \big]
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Compare $(1)$ and $(13)$, we can see that &lt;em&gt;the regularised cost function is essentially a constrained optimisation problem, and $\lambda$ - the regularisation parameter to govern the regularisation term - is indeed the Lagrangian multiplier&lt;/em&gt;.&lt;/p&gt;

&lt;h6 id=&quot;ell-1-and-ell-2-regularisations&quot;&gt;3. $\ell$-$1$ and $\ell$-$2$ regularisations&lt;/h6&gt;

&lt;p&gt;$\ell$-$1$ regularisation is &lt;em&gt;the sum of the absolute value of weights&lt;/em&gt;, which can be given by&lt;/p&gt;

&lt;p&gt;\begin{align}
R(\theta) = \lVert \theta \rVert_1 = \sum_{i=1}^{n} | \theta_i|
\end{align}&lt;/p&gt;

&lt;p&gt;$\ell$-$2$ regularisation is _the sum of the square of weights}, which can be given by&lt;/p&gt;

&lt;p&gt;\begin{align}
R(\theta) = \lVert \theta \rVert_2^2 = \sum_{i=1}^{n} \theta_i^2
\end{align}&lt;/p&gt;

&lt;p&gt;Characteristics of $\ell$-$1$ and $\ell$-$2$ regularisation techniques in comparison.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sparsity&lt;/strong&gt; $\ell$-$1$ exhibits a trait that it tends to create a &lt;em&gt;sparse coefficient vector&lt;/em&gt;, whereas $\ell$-$2$ does not. In case of $\ell$-$1$ regularisation, we can imagine that the shape of the constraint function in 2D plane would be a rectangular, and &lt;em&gt;in most of the time&lt;/em&gt;, the contour of the cost function $J_0$ would be tangent to the constraint &lt;em&gt;at a corner&lt;/em&gt;, which would be the extreme point that $J_0$ is minimal while the constraint is satisfied (Figure 1). At that extreme point, we easily see that one of the coefficient is 0 (in a 2D plane). That may explain why $\ell$-$1$ tends to create sparse vectors. In contrast, in $\ell$-$2$ regularisation, because the constraint function does not have sharp points, generally the tangent point will not be on an axis. Thus, the coefficients in $\ell$-$2$ regularisation will be non-zero.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/sparsity_explaination_regularization.png&quot; alt=&quot;Figure 1: Visualisation of the error and constraint functions of l-1 (left) and l-2 (right) (James, Witten, Hastie, &amp;amp; Tibshirani, 2013)&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1: Visualisation of the error and constraint functions of l-1 (left) and l-2 (right) (James, Witten, Hastie, &amp;amp; Tibshirani, 2013)
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Feature selection&lt;/strong&gt; $\ell$-$1$ regularisation is also considered as a feature selection method, because it leads to sparse coefficient vectors (i.e., if a feature has the zero value of weight, it is removed from the model).&lt;/p&gt;

&lt;h5 id=&quot;experiment&quot;&gt;4. Experiment&lt;/h5&gt;

&lt;p&gt;In this experiment, we will use the Iris data again to demonstrate how regularisation helps constraint coefficients, which results in a regulated cost function and less overfitting model. Figure 2, 3 and 4 evidently show that without the regularisation term, the cost function and the accuracy of both training and testing are severely vacillating.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/cost_l2_vs_no_reg.png&quot; alt=&quot;Figure 2: The cost function with l-2 regularisation and none&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2: The cost function with l-2 regularisation and none
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/training_accuracy_l2_vs_no_reg.png&quot; alt=&quot;Figure 3: The training accuracy with l-2 regularisation and none&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 3: The training accuracy with l-2 regularisation and none
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/testing_accuracy_l2_vs_no_reg.png&quot; alt=&quot;Figure 4: The testing function with l-2 regularisation and none&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 4: The testing function with l-2 regularisation and none
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Below is the R code implementation of the experiment.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/0c23e5d3ea50284f7bcc062c7cdaf7c9.js&quot;&gt;&lt;/script&gt;

&lt;h5 id=&quot;references&quot;&gt;References&lt;/h5&gt;

&lt;p&gt;James, G., Witten, D., Hastie, T., &amp;amp; Tibshirani, R. (2013, June). An Introduction to Statistical Learning. Berlin, Germany: Springer.&lt;/p&gt;

&lt;p&gt;Klein, D. (2004). Lagrange Multipliers Without Permanent Scarring. Online. Retrieved from &lt;a href=&quot;http://www.cs.berkeley.edu/∼klein/papers/lagrange- multipliers.pdf&quot;&gt;http://www.cs.berkeley.edu/∼klein/papers/lagrange- multipliers.pdf&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Smith, B. T. (2004, June). Lagrange Multipliers Tutorial in the Context of Support Vector Machines. Online. Retrieved from &lt;a href=&quot;http://www.engr.mun. ca/∼baxter/Publications/LagrangeForSVMs.pdf&quot;&gt;http://www.engr.mun. ca/∼baxter/Publications/LagrangeForSVMs.pdf&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 02 Feb 2017 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2017/02/02/regularisation/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2017/02/02/regularisation/</guid>
      </item>
    
      <item>
        <title>Prelude to feedforward neural networks (part 3)</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#posterior-probabilities-with-softmax-and-cross-entropy&quot;&gt;III. Posterior probabilities with softmax and cross-entropy&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#softmax&quot;&gt;1. Softmax&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cross-entropy&quot;&gt;2. Cross-entropy&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#neural-network-training&quot;&gt;2.1. Neural network training&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#output-layer&quot;&gt;2.1.1. Output layer&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#hidden-layer&quot;&gt;2.1.2. Hidden layer&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#softmax-and-sigmoid-in-neural-networks&quot;&gt;3. Softmax and sigmoid in neural networks&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#derivative-of-the-softmax-function&quot;&gt;3.1. Derivative of the softmax function&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#training&quot;&gt;3.2. Training&lt;/a&gt;
            &lt;ul&gt;
              &lt;li&gt;&lt;a href=&quot;#output-layer&quot;&gt;3.2.1. Output layer&lt;/a&gt;&lt;/li&gt;
              &lt;li&gt;&lt;a href=&quot;#hidden-layer&quot;&gt;3.2.2. Hidden layer&lt;/a&gt;&lt;/li&gt;
            &lt;/ul&gt;
          &lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiment&quot;&gt;4. Experiment&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#data-description&quot;&gt;4.1. Data description&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#classification-model&quot;&gt;4.2. Classification model&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#results&quot;&gt;4.3. Results&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;iii-posterior-probabilities-with-softmax-and-cross-entropy&quot;&gt;III. Posterior probabilities with softmax and cross-entropy&lt;/h4&gt;

&lt;h5 id=&quot;softmax&quot;&gt;1. Softmax&lt;/h5&gt;

&lt;p&gt;The &lt;em&gt;softmax&lt;/em&gt; function, also known as the &lt;em&gt;normalised exponential&lt;/em&gt;, is a function that maps a vector of real values to a vector whose values are normalised in the range (0, 1) and sum of them equals to 1 (Bishop, 2006). The function is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\varphi (s_j) = \frac{e^{s_j}}{\sum_{k=1}^{K} e^{s_k}}&lt;/script&gt;

&lt;p&gt;where $K$ is the number of possible outcomes. In multiclass classification models, $K$ is the number of classes that we want to classify.&lt;/p&gt;

&lt;p&gt;By analysing the formula of the softmax function, we can see why the softmax function allows to obtain normalised exponential values.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The numerator and denominator of the function are always greater than 0, thus, the output vector of the function is always positive.&lt;/li&gt;
  &lt;li&gt;The denominator of the function is the sum of $K$ corresponding classes, hence, the output values of the function are in the range (0, 1) and sum up to 1.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; Suppose we have a input vector $v$ of 4 elements, $v = \begin{bmatrix} 1 \ 2 \ 3 \ 4 \end{bmatrix}$, we can have the transformed vector $\varphi (v)$ by using the softmax, $\varphi (v) = \begin{bmatrix} 0.032 \ 0.087 \ 0.236 \ 0.644 \end{bmatrix}$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Relationship with logistic regression&lt;/strong&gt; Softmax and logistic regression have a relationship where softmax is a generalised version of logistic regression. In case we use softmax as the activation function of the output layer for a binary classification task, softmax behaves similarly to logistic regression.&lt;/p&gt;

&lt;p&gt;To demonstrate that mathematically, suppose we have a neural networks where we use logistic regression to perform a binary classification task. In the output layer of 1 single neuron, we would have the output $y$ given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
y = \frac{1}{1 + e^{-s_1}} = \frac{1}{1 + \frac{1}{e^{s_1}}} &amp;= \frac{e^{s_1}}{1 + e^{s_1}} \\
&amp;= \frac{e^{s_1}}{e^{s_0} + e^{s_1}} \tag*{$s_0 = 0$}\\
&amp;= \frac{e^{s_1}}{\sum_{k=0}^{1}e^{s^k}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;This equation demonstrates that a neural network with the softmax output layer of 2 neurons, where the weight of one output neuron always equals 0, behaves similarly to the neural network of the logistic regression output layer of 1 single neuron.&lt;/p&gt;

&lt;h5 id=&quot;cross-entropy&quot;&gt;2. Cross-entropy&lt;/h5&gt;

&lt;p&gt;The term &lt;em&gt;cross-entropy&lt;/em&gt; origins from information theory and is later applied to machine learning for model training. In training neural networks, the cross-entropy cost function is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;J(\theta) = -\frac{1}{m} \bigg[ \sum_{j=1}^{k} 1\{y^{(i)} = j\} \, \log \frac{e^{s_j}}{\sum_{i = 1}^{k} e^{s_k}} \bigg]&lt;/script&gt;

&lt;p&gt;where $k$ is the number of output neurons.&lt;/p&gt;

&lt;p&gt;Note that in the equation above, $1{.}$ is the &lt;em&gt;indicator function&lt;/em&gt;, such that &lt;script type=&quot;math/tex&quot;&gt;1\{ true \, statement \} = 1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;1\{ false \, statement \} = 0&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Intuition&lt;/strong&gt; In information theory, the concept of entropy was firstly introduced by (Shannon, 2001). Shannon’s entropy refers to the contained information in a message, basically represented by &lt;strong&gt;bits&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;An intuitive example is that suppose we need to inform our friends whenever we see a car or a bicycle crossing by our place. We can only use binary numbers and hence, for each kind of vehicles, we would use different bit sequences. We might want to encode our bit sequences in a way that we can send messages effectively. Should we encode them the same? No, we should not because the number of bicycles on the street is certainly smaller than the number of cars, hence, we would certainly broadcast bit sequences of car more frequent than bit sequences of bicycle. Those optimal number of bits we choose to encode are called &lt;em&gt;entropy&lt;/em&gt;, which can given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(x) = \sum_{i} x_i \log \frac{1}{x_i} = - \sum_{i} x_i \log x_i&lt;/script&gt;

&lt;p&gt;In our example, suppose the number of cars is 256 times higher than the number of bicycles. We would have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;b_{car} = \log_2 \frac{1}{256 * p_{bicycle}} = \log_2 \frac{1}{256} + \log_2 \frac{1}{p_{bicycle}} = b_{bicycle} - 8&lt;/script&gt;

&lt;p&gt;This result indicates that we should encode the bit sequences of car using 8 bits less than the number of bit sequences of bicycle.&lt;/p&gt;

&lt;p&gt;Recently, we are able to find the optimal number of bits or entropy by exploiting the underlying &lt;em&gt;factual&lt;/em&gt; distribution of cars and bicycles on the street. What if we are clueless about that distribution and thus, we would guess. By using that guess, we would subsequently encode our bit sequences in a different manner, such that&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(x, \hat{x}) = \sum_{i} x_i \log \frac{1}{\hat{x}_i} = - \sum_{i} x_i \log \hat{x}_i&lt;/script&gt;

&lt;p&gt;In this case, we would encode our bits using guessing values &lt;script type=&quot;math/tex&quot;&gt;\hat{x}_i&lt;/script&gt;, where $x$ is the desired values. And the number of bits we encode using $H(x, \hat{x})$ is known as &lt;em&gt;cross-entropy&lt;/em&gt;. We can intuitively see that our cross-entropy is always higher than entropy, and they only equal to each other if we can make perfect guesses.&lt;/p&gt;

&lt;h6 id=&quot;neural-network-training&quot;&gt;2.1. Neural network training&lt;/h6&gt;

&lt;h6 id=&quot;output-layer&quot;&gt;2.1.1. Output layer&lt;/h6&gt;

&lt;p&gt;Suppose we have&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$e_j$ is the error at the neuron $j$.&lt;/li&gt;
  &lt;li&gt;$d_j$ is the desire output at neuron $j$.&lt;/li&gt;
  &lt;li&gt;$E$ is the error of the neuron $j$ for all inputs of a class $C$.&lt;/li&gt;
  &lt;li&gt;$s_j$ is the sum of outputs $y_i$ of neurons from the previous layer $i$.&lt;/li&gt;
  &lt;li&gt;$y_j$ is the output of neuron $j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the MSE cost function, we define $e_j = d_j - y_j$ and $E = \frac{1}{2} \sum e^2_j$. In the cross-entropy cost function, we will adjust both of them, such that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
e_j &amp; = - \big[ d_j \, \ln y_j + (1 - d_j) \ln (1 - y_j) \big] \tag{1} \\
E &amp; = \sum_{j \in C} e_j &amp; \tag{2} \\
s_j &amp; = \sum_{i = 0} w_{ji} y_i \tag{3} \\
y_j &amp; = \varphi(s_j) \tag{4}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial e_j} \cdot \frac{\partial e_j}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}} \tag{5}
\end{align}&lt;/script&gt;

&lt;p&gt;Each component in ${(6)}$ can be resolved as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
{(2)} \quad &amp;\Rightarrow \quad \frac{\partial E}{\partial e_j} = 1 \\
{(1)} \quad &amp;\Rightarrow \quad \frac{\partial e_j}{\partial y_j} = - \frac{d - y}{y(1-y)} \\
{(4)} \quad &amp;\Rightarrow \quad \frac{\partial y_j}{\partial s_j} = \varphi &#39; (s_j) \\
{(3)} \quad &amp;\Rightarrow \quad \frac{\partial s_j}{\partial w_{ji}} = y_i \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Based on that, we can rewrite ${(5)}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
{(5)} \quad \Rightarrow \quad \frac{\partial E}{\partial w_{ji}} = - \bigg[ \frac{d - y}{y(1-y)} \cdot \varphi &#39; (s_j) \cdot y_i \bigg] \tag{6}
\end{align}&lt;/script&gt;

&lt;p&gt;We already know that the activation function has many types, however, when $\varphi (\cdot)$ is the sigmoid function and the cost function is cross-entropy, a beautiful and interpretable form of the gradient of training error, which we will see, appears.&lt;/p&gt;

&lt;p&gt;As said, when $\varphi (\cdot)$ is the sigmoid function, we can obtain $\varphi ‘ (\cdot) = \big[ \varphi (\cdot) (1 - \varphi (\cdot) \big]$. Thus, ${(6)}$ will become&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
 {(6)} \quad \Rightarrow \quad \frac{\partial E}{\partial w_{ji}} &amp; = - \bigg[ \frac{d - y}{y(1-y)} \cdot \varphi(s_j) (1-\varphi(s_j)) \cdot y_i \bigg] \\
 &amp;= - \bigg[ \frac{d - \varphi (s_j))}{\varphi (s_j) ( 1 - \varphi (s_j))} \cdot \varphi(s_j) (1-\varphi(s_j)) \cdot y_i \bigg] \\
 &amp;= - \bigg[ (d - y_j) \cdot y_i \bigg] \tag{7}
 \end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;${(7)}$ demonstrates how the weights change with respect to the cross-entropy cost function. By examining ${(7)}$, we can see the changing of the weights in the network is solely dependent on the difference between the actual output and the output of the network. That observation can lead to a more interesting conclusion that the speed of learning of our neurons is decided by the degree of the mistake they make. The bigger the mistake, the faster our neurons learn.&lt;/p&gt;

&lt;h6 id=&quot;hidden-layer&quot;&gt;2.1.2. Hidden layer&lt;/h6&gt;

&lt;p&gt;${(7)}$ is the gradient of the training error at the output layer. Next, we will find out $\frac{\partial E}{\partial w_{ji}}$ in case neuron $j$ is the hidden neuron. Again, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial E}{\partial w_{ji}} = \sum_{k=1}^{nout} \frac{\partial E}{\partial e_k} \cdot \frac{\partial e_k}{\partial y_k} \cdot \frac{\partial y_k}{\partial s_k} \cdot \frac{\partial s_k}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}} \tag{8}
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E}{\partial e_k} &amp;= \frac{\partial}{\partial e_k} {e_k} = 1 \tag{9} \\
\frac{\partial e_k}{\partial y_k} &amp;= - \frac{d - y_k}{y_k(1-y_k)} \tag{10} \\
\frac{\partial y_k}{\partial s_k} &amp;= \frac{\partial}{\partial s_k} \varphi (s_k) = \varphi (s_k) \big[1-\varphi (s_k) \big] \tag{11} \\
\frac{\partial s_k}{\partial y_j} &amp;= \frac{\partial}{\partial y_j} \sum w_{kj} y_j = w_{kj} \tag{12} \\
\frac{\partial y_j}{\partial s_j} &amp;= \frac{\partial}{\partial s_j} \varphi (s_j) = \varphi (s_j) \big[1-\varphi (s_j) \big] \tag{13} \\
\frac{\partial s_j}{\partial w_{ji}} &amp;= \frac{\partial}{\partial w_{ji}} \sum w_{ji} y_i = y_i \tag{14} \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Plug (9), (10), (11), (12), (13), (14) into (8), we would have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E}{\partial w_{ji}} &amp;= \sum_{k=1}^{nout} - \frac{d - y_k}{y_k(1-y_k)} \cdot \varphi (s_k) \big[1-\varphi (s_k) \big] \cdot w_{kj} \cdot \varphi (s_j) \big[1-\varphi (s_j) \big] \cdot y_i \\
&amp;= \sum_{k=1}^{nout} - (d-y_k) \cdot w_{kj} \cdot \varphi (s_j) \big[ 1-\varphi (s_j)\big] \cdot y_i \\
&amp;= \sum_{k=1}^{nout} \frac{\partial E}{\partial w_{kj}} \cdot w_{kj}(1-y_j)y_i \tag{15}
\end{align} %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;softmax-and-sigmoid-in-neural-networks&quot;&gt;3. Softmax and sigmoid in neural networks&lt;/h5&gt;

&lt;p&gt;In this section, we will see how we can train a neural network of 3 layers that uses softmax in the output layer, and sigmoid in the hidden layer (Figure 1). Compare to neural networks that use sigmoid function in the output layer, this network, with the support of softmax function, will be able to produce posterior probability in multi-class classification tasks.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/illustration_multilayer_perceptron_2.png&quot; alt=&quot;Figure 1: 3-layer neuron network, where the hidden nodes $a_j$ $(j = 1,..., m)$ use sigmoid and the output nodes $y_i$ $(i = 1,..., k)$ uses softmax&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1: 3-layer neuron network, where the hidden nodes $a_j$ $(j = 1,..., m)$ use sigmoid and the output nodes $y_i$ $(i = 1,..., k)$ uses softmax
  &lt;/p&gt;
&lt;/div&gt;

&lt;h6 id=&quot;derivative-of-the-softmax-function&quot;&gt;3.1. Derivative of the softmax function&lt;/h6&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x) = \frac{g(x)}{h(x)} \Rightarrow f&#39;(x) = \frac{g&#39;(x)h(x) - h&#39;(x)g(x)}{[h(x)]^2}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
g(s_j) &amp;= e^{s_j} \Rightarrow \frac{\partial g}{\partial s_i} = \begin{cases} 0  &amp; \text{if } i \neq j \\ e^{s_j}  &amp; \text{if } i = j \end{cases} \\
h(s_j) &amp;= \sum_{k=1}^{N} e^{s_k} \Rightarrow \frac{\partial h}{\partial s_i} = e^{s_j} \tag*{for both $i = j$ and $i \neq j$}
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;If $i \neq j$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial f}{\partial s_i} =  \frac{-e^{s_i}e^{s_j}}{\big( \sum \big) ^2} &amp;= - \frac{e^{s_i}}{\sum} \cdot \frac{e^{s_j}}{\sum} = - \varphi(s_i) \varphi(s_j)
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;If $i = j$&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial f}{\partial s_i} &amp;= \frac{\big( e^{s_i}\sum \big) - \big({e^{s_i}e^{s_i}} \big)}{\big( \sum \big) ^2}  = \frac{e^{s_i} \big( \sum - e^{s_i} \big)}{\big( \sum \big) ^2} \\
&amp;= \frac{e^{s_i}}{\sum} \cdot \bigg( 1 - \frac{e^{s_i}}{\sum} \bigg) = \varphi(s_j) \big[1 - \varphi(s_j)\big]
\end{align} %]]&gt;&lt;/script&gt;

&lt;h6 id=&quot;training&quot;&gt;3.2. Training&lt;/h6&gt;

&lt;h6 id=&quot;output-layer-1&quot;&gt;3.2.1. Output layer&lt;/h6&gt;

&lt;p&gt;Suppose we have&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$E$ is the error of the neuron $j$ for all inputs of a class $C$.&lt;/li&gt;
  &lt;li&gt;$s_j$ is the sum of outputs $y_i$ of neurons from the previous layer $i$.&lt;/li&gt;
  &lt;li&gt;$y_j$ is the output of neuron $j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the MSE cost function, we define $e_j = d_j - y_j$ and $E = \frac{1}{2} \sum e^2_j$. In the cross-entropy cost function, we will adjust both of them, such that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
E &amp; = - \sum_{j=1}^{nclass} t_j \log y_j \\
s_j &amp; = \sum_{i = 0} w_{ji} y_i \\
y_j &amp; = \varphi(s_j)
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}}
\end{align}&lt;/script&gt;

&lt;p&gt;Each component in ${(6)}$ can be resolved as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E}{\partial s_j}  &amp;=  - \frac{\partial}{\partial s_j} \bigg[ t_j  \log y_j + \sum_{j \neq i}^{nclass} t_j \log y_j \bigg] \\
&amp;= - \frac{t_j}{y_j} \frac{\partial y_j}{\partial s_j} - \sum_{i \neq j} \frac{t_i}{y_i} \frac{\partial y_i}{\partial s_j} \\
&amp;= - \frac{t_j}{y_j} y_j (1-y_j) + \sum_{i \neq j} \frac{t_i}{y_i} y_i y_j \\
&amp;= -t_j (1-y_j) + \sum_{i \neq j} t_i y_j \\
&amp;= -t_j + t_j y_j + \sum_{i \neq j} t_i y_j \\
&amp;= -t_j + \sum_{i = 1} t_i y_j \\
&amp;= -t_j + y_j \sum_{i = 1} t_i \\
&amp;= y_j - t_j \tag*{because $\sum_{j = 1} t_j = 1$}
\end{align} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial s_j}{\partial w_{ji}} =  y_i
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}} = (y_j - t_j) y_i
\end{align}&lt;/script&gt;

&lt;h6 id=&quot;hidden-layer-1&quot;&gt;3.2.2 Hidden layer&lt;/h6&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial E}{\partial s_k} = y_k - t_k
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial s_k}{\partial y_j} = \frac{\partial \sum w_{kj}y_j}{\partial y_j} = w_{kj}
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial y_j}{\partial s_j} = y_j (1 - y_j)
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{align}
\frac{\partial s_j}{\partial w_{ji}} = y_i
\end{align}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\frac{\partial E}{\partial w_{ji}} &amp;= \frac{\partial E}{\partial s_k} \cdot \frac{\partial s_k}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}}  \\
&amp;= (y_k - t_k) \cdot (w_{kj}) \cdot y_j (1-y_j) \cdot y_i
\end{align} %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;experiment&quot;&gt;4. Experiment&lt;/h5&gt;

&lt;p&gt;Up to now, we already know how to train 3-layer neural network having the sigmoid function in the hidden layer, and the softmax function in the output layer. In this section, we will use derived formulas to train that kind of network for a multi-category classification task.&lt;/p&gt;

&lt;h6 id=&quot;data-description&quot;&gt;4.1. Data description&lt;/h6&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_with_labels.jpg&quot; alt=&quot;Figure 2: An Iris flower with labels&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2: An Iris flower with labels
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The dataset used in this experiment is the Iris flower dataset (Anderson, 1935; Fisher, 1936). This dataset contains 50 samples of 3 species of Iris (i.e., &lt;em&gt;setosa&lt;/em&gt;, &lt;em&gt;virginica&lt;/em&gt;, and &lt;em&gt;versicolor&lt;/em&gt;). All of 3 species are characterised by 4 features, including the &lt;em&gt;sepal length&lt;/em&gt;, &lt;em&gt;sepal width&lt;/em&gt;, &lt;em&gt;petal length&lt;/em&gt;, and &lt;em&gt;petal width&lt;/em&gt;. Figure 2 is a typical Iris flower and Figure 3 provides a visual description of those features.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_features_boxplot.png&quot; alt=&quot;Figure 3: Iris flower dataset description of 4 features: sepal length, sepal width, petal length, and petal width&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 3: Iris flower dataset description of 4 features: sepal length, sepal width, petal length, and petal width
  &lt;/p&gt;
&lt;/div&gt;

&lt;h6 id=&quot;classification-model&quot;&gt;4.2. Classification model&lt;/h6&gt;

&lt;p&gt;The classification model used in this experiment is a 3-layer neuron network:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Input layer:&lt;/strong&gt; 5 inputs including $x_0$ is the bias unit and $x_1, x_2, x_3, x_4$ are for 4 features.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Hidden layer:&lt;/strong&gt; 5 sigmoid activation units $a_j$ $(j = 0,…,4)$.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Output layer:&lt;/strong&gt; 3 softmax outputs $y_j$ $(j = 1, 2, 3)$ according to 3 predicted classes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Figure 4 is a model representation of the network and Figure 5 is a signal flow model of the network.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_classification_model.png&quot; alt=&quot;Figure 4: 3-layer neuron network, where the hidden nodes $a_j$ $(j = 1,..., 4)$ use sigmoid and the output nodes $y_i$ $(i = 1,..., 3)$ uses softmax. Note that the bias term $x_0$ equals to 1. $\Theta^{(1)}$ and $\Theta^{(2)}$ are weights.&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 4: 3-layer neuron network, where the hidden nodes $a_j$ $(j = 1,..., 4)$ use sigmoid and the output nodes $y_i$ $(i = 1,..., 3)$ uses softmax. Note that the bias term $x_0$ equals to 1. $\Theta^{(1)}$ and $\Theta^{(2)}$ are weights.
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_signal_flow_model.png&quot; alt=&quot;Figure 5: A description of signal flow of the neural network&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 5: A description of signal flow of the neural network
  &lt;/p&gt;
&lt;/div&gt;

&lt;h6 id=&quot;results&quot;&gt;4.3. Results&lt;/h6&gt;
&lt;p&gt;As said, what we can expect from this model is the posterior probability. An example of the model output is that suppose we have a particular observation belongs to 1st class (e.g., setosa), the encoded output of that observation in this experiment would be &lt;script type=&quot;math/tex&quot;&gt;Y = \begin{bmatrix} 1 \\ 0 \\ 0 \end{bmatrix}&lt;/script&gt;. Corresponding with that desired output $Y$, the expected prediction of the classification model would be like &lt;script type=&quot;math/tex&quot;&gt;\hat{Y} = \begin{bmatrix} 0.6609379 \\ 0.1009062 \\ 0.2381559 \end{bmatrix}&lt;/script&gt;, where 0.6609379 represents the probability that this particular observation would be setosa is 66.09%.&lt;/p&gt;

&lt;p&gt;Figure 6 and 7 are the visual reports of the experiment. In this experiment, we divided the data into 80% for training and 20% for testing. Note that:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;The learning converges early&lt;/strong&gt;. Both Figure 7 demonstrates that both the training and testing accuracy reach their optimal points somewhere before 5000 epochs. When analysing experiment procedures, we can conclude that this early convergence can be achieved due to:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Optimal initialisation of weights&lt;/strong&gt; we initialised weights with mean equals 0 and standard devidation equals $1/\sqrt[2]{n_w}$ where $n_w$ is the number of connecting weights to the neuron. By initialising the weights by this method, we have better chance in avoiding the learning saturation of the model (Haykin, 2009).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Maximisation of learning&lt;/strong&gt; for every epoch, we shuffled the training data, which makes the learning process very healthy (Haykin, 2009).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Overfitting is controlled very well&lt;/strong&gt; Thanks to &lt;em&gt;L2 regularisation&lt;/em&gt;, it seems that overfitting is avoided in the experiment. Figure 7 shows that the accuracy of the testing set is essentially better than the accuracy of the training set. Besides, we can also observe that the accuracy of the testing set slightly correlates with the accuracy of the training set in this experiment.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_accuracy_vs_loss.png&quot; alt=&quot;Figure 6: Accuracy vs loss in the Iris flower classification model&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 6: Accuracy vs loss in the Iris flower classification model
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/iris_testing_vs_training.png&quot; alt=&quot;Figure 7: Training accuracy vs Testing accuracy in the Iris flower classification model&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 7: Training accuracy vs Testing accuracy in the Iris flower classification model
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;The code snippet of the experiment implementation in R.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/561f01c6e5cf5e958c1af0c92e2ef58c.js&quot;&gt;&lt;/script&gt;

&lt;h5 id=&quot;references&quot;&gt;References&lt;/h5&gt;

&lt;p&gt;Anderson, E. (1935, September). The Species Problem in Iris. Annals of the Missouri Botanical Garden, 23(3), 457–469. doi:10.2307/2394164&lt;/p&gt;

&lt;p&gt;Bishop, C. M. (2006). Pattern Recognition and Machine Learning. Berlin, Germany: Springer.&lt;/p&gt;

&lt;p&gt;Fisher, R. A. (1936, September). The use of multiple measurements in taxonomic problems. Annals of Eugenics, 7(2), 179–188. doi:10.1111/j.1469-1809. 1936.tb02137.x&lt;/p&gt;

&lt;p&gt;Haykin, S. (2009). Neural Networks and Learning Machines (3rd ed.). Upper Saddle River, NJ, USA: Pearson.&lt;/p&gt;

&lt;p&gt;Shannon, C. E. (2001, July). A Mathematical Theory of Communication. Bell Labs Technical Journal, 27(4), 623–656. doi:10.1002/j.1538-7305.1948. tb01338.x&lt;/p&gt;
</description>
        <pubDate>Mon, 23 Jan 2017 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2017/01/23/softmax-and-cross-entropy/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2017/01/23/softmax-and-cross-entropy/</guid>
      </item>
    
      <item>
        <title>Prelude to feedforward neural networks (part 2)</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#ii-from-single-layer-perceptrons-to-multilayer-perceptrons&quot;&gt;II. From single-layer perceptrons to multilayer perceptrons&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#activation-function&quot;&gt;2. Activation function&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#inside-hidden-neurons&quot;&gt;3. Inside hidden neurons&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#feedforward&quot;&gt;4. Feedforward&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#backpropagation&quot;&gt;5. Backpropagation&lt;/a&gt;
        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#error-signal&quot;&gt;5.1. Error signal&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#summary-of-backpropagation&quot;&gt;5.2. Summary of backpropagation&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiment&quot;&gt;6. Experiment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;ii-from-single-layer-perceptrons-to-multilayer-perceptrons&quot;&gt;II. From single-layer perceptrons to multilayer perceptrons&lt;/h4&gt;

&lt;h5 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h5&gt;
&lt;p&gt;In Part 1, we have discussed about threshold functions and perceptrons, the most simplest form of neural nets that is capable of learning but still constrained when it is only applicable to linear separable problems. In Part 2, our topic is the second form of neural networks, particularly &lt;strong&gt;feedforward multilayer perceptrons&lt;/strong&gt;. In details, we will discuss: (i) why this type of networks can solve nonlinear classification tasks, (ii) why it is called feedforward, and (iii) how a multilayer perceptron can learn from the data.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/illustration_multilayer_perceptron.png&quot; alt=&quot;Fig 1. Illustration of a multilayer perceptron&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. Illustration of a multilayer perceptron
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;A typical feedforward artificial neural network has three different layers, including the input layer, the hidden layer, and the output layer (Figure 1). The second layer is intuitively called the hidden layer because it is masked by both the input and output layers. Whilst a node in the input and output layers is corresponding to a predictor and an output, nodes in the hidden layer employ &lt;strong&gt;an activation function&lt;/strong&gt;, about which we will discuss later.&lt;/p&gt;

&lt;h5 id=&quot;activation-function&quot;&gt;2. Activation function&lt;/h5&gt;
&lt;p&gt;Activation function enables nonlinear mapping between inputs and outputs of neurons. Figure 2 illustrates examples of activation functions by which we can have nonlinear output from an initial linear input. Activation functions has three essentially attributes. First, they are nonlinear. Second, their outputs are constrained within certain boundaries, for example $(0, 1)$ in the case of the logistic functions, making them appropriate for classification tasks where we are only interested in nominal outputs. Finally, they are continually differentiable. This characteristic allows to optimise the cost function using differential equations, such as gradient descent.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/examples_activation_function.png&quot; alt=&quot;Fig 2. Examples of activation functions&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 2. Examples of activation functions
  &lt;/p&gt;
&lt;/div&gt;

&lt;h5 id=&quot;inside-hidden-neurons&quot;&gt;3. Inside hidden neurons&lt;/h5&gt;
&lt;p&gt;To understand how feedforward operates, it is important to understand the internal characteristic of hidden nodes. Similar to threshold neurons, a hidden neuron consists of a transfer function $\Sigma$ and an activation function $\varphi$ (instead of a threshold function in case of a threshold neuron). Mathematically, $\Sigma$ is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\Sigma &amp; = \Theta^T \mathbf{x} \\
\varphi &amp; = \varphi(\Sigma) = \varphi(\Theta^T \mathbf{x})
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\Theta = \begin{bmatrix} \theta_1 &amp;amp; \theta_2 \cdots \theta_n \end{bmatrix}^T$ is a vector of weights. Thus, we can calculate the output $y$ of the hidden node in Figure 3 by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = \varphi(\Theta^T \mathbf{x})&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/a_hidden_neuron.png&quot; alt=&quot;Fig 3. A hidden neuron consists of a transfer function $\sum$ and an activation function $\varphi$&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 3. A hidden neuron consists of a transfer function $\sum$ and an activation function $\varphi$
  &lt;/p&gt;
&lt;/div&gt;

&lt;h5 id=&quot;feedforward&quot;&gt;4. Feedforward&lt;/h5&gt;
&lt;p&gt;The term &lt;strong&gt;feedforward&lt;/strong&gt; is devised to help distinguish from another kinds of neural networks where connections between nodes can be circles, such as &lt;strong&gt;recurrent neural networks&lt;/strong&gt;. In feedforward neural networks, connections flows straightforwardly from the input layers to the output layer.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/a_multilayer_perceptron.png&quot; alt=&quot;Fig 4. A multilayer perceptron&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 4. A multilayer perceptron
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;In Figure 4, we have multilayer perceptrons with $n$ inputs, $m$ activation nodes and $k$ outputs. Denote $\mathbf{\Theta^{(1)}}$ and $\mathbf{\Theta^{(2)}}$ matrices of weights between inputs-hidden layers and hidden-output layers. $\mathbf{\Theta^{(1)}}$ and $\mathbf{\Theta^{(2)}}$ are given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{\Theta^{(1)}} =
\begin{bmatrix}
	\Theta^{(1)}_1 \\
	\Theta^{(1)}_2 \\
	\Theta^{(1)}_3 \\
	\cdots \\
	\Theta^{(1)}_{m}
\end{bmatrix}
=
\begin{bmatrix}
	\theta^{(1)}_{11} &amp; \theta^{(1)}_{12} &amp; \theta^{(1)}_{13} &amp; \cdots &amp; \theta^{(1)}_{1n} \\
	\theta^{(1)}_{21} &amp; \theta^{(1)}_{22} &amp; \theta^{(1)}_{23} &amp; \cdots &amp; \theta^{(1)}_{2n} \\
	\theta^{(1)}_{31} &amp; \theta^{(1)}_{32} &amp; \theta^{(1)}_{33} &amp; \cdots &amp; \theta^{(1)}_{3n} \\
	\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
	\theta^{(1)}_{m1} &amp; \theta^{(1)}_{m2} &amp; \theta^{(1)}_{m3} &amp; \cdots &amp; \theta^{(1)}_{mn} \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{\Theta^{(2)}} =
\begin{bmatrix}
	\Theta^{(2)}_1 \\
	\Theta^{(2)}_2 \\
	\Theta^{(2)}_3 \\
	\cdots \\
	\Theta^{(2)}_{m}
\end{bmatrix}
=
\begin{bmatrix}
	\theta^{(2)}_{11} &amp; \theta^{(2)}_{12} &amp; \theta^{(2)}_{13} &amp; \cdots &amp; \theta^{(2)}_{1m} \\
	\theta^{(2)}_{21} &amp; \theta^{(2)}_{22} &amp; \theta^{(2)}_{23} &amp; \cdots &amp; \theta^{(2)}_{2m} \\
	\theta^{(2)}_{31} &amp; \theta^{(2)}_{32} &amp; \theta^{(2)}_{33} &amp; \cdots &amp; \theta^{(2)}_{3m} \\
	\cdots &amp; \cdots &amp; \cdots &amp; \cdots &amp; \cdots \\
	\theta^{(2)}_{m1} &amp; \theta^{(2)}_{m2} &amp; \theta^{(2)}_{m3} &amp; \cdots &amp; \theta^{(2)}_{km} \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;where $\Theta^{(1)}_j$ $(j = 1,…, m)$ is a weight vector of connections from $a^{(2)}_j$ to $\mathbf{x^{(1)}}$ and $\Theta^{(2)}_i$ $(i = 1,…, k)$ is a weight vector of connections from $a^{(3)}_i $ to $\mathbf{a^{(2)}}$.&lt;/p&gt;

&lt;p&gt;From $\mathbf{\Theta^{(1)}}$ and $\mathbf{\Theta^{(2)}}$, we can calculate $\mathbf{a^{(2)}}$ and $\mathbf{a^{(3)}}$ respectively&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbf{a^{(2)}} &amp; = \varphi (\mathbf{\Theta^{(1)}} \cdot \mathbf{x^T}) \qquad \qquad \qquad \, \, {\mathbf{(1)}} \\
\mathbf{a^{(3)}} &amp; = \varphi (\mathbf{\Theta^{(2)}} \cdot \mathbf{a^{(2)}}) \qquad \qquad \qquad {\mathbf{(2)}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finally, the output $\mathbf{y}$ is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathbf{y} = \mathbf{a^{(3)}}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; Suppose we want to classify a set of data points generated by XOR operation.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/graph_data_points_xor.png&quot; alt=&quot;Fig 5. Graph of data points given in Table 1&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 5. Graph of data points given in Table 1
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 5 is a graph of XOR data points. Look at the graph, we can intuitively see that we cannot classify them using linear model, such as perceptron. Instead, in this example, we will use the feedforward neural network model. Particularly, we can classify such dataset using a feedforward neural network with 3 inputs, 3 hidden neurons, and 1 output (Figure 6). Note that $x^{(1)}_0$ is a bias unit.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/sample_multilayer_perceptron_xor.png&quot; alt=&quot;Fig 6. A sample multilayer perceptron to classify XOR gate&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 6. A sample multilayer perceptron to classify XOR gate
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Suppose we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{\Theta^{(1)}} =
\begin{bmatrix}
	-3.9628723 &amp; -8.481784 &amp; 7.694714 \\
	2.9412002 &amp; -7.297706 &amp; 9.293525 \\
	0.2272201 &amp; 4.008646 &amp; 3.443932
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\mathbf{\Theta^{(2)}} =
\begin{bmatrix}
	14.52022 &amp; -13.81069 &amp; 7.022461 \\
\end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;We begin with the first observation $x_1 = 1, x_2 = 1$, and set bias unit $x_0 = 1$. Thus, $\mathbf{x} = \begin{bmatrix} 1 &amp;amp; 1 &amp;amp; 1 \end{bmatrix}$. From &lt;strong&gt;(1)&lt;/strong&gt; and &lt;strong&gt;(2)&lt;/strong&gt;, we can obtain $\mathbf{a^{(2)}}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbf{a^{(2)}} &amp; = \varphi (\mathbf{\Theta^{(1)}} \cdot \mathbf{x^T}) \\
&amp; = \varphi \Bigg(
\begin{bmatrix}
	-3.9628723 &amp; -8.481784 &amp; 7.694714 \\
	2.9412002 &amp; -7.297706 &amp; 9.293525 \\
	0.2272201 &amp; 4.008646 &amp; 3.443932
\end{bmatrix}
\begin{bmatrix}
	1 \\ 1 \\ 1
\end{bmatrix}
\Bigg) \\
&amp;\approx \varphi \Bigg(
\begin{bmatrix}
	-4.749942  \\ 4.937019 \\ 7.679798
\end{bmatrix}
\Bigg)
\approx
\begin{bmatrix}
	0.008577978  \\ 0.992875171 \\ 0.999538145
\end{bmatrix}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;From $\mathbf{a^{(2)}}$, we now can obtain $\mathbf{a^{(3)}}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\mathbf{a^{(3)}} &amp; = \varphi (\mathbf{\Theta^{(2)}} \cdot \mathbf{a^{(2)}}) \\
&amp;= \varphi \Bigg(
\begin{bmatrix}
	14.52022 &amp; -13.81069 &amp; 7.022461 \\
\end{bmatrix}
\begin{bmatrix}
	0.008577978  \\ 0.992875171 \\ 0.999538145
\end{bmatrix}
\Bigg) \\
&amp;\approx \varphi (\begin{bmatrix}  -6.568518 \end{bmatrix})
\approx  0.001401908
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus we have $y = 0.001401908 \approx 0$ for $x_1 = 1, x_2 = 1$. Repeat the whole process with new inputs, we can obtain $y = 0.9988276$ with $x_1 = 1, x_2 = 0$, $y = 0.9992674$ with $x_0 = 1, x_2 = 1$, and finally $y = 0.0001311898$ with $x_0 = 0, x_2 = 0$.&lt;/p&gt;

&lt;h5 id=&quot;backpropagation&quot;&gt;5. Backpropagation&lt;/h5&gt;

&lt;p&gt;Suppose we have&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$e_j$ is the error at the neuron $j$.&lt;/li&gt;
  &lt;li&gt;$d_j$ is the desire output at neuron $j$.&lt;/li&gt;
  &lt;li&gt;$E$ is the error of the neuron $j$ for all inputs of a class $C$.&lt;/li&gt;
  &lt;li&gt;$s_j$ is the sum of outputs $y_i$ of neurons from the previous layer $i$.&lt;/li&gt;
  &lt;li&gt;$y_j$ is the output of neuron $j$.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/signal_flow_neuron_j.png&quot; alt=&quot;Fig 7. Graph of signal flow of a neuron j in the network&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 7. Graph of signal flow of a neuron j in the network
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We can represent them mathematically&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{aligned}
e_j = d_j - y_j \qquad \qquad \qquad {\mathbf{(1)}} \\
E = \frac{1}{2} \sum_{j \in C} e^2_j \qquad \qquad \qquad {\mathbf{(2)}} \\
s_j = \sum_{i = 0} w_{ji} y_i \qquad \qquad \qquad {\mathbf{(3)}} \\
y_j = \varphi(s_j) \qquad \qquad \qquad {\mathbf{(4)}}
\end{aligned}&lt;/script&gt;

&lt;p&gt;Our goal is to find $\Delta w_{ji}$ such that we can find the local minima of $E$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\Delta w_{ji} = - \eta \frac{\partial E}{\partial w_{ji}} \qquad \qquad \qquad {\mathbf{(5)}}&lt;/script&gt;

&lt;p&gt;Thus, to find $\Delta w_{ji}$, we need to find  $\frac{\partial E}{\partial w_{ji}}$. Apply the chain-rule, we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial E}{\partial w_{ji}} = \frac{\partial E}{\partial e_j} \cdot \frac{\partial e_j}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \cdot \frac{\partial s_j}{\partial w_{ji}}  \qquad \qquad \qquad {\mathbf{(6)}}&lt;/script&gt;

&lt;p&gt;Each component in ${\mathbf{(6)}}$ can be resolved as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
{\mathbf{(1)}} \quad &amp;\Rightarrow \quad \frac{\partial E}{\partial e_j} = e_j \\
{\mathbf{(2)}} \quad &amp;\Rightarrow \quad \frac{\partial e_j}{\partial y_j} = -1 \\
{\mathbf{(3)}} \quad &amp;\Rightarrow \quad \frac{\partial y_j}{\partial s_j} = \varphi &#39; (s_j) \\
{\mathbf{(4)}} \quad &amp;\Rightarrow \quad \frac{\partial s_j}{\partial w_{ji}} = y_i \\
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Based on that, we can rewrite ${\mathbf{(6)}}$&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(6)}} \quad \Rightarrow \quad \frac{\partial E}{\partial w_{ji}} = - e_j \cdot \varphi &#39; (s_j) \cdot y_i \qquad \qquad \qquad {\mathbf{(7)}}&lt;/script&gt;

&lt;p&gt;Using ${\mathbf{(7)}}$, we also rewrite ${\mathbf{(5)}}$ by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(5)}} \quad \Rightarrow \quad \Delta w_{ji} = \eta \frac{\partial E}{\partial w_{ji}} = \eta \cdot e_j \cdot \varphi &#39; (s_j) \cdot y_i \qquad \qquad \qquad {\mathbf{(8)}}&lt;/script&gt;

&lt;p&gt;Suppose we have
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta_j &amp;= - \frac{\partial E}{\partial s_j} \\
&amp;= - \frac{E}{\partial e_j} \cdot \frac{\partial e_j}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \\
&amp;= e_j \cdot \varphi &#39; (s_j) \qquad \qquad \qquad \qquad \qquad {\mathbf{(9)}}
\end{aligned} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;Plug ${\mathbf{(9)}}$ into ${\mathbf{(8)}}$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(7)}} + {\mathbf{(8)}} \quad \Rightarrow \quad \Delta w_{ji} = \eta \cdot {\delta_j} \cdot y_i \qquad \qquad \qquad {\mathbf{(10)}}&lt;/script&gt;

&lt;p&gt;Now we have a beautiful form of local gradient $\Delta w_{ji}$, but unfortunately, we still cannot calculate $\Delta w_{ji}$ because $\delta_j$, although in such a beautiful form, still cannot be easily calculated. Thus, our next goal is to find a computable form of $\delta_j$.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/signal_flow_hidden.png&quot; alt=&quot;Fig 8. Graph of signal flow when neuron j is a hidden neuron, neuron k is an output neuron&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 8. Graph of signal flow when neuron j is a hidden neuron, neuron k is an output neuron
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For a neuron $j$, there are two cases. It is either a output neuron or a hidden. If neuron $j$ is a output neuron, we can easily compute $\delta_j$ because we already known the desired output $d_j$ and predicted output $y_j$. Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta_j = e_j \cdot \varphi &#39; (s_j) = (d_j - y_j) \cdot \varphi &#39; (s_j) \qquad \qquad \qquad {\mathbf{(11)}}&lt;/script&gt;

&lt;p&gt;On the other hand, if neuron $j$ is a hidden neuron, we can have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
\delta_j &amp;= - \frac{\partial E}{\partial s_j} = - \frac{\partial E}{\partial y_j} \cdot \frac{\partial y_j}{\partial s_j} \\
&amp;= - \frac{\partial E}{\partial y_j} \cdot \varphi &#39; (s_j) \qquad \qquad \qquad {\mathbf{(12)}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Suppose neuron $k$ is the output neuron, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E = \frac{1}{2} \sum_{k \in C} e^2_k \qquad \qquad \qquad {\mathbf{(13)}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
{\mathbf{(13)}} \quad \Rightarrow \quad \frac{\partial E}{\partial y_j} &amp;= \sum_{k} e_k \cdot \frac{\partial e_k}{\partial y_j} \\
&amp;= \sum_{k} e_k \cdot \frac{\partial e_k}{\partial s_k} \cdot \frac{\partial s_k}{\partial y_j} \qquad \qquad \qquad {\mathbf{(14)}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Note that $e_k = d_k - y_k = d_k - \varphi (s_k)$, thus we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial e_k}{\partial s_k} = - \varphi &#39; (s_k) \qquad \qquad \qquad {\mathbf{(15)}}&lt;/script&gt;

&lt;p&gt;Also, for neuron $k$, we can have $s_k = \sum w_{kj} \cdot y_j$, thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial s_k}{\partial y_j} = w_{kj} \qquad \qquad \qquad {\mathbf{(16)}}&lt;/script&gt;

&lt;p&gt;Plug ${\mathbf{(15)}}$ and ${\mathbf{(16)}}$ into ${\mathbf{(14)}}$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(13)}} + {\mathbf{(14)}} + {\mathbf{(15)}} \quad \Rightarrow \quad \frac{\partial E}{\partial y_j} = - \sum_k e_k \cdot \varphi &#39; (s_k) \cdot w_{kj} \qquad \qquad {\mathbf{(16)}}&lt;/script&gt;

&lt;p&gt;And because neuron $k$ is an output neuron, from ${\mathbf{(11)}}$ we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(10)}} \quad \Rightarrow \quad \delta_k = e_k \cdot \varphi &#39; (s_k) \qquad \qquad \qquad {\mathbf{(18)}}&lt;/script&gt;

&lt;p&gt;Thus, plug ${\mathbf{(18)}}$ into ${\mathbf{(17)}}$, we have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;{\mathbf{(17)}} + {\mathbf{(18)}}\quad \Rightarrow \quad \frac{\partial E}{\partial y_j} = - \sum_k \delta_k \cdot w_{kj} \qquad {\mathbf{(19)}}&lt;/script&gt;

&lt;p&gt;Thus,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
{\mathbf{(12)}} + {\mathbf{(19)}} \quad \Rightarrow \quad \delta_j &amp;= - \frac{\partial E}{\partial y_j} \cdot \varphi &#39; (s_j) \\
&amp;= \varphi &#39; (s_j) \sum_k \delta_k \cdot w_{kj} \qquad {\mathbf{(20)}}
\end{aligned} %]]&gt;&lt;/script&gt;

&lt;p&gt;Finally, we find a comprehensive way to calculate $\delta_j$ when neuron $j$ is either a hidden neuron or output neuron in ${\mathbf{(11)}}$ and ${\mathbf{(20)}}$.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If neuron $j$ is an output neuron, $\delta_j = e_j \cdot \varphi ‘ (s_j) = (d_j - y_j) \cdot \varphi ‘ (s_j) $&lt;/li&gt;
  &lt;li&gt;If neuron $j$ is a hidden neuron, $\delta_j = \varphi ‘ (s_j) \sum \delta_k \cdot w_{kj}$ where $\delta_k$ is of output neuron $k$&lt;/li&gt;
&lt;/ul&gt;

&lt;h6 id=&quot;error-signal&quot;&gt;5.1. Error signal&lt;/h6&gt;
&lt;p&gt;Recall that to make our equations mathematically convenient, we have defined $\delta_j = - \frac{\partial E}{\partial s_j}$. But doing so is not just for mathematical convenience, in fact, $\delta_j$ does carry meanings in its own definition. In the network, $\delta_j$ is called &lt;strong&gt;the error signal&lt;/strong&gt; at neuron $j$, and it indicates how much the error at neuron $j$ varies with respect to the input $s_j$. If we can determine $\delta_j$, which of course we can by using backpropagation, we can subsequently decide new weight values $w$ that can lower the total error $E$.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Backward transmission&lt;/strong&gt; In contrast to feedforward where signals flow from the input layer to the output layer, in backpropagation, signals are transferred in the opposite direction, from the output layer back to the input layers. In particular, the transferred signals is error signals. And that is why the algorithm is named &lt;strong&gt;backpropagation&lt;/strong&gt; in the first place.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/error_signal_flow.png&quot; alt=&quot;Fig 9. Graph of error signal flow when neuron j is a hidden neuron&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 9. Graph of error signal flow when neuron j is a hidden neuron
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 9 depicts a graph of error signal flow for a hidden neuron $j$. First, at the output layer, the error signal $e_k$ of the output neuron $k$ is computed. Since neuron $k$ is an output neuron, the error signal of neuron $k$ can be computed by $\delta_k = e_k \cdot \varphi ‘ (s_j)$. By repeating the process for another output neurons, we can subsequently have $\delta_0$, $\delta_1$, … of the output layer. For every computed error signals, we have a corresponding weights $w_{0j},…, w_{kj}$, we can compute $\sum = \sum {w_{kj}}{\delta_k}$. Finally, we will have $\delta_j = \varphi ‘ (s_j) \sum {w_{kj}}{\delta_k}$.&lt;/p&gt;

&lt;h6 id=&quot;summary-of-backpropagation&quot;&gt;5.2. Summary of backpropagation&lt;/h6&gt;
&lt;p&gt;Backpropagation algorithm can be applied generally by following steps:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Use feedforward to compute corresponding sums of inputs and weights $s$ of every neuron and predicted outputs $y_k$ considering neuron $k$ as output neurons.&lt;/li&gt;
  &lt;li&gt;Compute the error signals $\delta$ for every neurons. If neuron $j$ is an output neuron, then $\delta_j = e_j \cdot \varphi ‘ (s_j) = (d_j - y_j) \cdot \varphi ‘ (s_j) $. If neuron $j$ is a hidden neuron, then $\delta_j = \varphi ‘ (s_j) \sum \delta_k \cdot w_{kj}$ where $\delta_k$ is of output neuron $k$.&lt;/li&gt;
  &lt;li&gt;Update network weights by $\Delta w_{ji} = \eta \cdot \delta_j y_i$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;experiment&quot;&gt;6. Experiment&lt;/h5&gt;
&lt;p&gt;In this section, we will implement a neuron network of three inputs, three hidden neurons and one output to classify the XOR data. The experiment is conducted using R.&lt;/p&gt;

&lt;p&gt;During this experiment, we tried to use different initial weight values $w$, and as a result, sometimes the optimisation of error rate did not converge even after 1000 epochs. Figure 10 depicts two scenarios, one when the error rate drop nearly to 0 after 1000 epochs, one does not. Thus, we can conclude that backpropagation, because employing gradient descent to optimise weights, does not promise that it can find local minima of the error rate. Thus, it is required to have gradient checking method to measure the process.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/error_rates.png&quot; alt=&quot;Fig 10. Error rates varies according the number of epochs&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 10. Error rates varies according the number of epochs
  &lt;/p&gt;
&lt;/div&gt;

</description>
        <pubDate>Tue, 03 Jan 2017 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2017/01/03/feedforward-neural-networks/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2017/01/03/feedforward-neural-networks/</guid>
      </item>
    
      <item>
        <title>Prelude to feedforward neural networks (part 1)</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#i-from-simple-threshold-neurons-to-perceptrons&quot;&gt;I. From simple threshold neurons to perceptrons&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#1-introduction&quot;&gt;1. Introduction&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#2-threshold-neurons&quot;&gt;2. Threshold neurons&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#3-perceptron&quot;&gt;3. Perceptron&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#4-geometric-interpretation&quot;&gt;4. Geometric interpretation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#5-experiment&quot;&gt;5. Experiment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;i-from-simple-threshold-neurons-to-perceptrons&quot;&gt;I. From simple threshold neurons to perceptrons&lt;/h4&gt;

&lt;h5 id=&quot;introduction&quot;&gt;1. Introduction&lt;/h5&gt;
&lt;p&gt;Perceptron is the earliest generation of artificial neural networks studied by Rosenblatt in 1958 (Rosenblatt, 1958). It is a progression of McCulloch-Pitts’s threshold neurons (McCulloch &amp;amp; Pitts, 1943), in which Rosenblatt basically made threshold neurons be able to learn by employing Hebbian learning (Hebb, 1949). In the book “Perceptrons” (Minsky &amp;amp; Papert, 1969), Minsky and Papert highlighted some strengths of perceptrons as well as identified their restrictions. A notable example of perceptrons’ limitations was demonstrated using XOR problem, in which perceptrons are incapable of learning to classify data points produced by the XOR function. The demonstrated limitations were thought applicable to not only perceptrons but also another kinds of neural networks. However, subsequent inventions in neural networks proved that it is not true.&lt;/p&gt;

&lt;h5 id=&quot;threshold-neurons&quot;&gt;2. Threshold neurons&lt;/h5&gt;
&lt;p&gt;In 1940, McCulloch-Pitts developed threshold neurons as an attempt to technically mimic biological neurons. A threshold neuron is characterised by the threshold function, by which inputs are mapped to outputs of either 0 or 1. More importantly, threshold neurons have no learning mechanism, because they have a fixed set of weights.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/threshold_neuron.png&quot; alt=&quot;Fig 1. Illustration of a threshold neuron&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. Illustration of a threshold neuron
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;Figure 1 is an illustration of a threshold neuron whose outputs are either 0 or 1. Inside the neuron, we have two distinct functions $\Sigma$ and $f(\cdot)$, by which we first calculate the sum of weighted inputs $s$ using $\Sigma$, and second the output using the threshold function $f(\cdot)$. We can calculate the output $y$ of the neuron by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y = f(s) = f(\sum_{j=1}^{N} w_jx_j) = f(\mathbf{w^Tx})&lt;/script&gt;

&lt;p&gt;where $f(\cdot)$ is given by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(\cdot) =
  \begin{cases}
      \hfill 1    \hfill &amp; \text{when $s \ge r$, where $r$ is any real number} \\
      \hfill 0 \hfill &amp; \text{otherwise} \\
  \end{cases} %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;perceptron&quot;&gt;3. Perceptron&lt;/h5&gt;

&lt;p&gt;In contrast to threshold neurons, perceptrons are able to adjust their weights during learning. At the beginning, we initialise the weight vector $\mathbf{w}$ by a random weight value ${w^{(0)}}$, and the learning rate $\eta$ by any random value $r$ satisfying $0 \le r \le 1$. We then start to go through every single observation ${x_j}$ of the input vector $\mathbf{x}$ and update $\mathbf{w}$ by the following rule.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Compute the net input $u$ corresponding to $x_j$ such that $u = w^Tx_j$.&lt;/li&gt;
  &lt;li&gt;Compute the output $\hat{y}_j$ given the threshold function $ f(u) = \begin{cases} 0  &amp;amp; \text{if } u &amp;lt; 0 \ 1  &amp;amp; \text{if } u \ge 0 \end{cases}$&lt;/li&gt;
  &lt;li&gt;Compute the error $E$ such that $E = y_j - \hat{y}_j$.&lt;/li&gt;
  &lt;li&gt;Update the weight $w^{(j+1)}$ by $w^{(j+1)} = w^{(j)} + \eta \, E \, x_j$.&lt;/li&gt;
  &lt;li&gt;Repeat steps 1, 2, 3, 4 until $j = N$.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The updating rule of $\mathbf{w}$ can be understood that we only update $\mathbf{w}$ if and only if the predicted output $\hat{y}_j$ is different with the actual output $y_j$. If $E = 1$, we have $w^{(j+1)} = w^{(j)} + \eta \, x_j$. On the other hand, if $E = -1$, we have $w^{(j+1)} = w^{(j)} - \eta \, x_j$. We can summary three possibilities by&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
w^{(j+1)} =
	\begin{cases}
		w^{(j)}  &amp; \text{if $E = 0$, $y_j = \hat{y}_j$} \\
		w^{(j)} - \eta \, x_j  &amp; \text{if $E = -1$, $y_j = 0$, $\hat{y}_j = 1$} \\
		w^{(j)} + \eta \, x_j &amp; \text{if $E = 1$, 	 $y_j = 1$, $\hat{y}_j = 0$}
	\end{cases} %]]&gt;&lt;/script&gt;

&lt;h5 id=&quot;geometric-interpretation&quot;&gt;4. Geometric interpretation&lt;/h5&gt;
&lt;p&gt;The learning algorithm of perceptrons can be understood geometrically. Suppose we have a plane defined by the weight vector $\mathbf{w} \in \mathbb{R}^2$ such that $w_0 + w_1 x_1 + w_2 x_2 = 0$ or $\mathbf{w^Tx} = 0$ where $x_0 = 1$. Since $\mathbf{w^Tx} = 0$, $\mathbf{w}$ is orthogonal to the plane (Figure 2).&lt;/p&gt;

&lt;p&gt;More importantly, the plane $w^Tx = 0$ divides the space into two distinct half-planes, such that given any vector $\mathbf{u}$, we have $\mathbf{u \cdot w} \ge 0$ if $\mathbf{w}$ and $\mathbf{u}$ are on the same half-plane, otherwise $\mathbf{u \cdot w} &amp;lt; 0$. This can be proven by using the formula $\mathbf{u \cdot w} = \lVert u \rVert \cdot \lVert w \rVert \cdot cos \theta$, where $\theta$ is the angle between two vectors. Since $\lVert u \rVert \ge 0$, $\lVert w \rVert \ge 0$, we can conclude that $\mathbf{u \cdot w} \ge 0 $ if $0^{\circ} \le \theta \le 90^{\circ}$. Other other hand, $\mathbf{u \cdot w} &amp;lt; 0 $ if $\theta \ge 90^{\circ}$.&lt;/p&gt;

&lt;p&gt;Based on that, we are now going to interpret the perceptron learning algorithm. When $E = -1$, (i.e., $y_j = 0$, $\hat{y}_j = 1$), two vectors &lt;script type=&quot;math/tex&quot;&gt;\mathbf{u}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\mathbf{w}&lt;/script&gt; should be on &lt;strong&gt;different&lt;/strong&gt; half-planes, but they are not. Thus, the plane needs to be rotated further away on the opposite direction of the vector $\mathbf{u}$ to increase the value of $\theta$. In contrast, when $E = 1$, (i.e., $y_j = 1$, $\hat{y}_j = 0$), $\mathbf{u}$ and $\mathbf{w}$ should be on the &lt;strong&gt;same&lt;/strong&gt; half-plane, but they are not. Thus, the plane need to be rotated closer on the direction of the vector $\mathbf{u}$, making the value of $\theta$ decrease.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/perceptron_visualisation.png&quot; alt=&quot;Fig 2. Visualisation of two-half planes divided by the plane $\mathbf{w^Tx = 0}$&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 2. Visualisation of two-half planes divided by the plane $\mathbf{w^Tx = 0}$
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; A visualised example is provided to illustrate the operation of the perceptron learning algorithm. Given a set of data points in the table below, the perceptron learning function will be used to classify them.&lt;/p&gt;

&lt;p&gt;At the beginning, we set the initial value of $\mathbf{w}$ equal to a random value and the learning rate $\eta$ equal to 1 (Figure 3a).&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;We begin with $x_1$. Because $E = -1$, the plane rotates on the direction of $x_1$, updating $w_1 = w_0 - x_1$ (Figure 3b).&lt;/li&gt;
  &lt;li&gt;At $x_2$, we have $E = 0$, the weight remains the same, thus $w_2 = w_1$ (Figure 3c).&lt;/li&gt;
  &lt;li&gt;At $x_3$, we have $E = 1$, the plane needs to rotate on the direction of $x_3$, thus $w_3 = w_2 + x_3$ (Figure 3d)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Because the perceptron algorithm is unable to classify the data in 1 iteration. We need to repeat the process one more time. Figure 4 visualises how the perceptron learns in the 2nd iteration. At the end of the 2nd iteration, the perceptron converges when all data points are successfully classified.&lt;/p&gt;

&lt;table class=&quot;data_table&quot;&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;$x_1$&lt;/th&gt;
      &lt;th&gt;$x_2$&lt;/th&gt;
      &lt;th&gt;y&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1.5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;-1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/perceptron_iter_1.png&quot; alt=&quot;Fig 3. Changes of $\mathbf{w}$ in the 1st iteration&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 3. Changes of $\mathbf{w}$ in the 1st iteration
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/perceptron_iter_2.png&quot; alt=&quot;Fig 4. Changes of $\mathbf{w}$ in the 2nd iteration&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 4. Changes of $\mathbf{w}$ in the 2nd iteration
  &lt;/p&gt;
&lt;/div&gt;

&lt;h5 id=&quot;experiment&quot;&gt;5. Experiment&lt;/h5&gt;
&lt;p&gt;In this experiment, we are going to use perceptrons to classify the salmon data (Johnson &amp;amp; Wichern, 2007). This experiment is conducted using R. The visualised output of the plane created by the perceptron learning function is given in Figure 5 and 6.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/linear_neuron.gif&quot; alt=&quot;Fig 5. Formation of the decision boundary step by step&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 5. Formation of the decision boundary step by step
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;We also conducted an additional experiment to verify an assumption that if the performance of the perceptron is higher when we increase the number of iterations. Figure 7 depicts the accuracy of the perceptron after numbers of epoch, given that an epoch indicates one pass of all training data points. According to the figure, we can see that the performance of the our model slightly improves from 0 epochs to 30 epochs, but after 30 epochs, it drops hugely. Thus, we can conclude that the performance of the model is not pertaining to the number of epochs. Instead, we have to perform many tests to realise an optimised number of epochs at which our model is at its best.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/perceptron_salmon.png&quot; alt=&quot;Fig 6. Visualisation of classified salmon data using perceptrons&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 6. Visualisation of classified salmon data using perceptrons
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/perceptron_salmon_epochs.png&quot; alt=&quot;Fig 7. Changes of the classification accuracy of perceptrons regarding to the number of epoch&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 7. Changes of the classification accuracy of perceptrons regarding to the number of epoch
  &lt;/p&gt;
&lt;/div&gt;

&lt;h5 id=&quot;references&quot;&gt;References&lt;/h5&gt;

&lt;p&gt;Hebb, D. O. (1949). The Organization of Behavior. New York, NY, USA: John Wiley.
Johnson, R. A. &amp;amp; Wichern, D. W. (2007, April). Applied Multivariate Statistical Analysis (6th ed.). London, UK: Pearson.&lt;/p&gt;

&lt;p&gt;Krizhevsky, A., Sutskever, I., &amp;amp; Hinton, G. E. (2012, December). ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, &amp;amp; K. Q. Weinberger (Eds.), Advances in Neural Information Processing Systems (Stateline, NV, USA) (pp. 1097– 1105). NIPS ’12. Red Hook, NY, USA: Curran Associates. Retrieved from http ://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&lt;/p&gt;

&lt;p&gt;McCulloch, W. S. &amp;amp; Pitts, W. (1943, December). A Logical Calculus of the Ideas Immanent in Nervous Activity. The Bulletin of Mathematical Biophysics, 5(4), 115–133. Retrieved from 10.1007/bf02478259
Minsky, M. &amp;amp; Papert, S. (1969). Perceptrons. Cambridge, MA, USA: MIT press.&lt;/p&gt;

&lt;p&gt;Rosenblatt, F. (1958, November). The Perceptron: a Probabilistic Model for Information Storage and Organization in the Brain. Psychological Review, 65(6), 386–408.&lt;/p&gt;
</description>
        <pubDate>Mon, 02 Jan 2017 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2017/01/02/perceptron/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2017/01/02/perceptron/</guid>
      </item>
    
      <item>
        <title>Multivariate imputation with mice package in R</title>
        <description>&lt;p&gt;This is a research experiment of using &lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; package (Buuren and Groothuis-Oudshoorn, 2011) to impute missing values in multivariate data. Basically, &lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; uses &lt;strong&gt;Fully Conditional Specification (FCS)&lt;/strong&gt; method, whereas &lt;em&gt;chained equation&lt;/em&gt; is considered an alias of that term. According to Liu and De (2015), FCS is more relaxed than the conventional method Joint Modeling (JM) in terms of specifiying distribution of the missing data.&lt;/p&gt;

&lt;p&gt;In this experiment, we will use an open source dataset &lt;strong&gt;Titanic&lt;/strong&gt; (Kaggle, 2012) as the sample data of the experiment.&lt;/p&gt;

&lt;h4 id=&quot;multiple-imputation-vs-single-imputation&quot;&gt;1. Multiple imputation vs Single imputation&lt;/h4&gt;
&lt;p&gt;Both multiple and single imputation techniques are ones that can be used to replace missing values in the data by plausible values. In single imputation, we substitute the missing values of the data by &lt;strong&gt;a single value&lt;/strong&gt;. For example, we can replace missing values of a variable by the most common value, or the &lt;em&gt;mean/median&lt;/em&gt; value (in case that variable is continuous). Multiple imputation is different in a sense that missing values are replaced by &lt;strong&gt;multiple plausible values&lt;/strong&gt;. Using &lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; package, we can even have multiple version of imputed data by trying to impute the data more than one time.&lt;/p&gt;

&lt;h4 id=&quot;notation&quot;&gt;2. Notation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;$\mathbf{Y_j}$ $(j = 1,…, p)$: one of $p$ incomplete variables.&lt;/li&gt;
  &lt;li&gt;The observed and missing parts of $Y_j$ are denoted by $\mathbf{Y_j^{obs}}$ and $\mathbf{Y_j^{mis}}$.
    &lt;ul&gt;
      &lt;li&gt;$Y^{obs} = (Y^{obs},…,Y^{obs}_p)$: observed data in Y.&lt;/li&gt;
      &lt;li&gt;$Y^{mis} = (Y^{mis},…,Y^{mis}_p)$: missing data in Y.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;$\mathbf{m \ge 1}$: the number of imputation.&lt;/li&gt;
  &lt;li&gt;$\mathbf{Q}$: predicted model that would be trained on $Y$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;features&quot;&gt;3. Features&lt;/h4&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; introduces a &lt;em&gt;modular approach&lt;/em&gt; to impute data. The imputation framework offered by &lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; comprises of 3 steps: &lt;em&gt;imputation&lt;/em&gt;, &lt;em&gt;analysis&lt;/em&gt; and &lt;em&gt;pooling&lt;/em&gt; (Figure 1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-1-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt; Impute incomplete data, creating imputed versions of the data $Y^{(1)}$, $Y^{(2)}$, …, $Y^{(m)}$.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;with()&lt;/code&gt; Estimate the predicted model $Q$ on each imputed dataset $Y^{(i)}$, creating multiple versions of the model, which are $\hat{Q}^{(1)}$, $\hat{Q}^{(2)}$,…, $\hat{Q}^{(m)}$&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pool()&lt;/code&gt;   Pool $\hat{Q}^{(1)}$, $\hat{Q}^{(2)}$,…, $\hat{Q}^{(m)}$ into one estimates $\bar{Q}$&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A notable feature of this imputation model is that we use our predicted model of interest $Q$ to analyse the imputed data by ourself. By doing that, we can specifically select the best imputed data that suits our investigation.&lt;/p&gt;

&lt;h4 id=&quot;experiment&quot;&gt;4. Experiment&lt;/h4&gt;

&lt;h4 id=&quot;missing-value-inspection&quot;&gt;4.1. Missing value inspection&lt;/h4&gt;
&lt;p&gt;First and foremost, load the package and data. In this experiment, we will use Titanic data free available from Kaggle.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Load library
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Imputation
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VIM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#aggr, marginplot
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lattice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#stripplot
# Load data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;data/titanic.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;na.strings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Inspect the data structure
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strict.width&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wrap&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## &#39;data.frame&#39;:	891 obs. of  12 variables:
## $ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ...
## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ...
## $ Pclass : int 3 1 3 1 3 3 1 3 3 2 ...
## $ Name : Factor w/ 891 levels &quot;Abbing, Mr. Anthony&quot;,..: 109 191 358 277 16
##    559 520 629 417 581 ...
## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
## $ Age : num 22 38 26 35 35 NA 54 2 27 14 ...
## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ...
## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ...
## $ Ticket : Factor w/ 681 levels &quot;110152&quot;,&quot;110413&quot;,..: 524 597 670 50 473
##    276 86 396 345 133 ...
## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ...
## $ Cabin : Factor w/ 147 levels &quot;A10&quot;,&quot;A14&quot;,&quot;A16&quot;,..: NA 82 NA 56 NA NA 130
##    NA NA NA ...
## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Remove unnecessary columns
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;PassengerId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Cabin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ticket&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NULL&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Summary the data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##     Survived          Pclass          Sex           Age       
##  Min.   :0.0000   Min.   :1.000   female:314   Min.   : 0.42  
##  1st Qu.:0.0000   1st Qu.:2.000   male  :577   1st Qu.:20.12  
##  Median :0.0000   Median :3.000                Median :28.00  
##  Mean   :0.3838   Mean   :2.309                Mean   :29.70  
##  3rd Qu.:1.0000   3rd Qu.:3.000                3rd Qu.:38.00  
##  Max.   :1.0000   Max.   :3.000                Max.   :80.00  
##                                                NA&#39;s   :177    
##      SibSp           Parch             Fare        Embarked  
##  Min.   :0.000   Min.   :0.0000   Min.   :  0.00   C   :168  
##  1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:  7.91   Q   : 77  
##  Median :0.000   Median :0.0000   Median : 14.45   S   :644  
##  Mean   :0.523   Mean   :0.3816   Mean   : 32.20   NA&#39;s:  2  
##  3rd Qu.:1.000   3rd Qu.:0.0000   3rd Qu.: 31.00             
##  Max.   :8.000   Max.   :6.0000   Max.   :512.33             
## &lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Fare = 0 doesn&#39;t make sense, set 0 values in Fare as missing values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Our data contains a lot of missing values. We will use different methods to inspect them. First, we will use &lt;code class=&quot;highlighter-rouge&quot;&gt;md.pattern&lt;/code&gt; method to inspect missing-data patterns.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;md.pattern&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##     Survived Pclass Sex SibSp Parch Embarked Fare Age    
## 705        1      1   1     1     1        1    1   1   0
## 169        1      1   1     1     1        1    1   0   1
##   7        1      1   1     1     1        1    0   1   1
##   2        1      1   1     1     1        0    1   1   1
##   8        1      1   1     1     1        1    0   0   2
##            0      0   0     0     0        2   15 177 194&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Let’s intepret information from the retrieved table. Note that in the table, 0 represents missing in the data. Look at the table by row, the table tells us that we have 705 complete rows, 169 rows where only &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; is missing, &lt;code class=&quot;highlighter-rouge&quot;&gt;7&lt;/code&gt; rows where only &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; is missing, 2 rows where &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; is missing, and 8 rows where both &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; are missing. Vertically, the tables tells us that &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; has 2 missing values, &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; 15, and &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; 177. In overall, we have 194 missing values in the data, and 169 + 7 + 2 + 8 = 186 incomplete rows.&lt;/p&gt;

&lt;p&gt;Besides &lt;code class=&quot;highlighter-rouge&quot;&gt;md.pattern&lt;/code&gt;, we can visually inspect missing patterns of the data using the package &lt;code class=&quot;highlighter-rouge&quot;&gt;VIM&lt;/code&gt;. First, we will use &lt;code class=&quot;highlighter-rouge&quot;&gt;aggr&lt;/code&gt; to aggregate missing values.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;aggr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;navyblue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortVars&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-4-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  Variables sorted by number of missings: 
##  Variable       Count
##       Age 0.198653199
##      Fare 0.016835017
##  Embarked 0.002244669
##  Survived 0.000000000
##    Pclass 0.000000000
##       Sex 0.000000000
##     SibSp 0.000000000
##     Parch 0.000000000&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The method &lt;code class=&quot;highlighter-rouge&quot;&gt;aggr&lt;/code&gt; is essentially similar to &lt;code class=&quot;highlighter-rouge&quot;&gt;md.patterns&lt;/code&gt;, except that &lt;code class=&quot;highlighter-rouge&quot;&gt;aggr&lt;/code&gt; displays missing values in portion. For example, roughly 20% (177 rows) of &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; are incomplete, 1.6% (15 rows) of &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; missing, and 0.2% (2 rows) of &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; missing. There are roughly 0.9% (8 rows) of the data where both &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; are missing. Thus, it seems more convenient when combine both &lt;code class=&quot;highlighter-rouge&quot;&gt;md.pattern&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;aggr&lt;/code&gt; together to inspect missing values of the data.&lt;/p&gt;

&lt;p&gt;We can also observe missing values by pair. The function &lt;code class=&quot;highlighter-rouge&quot;&gt;marginplot&lt;/code&gt; is perfect for this purpose. We will use &lt;code class=&quot;highlighter-rouge&quot;&gt;marginplot&lt;/code&gt; to inspect missing patterns of the pair &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;marginplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Fare&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Age&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;navyblue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
           &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pch&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;19&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-5-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Let’s intepret information given by the plot.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Blue points in the main area indicates the number of rows that both &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; are complete. The red points in the left side indicates missing values of &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; is observed. Similarly, the red points in the bottom margin indicates missing values of &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; is observed.&lt;/li&gt;
  &lt;li&gt;Three numbers tells us that there are 15 missing values in &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; is observed, 177 missing values in &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; is observed, and 8 commonly missing values for both.&lt;/li&gt;
  &lt;li&gt;The blue and red boxplots summarise the marginal distribution of complete and incomplete rows. For example, the distribution of complete &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; values is (0, 80) years old, the distribution of missing value in &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; when &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; is observed is from (20, 50) years old (i.e., 15 passengers who has missing values of &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; are from 20 to 50 years old).&lt;/li&gt;
&lt;/ul&gt;

&lt;h5 id=&quot;data-imputation-with-mice&quot;&gt;4.2. Data imputation with &lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;We can simply impute the data by using the function &lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt;. For example,&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Impute missing values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;By default, &lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt; will create &lt;strong&gt;5 version of imputed data&lt;/strong&gt; (i.e., &lt;code class=&quot;highlighter-rouge&quot;&gt;m = 5&lt;/code&gt;), and for each version, the default number of iterations is also 5 (i.e., &lt;code class=&quot;highlighter-rouge&quot;&gt;maxit = 5&lt;/code&gt;). According to the authors, the convergence of Gibbs sampling often happens when &lt;code class=&quot;highlighter-rouge&quot;&gt;maxit&lt;/code&gt; is between 10 and 20.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Extract the 1st version of the imputed data using complete()
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_imp_1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_imp_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##     Survived          Pclass          Sex           Age       
##  Min.   :0.0000   Min.   :1.000   female:314   Min.   : 0.42  
##  1st Qu.:0.0000   1st Qu.:2.000   male  :577   1st Qu.:20.00  
##  Median :0.0000   Median :3.000                Median :28.00  
##  Mean   :0.3838   Mean   :2.309                Mean   :28.83  
##  3rd Qu.:1.0000   3rd Qu.:3.000                3rd Qu.:37.00  
##  Max.   :1.0000   Max.   :3.000                Max.   :80.00  
##      SibSp           Parch             Fare         Embarked
##  Min.   :0.000   Min.   :0.0000   Min.   :  4.013   C:170   
##  1st Qu.:0.000   1st Qu.:0.0000   1st Qu.:  7.925   Q: 77   
##  Median :0.000   Median :0.0000   Median : 14.500   S:644   
##  Mean   :0.523   Mean   :0.3816   Mean   : 32.518           
##  3rd Qu.:1.000   3rd Qu.:0.0000   3rd Qu.: 31.275           
##  Max.   :8.000   Max.   :6.0000   Max.   :512.329&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We can see that missing values no longer exist in the data. Let’s see how the data is imputed across 5 versions. First begin with &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Embarked
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# get index of missing values in the original data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])})&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##      [,1] [,2] [,3] [,4] [,5]
## [1,] &quot;C&quot;  &quot;C&quot;  &quot;C&quot;  &quot;S&quot;  &quot;C&quot; 
## [2,] &quot;C&quot;  &quot;S&quot;  &quot;S&quot;  &quot;S&quot;  &quot;C&quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;So, &lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt; imputed missing values in &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; with different values. Note that in the original data, there are 644 rows of &lt;code class=&quot;highlighter-rouge&quot;&gt;S&lt;/code&gt;. So I suspect the imputed values are likely (S,S).&lt;/p&gt;

&lt;p&gt;Next, we will explore the distribution of imputed values of &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt;. It can be seen from both plots that the distribution of imputed values are similar to that of observed values.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Age
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_imp_1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;long&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inc&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stripplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_imp_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fac&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Imputation Number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-9-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Fare
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;rep&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;blue&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;red&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;6&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stripplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;.imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dt_imp_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;jit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fac&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
          &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Imputation Number&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-9-2.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h5 id=&quot;data-analysis-with-with&quot;&gt;4.2. Data analysis with &lt;code class=&quot;highlighter-rouge&quot;&gt;with()&lt;/code&gt;&lt;/h5&gt;
&lt;p&gt;After imputation, we will analyse imputed data using &lt;code class=&quot;highlighter-rouge&quot;&gt;with()&lt;/code&gt;. As said, we analyse the imputed data by our predict model of interest $Q$.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;with&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;glm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                     &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;family&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;binomial&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;result-pooling-with-pool&quot;&gt;4.3. Result pooling with &lt;code class=&quot;highlighter-rouge&quot;&gt;pool()&lt;/code&gt;&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Call: pool(object = fit)
## 
## Pooled coefficients:
##  (Intercept)         Sex2          Age         Fare       Pclass 
##  5.721573191 -2.742309073 -0.044220471  0.001294004 -1.229816797 
##        SibSp        Parch    Embarked2    Embarked3 
## -0.384499645 -0.080942900  0.008981836 -0.388565364 
## 
## Fraction of information about the coefficients missing due to nonresponse: 
## (Intercept)        Sex2         Age        Fare      Pclass       SibSp 
##  0.21052610  0.02914752  0.39931480  0.01121808  0.11571302  0.05188839 
##       Parch   Embarked2   Embarked3 
##  0.01208783  0.08927502  0.01918828&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##                      est          se            t        df     Pr(&amp;gt;|t|)
## (Intercept)  5.721573191 0.649360179   8.81109340  92.70510 6.838974e-14
## Sex2        -2.742309073 0.206085341 -13.30666732 744.34461 0.000000e+00
## Age         -0.044220471 0.009322114  -4.74360962  29.31085 5.051145e-05
## Fare         0.001294004 0.002370095   0.54597163 857.30898 5.852275e-01
## Pclass      -1.229816797 0.160296256  -7.67214923 237.70923 4.343192e-13
## SibSp       -0.384499645 0.112323317  -3.42315072 560.94776 6.641687e-04
## Parch       -0.080942900 0.120734754  -0.67041923 853.63150 5.027719e-01
## Embarked2    0.008981836 0.406651902   0.02208728 333.63553 9.823915e-01
## Embarked3   -0.388565364 0.240072851  -1.61853105 815.52976 1.059348e-01
##                    lo 95        hi 95 nmis        fmi      lambda
## (Intercept)  4.432018506  7.011127875   NA 0.21052610 0.193675922
## Sex2        -3.146886777 -2.337731369   NA 0.02914752 0.026542410
## Age         -0.063277556 -0.025163385  177 0.39931480 0.359679804
## Fare        -0.003357863  0.005945872   15 0.01121808 0.008914053
## Pclass      -1.545599435 -0.914034160    0 0.11571302 0.108304116
## SibSp       -0.605125331 -0.163873959    0 0.05188839 0.048514010
## Parch       -0.317914664  0.156028863    0 0.01208783 0.009775928
## Embarked2   -0.790943030  0.808906703   NA 0.08927502 0.083831931
## Embarked3   -0.859798868  0.082668139   NA 0.01918828 0.016785894&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;convergence-assessment&quot;&gt;5. Convergence assessment&lt;/h4&gt;
&lt;p&gt;Convergence assessment of Gibbs sampling algorithm is important to check that if the algorithm is converged. According to the paper: &lt;em&gt;“There is no clear-cut method for determining whether the Gibbs sampling algorithm has converged. What is often done is to plot one or more parameters against the iteration number”&lt;/em&gt; and &lt;em&gt;“on convergence, the different streams should be freely intermingled with each other, without showing any definite trends. Convergence is diagnosed when the variance between different sequences is no larger than the variance with each individual sequence”&lt;/em&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-19-mice/unnamed-chunk-12-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;conclusion&quot;&gt;6. Conclusion&lt;/h4&gt;
&lt;p&gt;In this experiment, we have demonstrated how missing values in the data can be imputed using FCS technique. When compare FCS with JM technique, MICE is considered as a more adaptable method when we can select appropriate imputation models for  variables on a case by case basis. For example, we can choose linear regression or Bayesian linear regression for continuous variables, and logistic regression or linear discriminant analysis for nominal variables.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; package in R is a powerful and convenient library that enables multivariate imputation in a modular approach consisting of three subsequent steps. First, we can impute missing values by using a single &lt;code class=&quot;highlighter-rouge&quot;&gt;mice()&lt;/code&gt; function, then effectively analyse imputed versions of data by using &lt;code class=&quot;highlighter-rouge&quot;&gt;with()&lt;/code&gt; method with our own model of choice, and finally report the imputation result by using &lt;code class=&quot;highlighter-rouge&quot;&gt;pool()&lt;/code&gt; method.&lt;/p&gt;

&lt;p&gt;After this experiment, we believe that &lt;code class=&quot;highlighter-rouge&quot;&gt;mice&lt;/code&gt; package is capable of supporting our future research experiments, where we would have chance to explore additional features of it, such as, implementing our own imputation models.&lt;/p&gt;

&lt;h4 id=&quot;references&quot;&gt;References&lt;/h4&gt;
&lt;p&gt;Buuren, S. &amp;amp; Groothuis-Oudshoorn, K. (2011, December). MICE: Mulitivariate Imputation by Chained Equations in R. Journal of Statistical Software, 45(3), 1–67. Retrieved from http://doc.utwente.nl/78938/&lt;/p&gt;

&lt;p&gt;Kaggle. (2012, September). Titanic: machine learning from disaster. Retrieved from https://www.kaggle.com/c/titanic&lt;/p&gt;

&lt;p&gt;Liu, Y. &amp;amp; De, A. (2015, July). Multiple Imputation by Fully Conditional Specification for Dealing with Missing Data in a Large Epidemiologic Study. International Journal of Statistics in Medical Research, 4(3), 287–295. doi:10.6000/1929-6029.2015.04.03.7&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Dec 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/12/19/mice/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/12/19/mice/</guid>
      </item>
    
      <item>
        <title>Drawing neural nets using Tikz</title>
        <description>&lt;p&gt;Over the last few weeks, I had been looking for a versatile software to draw neural net beautifully and consistently. &lt;a href=&quot;https://www.omnigroup.com/omnigraffle&quot;&gt;OmniGraffle&lt;/a&gt; is my most favorite application when it comes to chart drawing. But unfortunately, it did not work so well when I attempted to use it to draw a neural net.&lt;/p&gt;

&lt;p&gt;Using Tikz requires spending sometimes on reading &lt;a href=&quot;https://www.bu.edu/math/files/2013/08/tikzpgfmanual.pdf&quot;&gt;its manual&lt;/a&gt;, which is about 400 pages. It is exceedingly overwhelming for a package but we actually don’t need to read all of that. To draw neural net, solely reading Chapter 13 of the manual should be sufficient.&lt;/p&gt;

&lt;p&gt;As far as I am concerned, using Tikz have some exceptional advantages over using applications. For example,&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Consistency&lt;/strong&gt; Because Tikz allows to define &lt;em&gt;for-loop&lt;/em&gt; and styles in a similar manner to CSS, I can consistenly define and adjust the whole structure of the network within seconds.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Elegance&lt;/strong&gt; Beautifulness is more or less about personal taste, but to me figures generated by Tikz are very pleasant.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Besides some pros, there are certainly a few cons can be listed.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Clumsiness&lt;/strong&gt; It is definitely clumsy to write codes in Latex.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Readability&lt;/strong&gt; When I write some lines of code, get away a few days and look back, it is challenging to understand even my own codes. I strongly advocate a good comment and coding style if you don’t want to be like “What the hell!” when look back at your own codes.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below are some examples of neural nets generated by Tikz. They are accompanied with source codes for further personal modifications as well.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/nn_1.png&quot; alt=&quot;Fig 1. General three layers NN&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. General three layers NN
  &lt;/p&gt;
&lt;/div&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/a1455e58a93af387df422544b2e7bf32.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/nn_2.png&quot; alt=&quot;Fig 1. A specific NN with 3 inputs, 3 hidden neurons, and 3 outputs&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. A specific NN with 3 inputs, 3 hidden neurons, and 3 outputs
  &lt;/p&gt;
&lt;/div&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/b61bd9f2027e9baade14f1fb0c94ca7b.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/nn_3.png&quot; alt=&quot;Fig 1. Signal flow at neuron j&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. Signal flow at neuron j
  &lt;/p&gt;
&lt;/div&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/fc7f9a323d592bd457cd297ff7796176.js&quot;&gt;&lt;/script&gt;

</description>
        <pubDate>Fri, 16 Dec 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/12/16/tikz/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/12/16/tikz/</guid>
      </item>
    
      <item>
        <title>Titanic</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#data-munging&quot;&gt;1. Data munging&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#data-loading&quot;&gt;1.1 Data loading&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;missing-data&quot;&gt;1.2 Missing data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;data-facets&quot;&gt;1.3. Data facets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#modeling&quot;&gt;2. Modeling&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#feature-engineer&quot;&gt;2.1. Feature Engineer&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#missing-data-imputation&quot;&gt;2.2. Missing data imputation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#categorizing-data&quot;&gt;2.3. Categorizing data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#model-construction&quot;&gt;2.4. Model construction&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#testing&quot;&gt;3. Testing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;Titanic is a entry level competition that Kaggle, a data science competition platform, has created to challenge novice data scientists so that they can have chance to earn some practical experiences before getting into advanced level competitions. I also spent hours on this challenge, and can’t wait any longer to share my experience.&lt;/p&gt;

&lt;h4 id=&quot;data-munging&quot;&gt;1. Data munging&lt;/h4&gt;
&lt;p&gt;First thing first. We will spend sometimes on discovering the data. Data munging (also known as &lt;strong&gt;data wrangling&lt;/strong&gt;) involves getting our hands dirty on the data to explore different aspects of the data, for example structure, statistics. I believe that there exists no standard guideline for data munging, except that we are encouraged to properly use all possible methods.&lt;/p&gt;

&lt;h5 id=&quot;data-loading&quot;&gt;1.1 Data loading&lt;/h5&gt;
&lt;p&gt;We load the dataset first. In this case, I do not process the draw data but just a copy of it.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;VIM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#aggr
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#ggplot
################################################################################
# LOAD DATA
################################################################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_raw&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read.csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;data/train.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;na.strings&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;NA&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Back up
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_raw&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Next, we discover the structure and data types of the data.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strict.width&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wrap&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## &#39;data.frame&#39;:	891 obs. of  12 variables:
## $ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ...
## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ...
## $ Pclass : int 3 1 3 1 3 3 1 3 3 2 ...
## $ Name : Factor w/ 891 levels &quot;Abbing, Mr. Anthony&quot;,..: 109 191 358 277 16
##    559 520 629 417 581 ...
## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
## $ Age : num 22 38 26 35 35 NA 54 2 27 14 ...
## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ...
## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ...
## $ Ticket : Factor w/ 681 levels &quot;110152&quot;,&quot;110413&quot;,..: 524 597 670 50 473
##    276 86 396 345 133 ...
## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ...
## $ Cabin : Factor w/ 147 levels &quot;A10&quot;,&quot;A14&quot;,&quot;A16&quot;,..: NA 82 NA 56 NA NA 130
##    NA NA NA ...
## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Currently, &lt;code class=&quot;highlighter-rouge&quot;&gt;Name&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Ticket&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Cabin&lt;/code&gt; are factors. We’ll convert them to characters.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Convert Name, Ticket, Cabin to character
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Name&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ticket&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Cabin&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;indx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.character&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strict.width&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;wrap&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## &#39;data.frame&#39;:	891 obs. of  12 variables:
## $ PassengerId: int 1 2 3 4 5 6 7 8 9 10 ...
## $ Survived : int 0 1 1 1 0 0 0 0 1 1 ...
## $ Pclass : int 3 1 3 1 3 3 1 3 3 2 ...
## $ Name : chr &quot;Braund, Mr. Owen Harris&quot; &quot;Cumings, Mrs. John Bradley
##    (Florence Briggs Thayer)&quot; &quot;Heikkinen, Miss. Laina&quot; &quot;Futrelle, Mrs.
##    Jacques Heath (Lily May Peel)&quot; ...
## $ Sex : Factor w/ 2 levels &quot;female&quot;,&quot;male&quot;: 2 1 1 1 2 2 2 2 1 1 ...
## $ Age : num 22 38 26 35 35 NA 54 2 27 14 ...
## $ SibSp : int 1 1 0 1 0 0 0 3 0 1 ...
## $ Parch : int 0 0 0 0 0 0 0 1 2 0 ...
## $ Ticket : chr &quot;A/5 21171&quot; &quot;PC 17599&quot; &quot;STON/O2. 3101282&quot; &quot;113803&quot; ...
## $ Fare : num 7.25 71.28 7.92 53.1 8.05 ...
## $ Cabin : chr NA &quot;C85&quot; NA &quot;C123&quot; ...
## $ Embarked : Factor w/ 3 levels &quot;C&quot;,&quot;Q&quot;,&quot;S&quot;: 3 1 3 3 3 2 3 3 3 1 ...&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;missing-data&quot;&gt;1.2 Missing data&lt;/h5&gt;
&lt;p&gt;We’ll visualise the missing data pattern of our data by using the function &lt;code class=&quot;highlighter-rouge&quot;&gt;aggr&lt;/code&gt; of the package &lt;code class=&quot;highlighter-rouge&quot;&gt;VIM&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Missing data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;missing_plot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aggr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;navyblue&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;red&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortVars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Histogram of missing data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Pattern&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-4-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  Variables sorted by number of missings: 
##     Variable       Count
##        Cabin 0.771043771
##          Age 0.198653199
##     Embarked 0.002244669
##  PassengerId 0.000000000
##     Survived 0.000000000
##       Pclass 0.000000000
##         Name 0.000000000
##          Sex 0.000000000
##        SibSp 0.000000000
##        Parch 0.000000000
##       Ticket 0.000000000
##         Fare 0.000000000&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Our visualisation tells us that there are 77% of missing data in &lt;code class=&quot;highlighter-rouge&quot;&gt;Cabin&lt;/code&gt;, 20% in &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and exactly 0.2% in &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt; (which is exactly 2 missing records).&lt;/p&gt;

&lt;h5 id=&quot;data-facets&quot;&gt;1.3. Data facets&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Find the relation between Survived, Pclass, Age, and Fare
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facet_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Warning: Removed 177 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-5-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Find the relation between Survived, Embarked, Age, and Fare
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_point&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;facet_grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Warning: Removed 177 rows containing missing values (geom_point).&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-6-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Find the relation between Survived, Sibsp
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-7-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Find the relation between Survived, Parch
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_histogram&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-8-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Find the relation between Survived, Embarked
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Embarked&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Survived&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Quantity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Quantity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-9-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Find the relation between Survived, Pclass
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Pclass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Survived&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Quantity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Quantity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-10-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;## Find the relation between Survived, Sex
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Sex&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Survived&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Quantity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ct_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
       &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mapping&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Quantity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fill&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;geom_bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;identity&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ggtitle&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Distribution of Survived by Sex&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Survived&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;# of Passengers&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-11-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;modeling&quot;&gt;2. Modeling&lt;/h4&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;################################################################################
# LOAD LIBRARY
################################################################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;#missing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;library&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;caret&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;feature-engineer&quot;&gt;2.1. Feature Engineer&lt;/h5&gt;
&lt;p&gt;Next, we will extract first name, last name and title of passengers from &lt;code class=&quot;highlighter-rouge&quot;&gt;Name&lt;/code&gt;, making 3 new columns in the dataset, which are &lt;code class=&quot;highlighter-rouge&quot;&gt;FName&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Title&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;LName&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#########################################
# LASTNAME &amp;amp; FIRSTNAME &amp;amp; TITLE
#########################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;[,.]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# First name
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FName&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trimws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;both&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Title
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trimws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;both&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Last name
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trimws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;both&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;strsplit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;[()]&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trimws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;both&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Some don&#39;t have lastname, need to impute
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_imp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trimws&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]][&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;both&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_splt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_missing_idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_missing_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_imp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_missing_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##                William                   John                  James 
##                     11                     10                      9 
##                   Mary                 Joseph                 Samuel 
##                      7                      5                      5 
##                 Thomas                 Bertha                 Edward 
##                      5                      4                      4 
##                 George                   Ivan                 Martin 
##                      4                      4                      4 
##          William Henry           William John                 Alfred 
##                      4                      4                      3 
##                  Alice                   Anna             Anna Sofia 
##                      3                      3                      3 
##               Benjamin              Elizabeth                   Emil 
##                      3                      3                      3 
##              Frederick                  Harry                  Johan 
##                      3                      3                      3 
##                Patrick                Tannous                 Victor 
##                      3                      3                      3 
##                 Albert          Albert Adrian              Alexander 
##                      2                      2                      2 
##        Alexander Oskar           Anders Johan                 Antoni 
##                      2                      2                      2 
##                 Arthur      Charles Alexander              Charles H 
##                      2                      2                      2 
##          Charles Henry                  David            Dickinson H 
##                      2                      2                      2 
##           Edgar Joseph            Edwy Arthur         Ellen &quot;Nellie&quot; 
##                      2                      2                      2 
##           Elmer Zebley       Ernest Courtenay          Ernst Gilbert 
##                      2                      2                      2 
##             Frank John      Frederick Charles     Frederick Maxfield 
##                      2                      2                      2 
##      Frederick William           George Henry                Gerious 
##                      2                      2                      2 
##                  Hanna          Hanora &quot;Nora&quot;                 Harvey 
##                      2                      2                      2 
##                  Henry        Henry Birkhardt          Henry Sleeper 
##                      2                      2                      2 
##          Henry William          Jacques Heath           Jakob Alfred 
##                      2                      2                      2 
##          Jean Baptiste           John Borland             John Henry 
##                      2                      2                      2 
##                  Josef                   Juha                   Juho 
##                      2                      2                      2 
##            Karl Alfred                   Kate      Katherine &quot;Katie&quot; 
##                      2                      2                      2 
##                  Lalio               Lawrence                    Lee 
##                      2                      2                      2 
##                   Luka                 Marija                Maurice 
##                      2                      2                      2 
##                Michael               Nicholas        Norman Campbell 
##                      2                      2                      2 
##          Pekka Pietari               Percival            Peter Henry 
##                      2                      2                      2 
##                Richard        Richard Leonard               Samuel L 
##                      2                      2                      2 
##          Sidney Samuel                  Sinai           Thomas Henry 
##                      2                      2                      2 
##              Thomas Jr Thomas William Solomon       Victor de Satode 
##                      2                      2                      2 
##                Wilhelm         William Arthur          William Baird 
##                      2                      2                      2 
##         William Ernest          William James    William John Robert 
##                      2                      2                      2 
##         William Thomas       William Thompson                Abraham 
##                      2                      2                      1 
##                (Other) 
##                    629&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Looks like we get something here. There are a lot of people having the same last name. It hints us that they are likely families.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Family&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;In fact, there are many families traveled, and we can easily identify the size of the family by combining both &lt;code class=&quot;highlighter-rouge&quot;&gt;SibSp&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Parch&lt;/code&gt;. Doing that, if a passenger has 0s on &lt;code class=&quot;highlighter-rouge&quot;&gt;SibSp&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Parch&lt;/code&gt;, we would have a family size of 0. That doesn’t make sense. Thus, we have to +1.&lt;/p&gt;

&lt;p&gt;Next, we have to find a way to increase the correctness of &lt;code class=&quot;highlighter-rouge&quot;&gt;Family&lt;/code&gt;. We can see that&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A lot of people have the same &lt;code class=&quot;highlighter-rouge&quot;&gt;Ticket&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;A lot of people have the same &lt;code class=&quot;highlighter-rouge&quot;&gt;LName&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Thus, we will create a new column, &lt;code class=&quot;highlighter-rouge&quot;&gt;FamilyID&lt;/code&gt;, and identify families in the dataset based on two assumptions. First, people who have the same &lt;code class=&quot;highlighter-rouge&quot;&gt;Ticket&lt;/code&gt; should be a family, or a group of passengers. Second, people who have the same &lt;code class=&quot;highlighter-rouge&quot;&gt;LName&lt;/code&gt; could belong to a family, but only if they have the same &lt;code class=&quot;highlighter-rouge&quot;&gt;Pclass&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Embarked&lt;/code&gt;. It does not make sense if people in the same family stay in different classess, and/or embark from different places.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;##############
# Grouping by similar tickets
###############
# There are a lot of similar tickets, So if passengers have the same ticket,
# they should be a family or a least travel together
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ticket&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ticket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Find tickets ID that used by &amp;gt; 1 passengers
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ticket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similar_ticket&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Freq&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Assign all ticket to FamilyID as suitable familyID 
# then remove ones that not in similar ticket
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Ticket&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Find positions that do not be removed
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket_idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similar_ticket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket_idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Set NA to position not in idx
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;##############
# Grouping by similar last name, pclass, embarked
###############
# Assign the same familyID to people who have the same lname by using 
# the pattern lname_embark_pclass
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similar_lname&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Freq&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;similar_lname&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unlist&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;setdiff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lname_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ticket_idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.character&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sapply&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LName&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;diff&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;function&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;return&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                               &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gsub&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;[[:space:]]&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;##############
# Update Family(size) by FamilyID
###############
# People who have family ID but FamilySize = 1
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;intersect&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Family&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data.frame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;idx&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;FamilyId&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Freq&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.character&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nrow&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilyId&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;freq&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fre&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;After enhancing &lt;code class=&quot;highlighter-rouge&quot;&gt;Family&lt;/code&gt;, we will have a look at our new feature &lt;code class=&quot;highlighter-rouge&quot;&gt;Title&lt;/code&gt;. We can see that there are a variety of factors in &lt;code class=&quot;highlighter-rouge&quot;&gt;Title&lt;/code&gt;. We reduce the number of factors by grouping them.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;##############
# TITLE
###############
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;##         Capt          Col          Don           Dr     Jonkheer 
##            1            2            1            7            1 
##         Lady        Major       Master         Miss         Mlle 
##            1            2           40          182            2 
##          Mme           Mr          Mrs           Ms          Rev 
##            1          517          125            1            6 
##          Sir the Countess 
##            1            1&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;as.character&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Too many factors in Title, we need to merge them
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Capt&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Col&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Major&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Don&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Dr&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Jonkheer&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Rev&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Sir&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mme&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mr&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Lady&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mrs&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;the Countess&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mrs&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Ms&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Miss&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Mlle&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Miss&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;summary&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Master   Miss     Mr    Mrs 
##     40    185    539    127&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;missing-data-imputation&quot;&gt;2.2. Missing data imputation&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Fare == 0 doesnt make sense, thus should be missing values
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NA&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#########################################
# CABIN
#########################################
# More than 70% of Cabin is missing, thus we have to remove
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Cabin&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;NULL&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#########################################
# EMBARKED
#########################################
# Impute Embarked
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;which&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;is.na&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;S&quot;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;We use multiple imputation by chained equation to impute missing values in &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt;. We will use &lt;code class=&quot;highlighter-rouge&quot;&gt;Sex&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Title&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;Pclass&lt;/code&gt; as predictors to impute. More importantly, because it does not make sense if imputed values of &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; are negative, we have to squeeze values into two corresponding ranges, (1, 80) for &lt;code class=&quot;highlighter-rouge&quot;&gt;Age&lt;/code&gt; and (1, 500) for &lt;code class=&quot;highlighter-rouge&quot;&gt;Fare&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;#########################################
# AGE
#########################################
# 19% of Age is missing. We&#39;ll find a way to impute the data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp_train&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Sex&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Age&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Title&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Fare&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Pclass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Impute age
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;maxit&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Age&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;imp[[j]][,i] &amp;lt;- squeeze(imp[[j]][,i], c(1,80))&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Fare&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;imp[[j]][,i] &amp;lt;- squeeze(imp[[j]][,i], c(1,500))&quot;&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;restricted&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp_train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;m&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;post&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seed&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;567&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;norm&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  iter imp variable
##   1   1  Age  Fare
##   1   2  Age  Fare
##   1   3  Age  Fare
##   1   4  Age  Fare
##   1   5  Age  Fare
##   2   1  Age  Fare
##   2   2  Age  Fare
##   2   3  Age  Fare
##   2   4  Age  Fare
##   2   5  Age  Fare
##   3   1  Age  Fare
##   3   2  Age  Fare
##   3   3  Age  Fare
##   3   4  Age  Fare
##   3   5  Age  Fare
##   4   1  Age  Fare
##   4   2  Age  Fare
##   4   3  Age  Fare
##   4   4  Age  Fare
##   4   5  Age  Fare
##   5   1  Age  Fare
##   5   2  Age  Fare
##   5   3  Age  Fare
##   5   4  Age  Fare
##   5   5  Age  Fare&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;train_temp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;complete&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;restricted&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;The next move is to ensure there is no missing values in the data.&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Assure that no more missing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;missing_plot&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;aggr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;navyblue&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;red&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numbers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sortVars&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;names&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cex.axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;gap&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                     &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylab&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Histogram of missing data&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Pattern&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-23-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
##  Variables sorted by number of missings: 
##     Variable Count
##  PassengerId     0
##     Survived     0
##       Pclass     0
##         Name     0
##          Sex     0
##          Age     0
##        SibSp     0
##        Parch     0
##       Ticket     0
##         Fare     0
##     Embarked     0
##        FName     0
##        Title     0
##        LName     0
##       Family     0
##     FamilyId     0&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;categorizing-data&quot;&gt;2.3. Categorizing data&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;################################################################################
# CATEGORISE DATA &amp;amp; CREATE DUMMY VARIABLES FOR NOMINAL DATA
################################################################################
# Pclass 
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Pclass&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Embarked
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Embarked&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Sibsp
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;SibSp&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SibSp&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Parch
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Parch&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Parch&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Sex
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Sex&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Fare
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare_factor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Fare_factor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare_factor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Age
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age_factor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;21.1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;37&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Age_factor&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age_factor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# FamilySize
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilySize&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;findInterval&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Family&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;in&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unique&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age_factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)){&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;FamilySize&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;_&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ifelse&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age_factor&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;==&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;level&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h5 id=&quot;model-construction&quot;&gt;2.4. Model construction&lt;/h5&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;################################################################################
# BUIDING MODEL
################################################################################
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;set.seed&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;234&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Split data
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTraining&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;createDataPartition&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;p&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;FALSE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTraining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inTraining&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,]&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Train control
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fitControl&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trainControl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;## 10-fold CV
&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;repeatedcv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;number&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;repeats&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;allowParallel&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Model tuning
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;  &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;expand.grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mtry&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;seq&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;

&lt;/span&gt;&lt;span class=&quot;c1&quot;&gt;# Model
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;as.factor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;~&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sex_male&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass_3&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Pclass_1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Embarked&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
                 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Age&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
                 &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Fare&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;FamilySize&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;training&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;method&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;parRF&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;trControl&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fitControl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;verbose&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;TRUE&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; 
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tuneGrid&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preProc&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;nf&quot;&gt;c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;center&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;scale&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
               &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metric&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Kappa&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Loading required package: e1071&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Loading required package: randomForest&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## randomForest 4.6-12&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Type rfNews() to see new features/changes/bug fixes.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## 
## Attaching package: &#39;randomForest&#39;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Loading required package: foreach&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Warning: executing %dopar% sequentially: no parallel backend registered&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## Parallel Random Forest 
## 
## 446 samples
##   7 predictor
##   2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## Pre-processing: centered (9), scaled (9) 
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 402, 402, 401, 401, 401, 401, ... 
## Resampling results across tuning parameters:
## 
##   mtry  Accuracy   Kappa    
##   1     0.7898411  0.5352829
##   2     0.8122511  0.5898479
##   3     0.8086648  0.5865441
##   4     0.8106499  0.5935032
##   5     0.8073165  0.5889437
##   6     0.8058973  0.5865967
##   7     0.7993973  0.5751578
## 
## Kappa was used to select the optimal model using  the largest value.
## The final value used for the model was mtry = 4.&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Plot model performance
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;&lt;img src=&quot;/../figs/2016-12-02-titanic/unnamed-chunk-25-1.png&quot; alt=&quot;center&quot; /&gt;&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Check feature importances
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;varImp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## parRF variable importance
## 
##             Overall
## Sex_male    100.000
## Age          83.674
## Fare         73.573
## Pclass_3     25.171
## FamilySize2   9.530
## FamilySize1   8.053
## Pclass_1      7.275
## EmbarkedS     6.543
## EmbarkedQ     0.000&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-r&quot; data-lang=&quot;r&quot;&gt;&lt;span class=&quot;c1&quot;&gt;# Testing
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;newdata&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pred&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;testing&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;$&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Survived&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;w&quot;&gt;
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;paste&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;Accuracy &quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;w&quot;&gt; &lt;/span&gt;&lt;span class=&quot;n&quot;&gt;error&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-text&quot; data-lang=&quot;text&quot;&gt;## [1] &quot;Accuracy  0.835955056179775&quot;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;h4 id=&quot;testing&quot;&gt;3. Testing&lt;/h4&gt;
&lt;p&gt;We will re-apply all our necessary step above to the test data given by Kaggle. This model is able to achieve &lt;strong&gt;0.8038&lt;/strong&gt;. The full source code can be found in &lt;a href=&quot;https://github.com/newbiettn/R/tree/master/kaggle_titanic&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
</description>
        <pubDate>Sat, 10 Dec 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/12/10/titanic/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/12/10/titanic/</guid>
      </item>
    
      <item>
        <title>Unconstrained optimization methods</title>
        <description>&lt;p&gt;&lt;strong&gt;Table of Contents&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;#gradient-descent&quot;&gt;1. Gradient descent&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;1.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;1.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#newtons-method&quot;&gt;2. Newton’s method&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;2.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#minimization-and-maximization-problem&quot;&gt;2.2. Minimization and maximization problem&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;2.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#gauss-newton-method&quot;&gt;3. Gauss-Newton method&lt;/a&gt;
    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#concept&quot;&gt;3.1. Concept&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#experiments&quot;&gt;3.2. Experiments&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;In this post, we are going to review classical unconstrained optimization algorithms, including gradient descent, Newton’s and Gauss-Newton. Those three algorithms have been heavily used as optimization methods of different machine learning algorithms, such as linear regression, logistic regression, neural network, and so on.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; All R codes of experiments can be found &lt;a href=&quot;https://github.com/newbiettn/R/tree/master/optimization_algorithms&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;1. Gradient descent&lt;/h3&gt;

&lt;h4 id=&quot;concept&quot;&gt;1.1. Concept&lt;/h4&gt;

&lt;p&gt;Gradient descent (or steepest descent) is the most intuitively simple among three algorithms. The underlying mechanism of the algorithm is the concept of the gradient $\nabla$ of the function $f$. Intuitively speaking, $\nabla f$ indicates the direction by which if we traverse along the graph curve of $f$, we are able to find the increasing function values (e.g., $f(x_{n+1}) \gt f(x)$). Conversely, if we traverse the opposite direction of $\nabla f$, we can obtain decreasing values of $f$ such that $f(x_{n+1}) \lt f(x)$. And that’s how gradient descent works!&lt;/p&gt;

&lt;p&gt;Mathematically speaking, given a differential function $f(x)$, if&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x_{n+1} = x_n - \eta \nabla f(x)&lt;/script&gt;

&lt;p&gt;where $\eta$ ($0 \lt \eta &amp;lt; 1$) is &lt;strong&gt;the learning rate&lt;/strong&gt;, $\nabla f(x)$ is the gradient of $f$ at $x_n$.&lt;/p&gt;

&lt;p&gt;then,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f(x_{n+1}) \geq f(x_n)&lt;/script&gt;

&lt;p&gt;This process stops either after $m$ predefined iterations or until $f(x_{n+1}) \approx f(x_n)$.&lt;/p&gt;

&lt;h4 id=&quot;experiments&quot;&gt;1.2. Experiments&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      

Consider $f(x)$ as a quadratic function. Suppose we have $f(x) = ax^2 + bx + c$ with $a = 1, b = 2, c = 3$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.


  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;At the beginning, we blindly set $x_0 = 45$, and sequentially change $\eta = (0.1, 0.3, 0.5, 0.7)$ to observe how the learning rate affects the convergence of the algorithm. Look at Figure 1, at $\eta = 0.1$ (the top left), gradient descent can find the minima of $f(x)$ after roughly 10 iterations. However, from the figure, we can also see that the rate of convergence of the algorithm significantly reduces as soon as it is close to the minima. When $\eta = 0.3$ (the top right), we need roughly 5 iterations, and it seems like at $\eta = 0.5$ (the bottom left), we only need 2 iterations, which is really computationally effective. The worst case is when $\eta = 0.7$ (the bottom right), we “jump” over the minima several times before we approach it.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/bfb06b4f890c538b0e5da98474904a61.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc_quad_func.gif&quot; alt=&quot;Fig 1. Find the local  minima of a quadratic function&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 1. Find the local  minima of a quadratic function
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 2:&lt;/p&gt;
      

Consider $f(x)$ as a polynomial function. Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.


  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This example demonstrates that &lt;strong&gt;gradient descent cannot guarantee finding global minima of the function&lt;/strong&gt; although we keep changing $\eta$. The worst case happens when $\eta = 0.02$ (the top right), gradient descent can only find the sub minimal value of the function.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/3a3ad725cf87f03d1cca10c39a6cf77d.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc_poly_func.gif&quot; alt=&quot;Fig 2. Find the local minima of a polynomial function&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 2. Find the local minima of a polynomial function
  &lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;newtons-method&quot;&gt;2. Newton’s method&lt;/h3&gt;

&lt;h4 id=&quot;concept-1&quot;&gt;2.1. Concept&lt;/h4&gt;

&lt;p&gt;In my opinion, Newton’s method (or Newton-Raphson method) can be intuitively comprehended as &lt;strong&gt;a method to approximate an approximation&lt;/strong&gt;. It means that if we already have a reasonably good approximation of a function $f$ at a certain value $x_0$, we still can have a better approximation by estimating an approximation of the approximation itself.&lt;/p&gt;

&lt;p&gt;I found &lt;a href=&quot;http://tutorial.math.lamar.edu/Classes/CalcI/NewtonsMethod.aspx&quot;&gt;this page&lt;/a&gt; has a brilliant explanation for the algorithm and thus I will reiterate that explanation by my own words.&lt;/p&gt;

&lt;p&gt;Suppose we have a function $f(x) \in \mathbb{R^2}$, and we want to find the value of $f$ at a certain value $x_0$. One can say that we can simply plug $x_0$ into $f$ to get the searching value. However, there exist many functions, such as $f = \sqrt{x}$, where we cannot straightforwardly get the value. One can also say that computers can do that. But unlike simple equation like $f(x) = 2x$, where we can compute manually, we &lt;strong&gt;do not have any exact expression to describe that root function&lt;/strong&gt;. In fact, computers use approximation methods behind the scene (like Newton’s method) to compute root value of numbers.&lt;/p&gt;

&lt;p&gt;Suppose we want to find an approximation of $f(x^*) = 0$. We do that as follows.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/newton_exp.gif&quot; alt=&quot;Fig 3. Using 2nd tangent line to have a better approximation&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 3. Using 2nd tangent line to have a better approximation
  &lt;/p&gt;
&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;First, we construct &lt;strong&gt;a tangent line&lt;/strong&gt; (the blue line) at a point $x_0$ where $x_0$ is a guessing point and is reasonably close to the target point where $y = 0$. We can construct the tangent line by using &lt;strong&gt;slope-point form&lt;/strong&gt;. We can see that $x_1$ is a better approximation than $x_0$. However, we are still able to get a better one.&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;y - f(x_0) = f&#39;(x_0)(x-x_0)&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;Second, we construct a second tangent line at $x_1$ (the green line). Again, by using the slope-point form of the tangent line, we have,&lt;/li&gt;
&lt;/ul&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;0 = f(x_0) + f&#39;(x_0)(x_1 - x_0)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \Longleftrightarrow x_1 - x_0 = - \frac{f(x_0)}{f&#39;{x_0}} \\
&amp; \Longleftrightarrow x_1 = x_0 - \frac{f(x_0)}{f&#39;{x_0}}
\end{align} %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;As a result, we can find a closer point $x_1$ by using the formula $x_1 = x_0 - \frac{f(x_0)}{f’{x_0}}$.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;minimization-and-maximization-problem&quot;&gt;2.2. Minimization and maximization problem&lt;/h4&gt;
&lt;p&gt;Similar to gradient descent, Newton’s method can be used to find optimize the function by setting the derivative to 0 like we explained above.&lt;/p&gt;

&lt;h4 id=&quot;experiments-1&quot;&gt;2.2. Experiments&lt;/h4&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      
Consider $f(x)$ as a polynomial function. Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We aim to find a value $x^*$ at which we reach the minima of $f(x)$.

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In this example, we are going to find the local minima of $f(x)$ by using both gradient descent and Newton’s method. We can see that the rate of convergence of Newton’s method is higher than gradient descent. This happens due to the fact the Newton’s method makes use of the 2nd-order derivative of $f$.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/76430b8d6e124f994f56f47d3116b921.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/grad_desc__and_newton_poly_func.gif&quot; alt=&quot;Fig 4. Find the local minima of a polynomial function using both gradient descent and Newton&#39;s method&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 4. Find the local minima of a polynomial function using both gradient descent and Newton&#39;s method
  &lt;/p&gt;
&lt;/div&gt;

&lt;h3 id=&quot;gauss-newton-method&quot;&gt;3. Gauss-Newton method&lt;/h3&gt;

&lt;h4 id=&quot;concept-2&quot;&gt;3.1. Concept&lt;/h4&gt;

&lt;p&gt;Unlike gradient descent and Newton method, Gauss-Newton method can only be used to minimize the sum of squares, such as $C = \frac{1}{2} \sum_{i=1}^{n} (\hat{y}_i - y_i)^2$ where $\hat{y}$ is the predicted output, $y$ is the actual output and $n$ is the total number of observations in the dataset.&lt;/p&gt;

&lt;p&gt;Let’s set $e = (\hat{y}_i - y_i)$ , and suppose &lt;strong&gt;${\theta}_j$ is the coefficient vector of the hypothesis function &lt;script type=&quot;math/tex&quot;&gt;h_\theta(x)&lt;/script&gt; in the $j^{th}$ iteration&lt;/strong&gt;. For the &lt;script type=&quot;math/tex&quot;&gt;(j+1)^{th}&lt;/script&gt; iteration, the updated error term &lt;script type=&quot;math/tex&quot;&gt;e_{j+1}&lt;/script&gt; would be,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;e_{j+1} = e_j + \begin{bmatrix}\frac{\partial e}{\partial \theta}\end{bmatrix}_{\theta = \theta_j}(\theta_{j+1} - \theta_j) \qquad (I)&lt;/script&gt;

&lt;p&gt;We can make &lt;strong&gt;(I)&lt;/strong&gt; more beautiful by employing the concept of &lt;strong&gt;Jacobian matrix&lt;/strong&gt; $J$ such that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
J = \begin{bmatrix}\frac{\partial f}{\partial x_1} &amp; \frac{\partial f}{\partial x_2} &amp; \dots &amp; \frac{\partial f}{\partial x_n} \end{bmatrix} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, we have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;(I) \Longleftrightarrow e_{j+1} = e_j + J(\theta_{j+1} - \theta_j) \qquad (II)&lt;/script&gt;

&lt;p&gt;Our goal is finding $\theta_{j+1}$ such that &lt;strong&gt;the error term &lt;script type=&quot;math/tex&quot;&gt;e_{j+1}&lt;/script&gt; is minimal&lt;/strong&gt;. Mathematically, we have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\theta_{j+1} = \underset{\theta}argmin(\frac{1}{2}\lVert e_{j+1} \rVert_2^2) \qquad (III)&lt;/script&gt;

&lt;p&gt;Remember that &lt;script type=&quot;math/tex&quot;&gt;\lVert v \rVert_2&lt;/script&gt; is the &lt;em&gt;$l_2$-norm&lt;/em&gt; or Euclidiean norm of the vector $v$.&lt;/p&gt;

&lt;p&gt;We now need to elaborate &lt;script type=&quot;math/tex&quot;&gt;\lVert e_{j+1} \rVert^2&lt;/script&gt;. First, remember that the norm of a vector $v$ can be defined as &lt;script type=&quot;math/tex&quot;&gt;\lVert v \rVert^2 = v^Tv&lt;/script&gt;. Second, from the equation &lt;strong&gt;(II)&lt;/strong&gt;, suppose that,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; A = e_j \\
&amp; B = J(\theta_{j+1} - \theta_j) \\
&amp; C = e_{j+1}
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Thus, &lt;script type=&quot;math/tex&quot;&gt;(II) \Longleftrightarrow C = A + B&lt;/script&gt;. From that, we also have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
\lVert C \rVert^2 &amp; = C^TC \\
&amp; = (A + B)^T(A+B) \\
&amp; = (A^T + B^T)(A+B) \\
&amp; = A^TA + A^TB + B^TA + B^TB
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;Because &lt;script type=&quot;math/tex&quot;&gt;A^TB = \sum a_jb_j = \sum b_ja_j = B^TA&lt;/script&gt;, we can rewrite,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;C = \lVert A \rVert^2 + 2A^TB + \lVert B \rVert^2 \qquad (IV)&lt;/script&gt;

&lt;p&gt;From &lt;strong&gt;(II)&lt;/strong&gt; and &lt;strong&gt;(IV)&lt;/strong&gt;, we can have,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\lVert e_{j+1} \rVert^2 = \lVert e_{j} \rVert^2 + 2e^TJ(\theta_{j+1} - \theta_j) + (\theta_{j+1} - \theta_j)^TJ^TJ(\theta_{j+1} - \theta_j) \qquad (V)&lt;/script&gt;

&lt;p&gt;Now, we are going to minimize &lt;strong&gt;(V)&lt;/strong&gt;. First, we need to take the derivative of &lt;strong&gt;(V)&lt;/strong&gt; w.r.t. $\theta$ and set it equal to 0, and then find $\theta$. We have&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{align}
&amp; \frac{\partial V}{\partial \theta} = J^Te + J^TJ(\theta_{j+1} - \theta_j) = 0 \\
\Longleftrightarrow &amp; J^TJ(\theta_{j+1} - \theta_j) = -J^Te \\
\Longleftrightarrow &amp; \theta_{j+1} - \theta_j = -(J^TJ)^{-1}J^Te \\
\Longleftrightarrow &amp; \theta_{j+1} = \theta - (J^TJ)^{-1}J^Te \\
\end{align} %]]&gt;&lt;/script&gt;

&lt;p&gt;The formula &lt;script type=&quot;math/tex&quot;&gt;\theta_{j+1} = \theta - (J^TJ)^{-1}J^Te&lt;/script&gt; is &lt;strong&gt;the pure form of Gauss-Newton method&lt;/strong&gt;. By using this equation, we can update the coefficient &lt;script type=&quot;math/tex&quot;&gt;\theta_{j+1}&lt;/script&gt;. To do that, all we need to do is to find the Jacobian matrix. Note that the Gass-Newton method always converge because the Gauss-Newton methods assumes that &lt;script type=&quot;math/tex&quot;&gt;J^TJ&lt;/script&gt; is &lt;strong&gt;non-singular matrix&lt;/strong&gt;, making it possible to find the inverse matrix.&lt;/p&gt;

&lt;h4 id=&quot;experiments-2&quot;&gt;3.2. Experiments&lt;/h4&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 1:&lt;/p&gt;
      
Suppose we have $f(x) = -120*x - 154x^2 + 49x^3 + 140x^4 + 70x^5 + 14x^6 + x^7$. We first will use $f(x)$ to generate sample data points and second, we will use Gauss-Newton method to fit a model to the generated data points.

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;In Figure 5, we can see that at the beginning, we naively set $\theta =$ [1 1 1 1 1 1 1 1]. In the second iteration, $\theta$ is updated by Gauss-Newton method, allowing the model to fit to the generated data points.&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/969cf66470de9ce193e51992f4b691e2.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/gauss_newton_poly_func.gif&quot; alt=&quot;Fig 5. Use Gauss-Newton method to fit generated data points of $f(x)$&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 5. Use Gauss-Newton method to fit generated data points of $f(x)$
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;row&quot;&gt;
  &lt;div class=&quot;ten columns centered site-example&quot;&gt;
      &lt;p style=&quot;font-weight: bold&quot;&gt;Example 2:&lt;/p&gt;
      
Using the dataset Speed and Stopping Distances of Cars, we will find a model that find the relationship between speed and distance

  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/newbiettn/d04f39453c563c86a1fe2ca3ce68be95.js&quot;&gt;&lt;/script&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/gauss_newton_mtcars.gif&quot; alt=&quot;Fig 6. Use Gauss-Newton method to fit a curve line to the dataset&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Fig 6. Use Gauss-Newton method to fit a curve line to the dataset
  &lt;/p&gt;
&lt;/div&gt;

</description>
        <pubDate>Wed, 23 Nov 2016 00:00:00 +1100</pubDate>
        <link>http://127.0.0.1:4000/2016/11/23/uncontrained-optimization-methods/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/11/23/uncontrained-optimization-methods/</guid>
      </item>
    
      <item>
        <title>Classification using Rough Set theory</title>
        <description>&lt;h4 id=&quot;i-introduction&quot;&gt;I. Introduction&lt;/h4&gt;
&lt;p&gt;Rough Set (RS) algorithm targets the &lt;strong&gt;vagueness&lt;/strong&gt; of the knowledge, e.g., the boundary between observations is not strong enough to set them apart. Look at Figure 1, we can see that we have no clue how to correctly classify items $x_3$ and $x_4$ when the attribute values of those items are identical.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 1: it is impractical to classify x3 and x4&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 1: it is impractical to classify x3 and x4
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;ii-primary-concepts-of-rs&quot;&gt;II. Primary concepts of RS&lt;/h4&gt;
&lt;p&gt;RS theory defines a number of concepts:&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/headache-muscle-temp-dataset.png&quot; alt=&quot;Figure 2: Sample dataset&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 2: Sample dataset
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;information-system-i--u-a&quot;&gt;1) Information system: $I = (U, A)$&lt;/h4&gt;
&lt;p&gt;Information system can be understood as a body of information/knowledge that we have. Mathematically the information system is denoted as a pair $(U, A)$ where:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$U = {x_1, x_2, …, x_n}$: universally represents a non-empty $(U \neq \emptyset)$, finite set of observations $x_i$.&lt;/li&gt;
  &lt;li&gt;$A = {a_1, a_2, …, a_n}$: represent a non-empty $(A \neq \emptyset)$, finite set of attributes $a_j$ such that $a \in A$, $a: U \rightarrow V_a$, where $V_a$ is the value set of $a$ (which means every observation in $U$ can be set to attributes that belong to $V_a$).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In ML, we usually call $x_i$ observations, objects, rows; and $a_j$ predictors, features, attributes.&lt;/p&gt;

&lt;h4 id=&quot;decision-system-ds--u-a-cup-d--ds-subseteq-i&quot;&gt;2) Decision system: $DS = (U, A \cup {d})$ | $DS \subseteq I$&lt;/h4&gt;
&lt;p&gt;Decision system $DS$ is a special type of information system $I$ that can be used in classification. $d$ is the &lt;em&gt;decision&lt;/em&gt; values (i.e., Yes/No, 1/0, and so on). In ML, we call $d_k \in {d}$ response, or outcome.&lt;/p&gt;

&lt;h4 id=&quot;indiscernibility-relation&quot;&gt;3) Indiscernibility relation&lt;/h4&gt;
&lt;p&gt;Given an information system $I = (U, A)$, for any subset $B \subseteq A$, the indiscernibility or equivalence relation is defined by:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{(x, y) \in U^2 | \forall a \in B, a(x) = a(y)\}&lt;/script&gt;
where $IND(B)$ is called the &lt;strong&gt;B-indiscernibility relation&lt;/strong&gt;. The equivalence classes of $IND(B)$ are denoted as $[x]_B$.&lt;/p&gt;

&lt;p&gt;Take an example from Figure 2. Suppose we have $B= { Headache, Musclepain }$. Thus, we can have:
&lt;script type=&quot;math/tex&quot;&gt;IND(B) = \{\{e1, e2, e3\}, \{e4, e6\}, \{e5\}\}&lt;/script&gt;
We can also say that $e1, e2, e3$ are indiscernible from each other.&lt;/p&gt;

&lt;h4 id=&quot;approximation&quot;&gt;4) Approximation&lt;/h4&gt;
&lt;p&gt;The concept of approximation regards the problem of &lt;strong&gt;data reduction&lt;/strong&gt; in ML. For that, we can define a approximation set $X (X \subseteq U)$ by using the information from the set $B (B \subseteq A)$. To do that, we have to construct $B-lower$ $(\underline{B}X)$ and $B-upper$ $(\overline{B}X)$ approximations of $X$:
&lt;script type=&quot;math/tex&quot;&gt;\underline{B}X = \{x | [x]_B \subseteq X\}&lt;/script&gt;
&lt;script type=&quot;math/tex&quot;&gt;\overline{B}X = \{x | [x]_B \cap X \neq \emptyset\}&lt;/script&gt;
Whereas &lt;em&gt;lower-approximation&lt;/em&gt; consists of all observations which &lt;strong&gt;certainly&lt;/strong&gt; belong to the class, &lt;em&gt;upper-approximation&lt;/em&gt; contains all observations which &lt;strong&gt;likely&lt;/strong&gt; belong to the class.&lt;/p&gt;

&lt;p&gt;From that we have $B-boundary$:
&lt;script type=&quot;math/tex&quot;&gt;BN_B(X) = \overline{B}X - \underline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;and $B-outside$:
&lt;script type=&quot;math/tex&quot;&gt;BO_B = U - \overline{B}X&lt;/script&gt;&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/vagueness-o-data.png&quot; alt=&quot;Figure 3&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Figure 3
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;For example, in Figure 3, let
&lt;script type=&quot;math/tex&quot;&gt;W = \{x | Walk(x) = yes\}&lt;/script&gt;
we can have:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;$\underline{B}W = {x1, x6}$&lt;/li&gt;
  &lt;li&gt;$\overline{B}W = {x1, x3, x4, x6}$&lt;/li&gt;
  &lt;li&gt;$BN_B(W) = {x3, x4}$&lt;/li&gt;
  &lt;li&gt;$BO_B = {x2, x5, x7}$&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Tue, 06 Sep 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/09/06/classification-using-roughset-theory/</guid>
      </item>
    
      <item>
        <title>Precision, Recall, Sensitivity and Specificity</title>
        <description>&lt;h4 id=&quot;introduction&quot;&gt;Introduction&lt;/h4&gt;
&lt;p&gt;I recently discovered that the concepts of precision, recall, sensitivity and specificity are very much important to what I am currently working on. Those are equally important but not for a single domain. Instead we have different indicators for different domains. For example, in information retrieval, precision and recall are two important measures that need to be taken into account when evaluate a search system. In pattern recognition and machine learning, precision and recall are important as well but there is a slight difference when interpret them. On the other hand, sensitivity and specificity are alternations to precision and recall when it comes to medical application. In this writing, we aim to explain why there exist those differences in the first place.&lt;/p&gt;

&lt;p&gt;In order to begin, first we look at &lt;strong&gt;the confusion matrix&lt;/strong&gt; to understand where terms TP, FP, TN, and FN come from, and second, look at the Venn digram that perfectly portraits the territories of TP, FP, TN, and FN within the search space.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/confusion-matrix-noted.jpg&quot; alt=&quot;The confusion matrix of the search space&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The confusion matrix of the search space
  &lt;/p&gt;
&lt;/div&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/precision-recall.jpg&quot; alt=&quot;The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    The Venn diagram of the confusion matrix (I guess it better applies to information retrieval)
  &lt;/p&gt;
&lt;/div&gt;

&lt;p&gt;And mathematically, precision, recall, sensitivity and specificity are denoted as below.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;Precision = \frac{TP}{TP + FP} \\
\\
Recall = Sensitivity = \frac{TP}{TP + FN} \\
\\
Specificity = \frac{TN}{TN + FP}&lt;/script&gt;

&lt;h4 id=&quot;precision&quot;&gt;Precision&lt;/h4&gt;
&lt;p&gt;I found that we can easily understand the intuitive concept of precision by simply looking at the Venn diagram. Simply speaking, precision is the ratio between the documents that match the user expectation and the total number of documents returned by the system. One cay say that the higher the precision, the better. But it is always true because a system can &lt;em&gt;cheat&lt;/em&gt; the precision score up to 100% by only returning documents about which it is extremely confident. By doing this, the amount of returned documents is severely reduced, making it so resistant to FP.Look at the figure below, because the space of returned documents is so small, it can fall within the space of the expected documents easily. What is the problem with that? Because the amount of search result is extremely small, the information is likely inadequate to support the user tasks. Mathematically, we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt;, thus &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-precision-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;recall-and-sensitivity&quot;&gt;Recall and Sensitivity&lt;/h4&gt;
&lt;p&gt;We can also grasp the concept of recall and sensitivity smoothly through the Venn diagram observation. Both recall and sensitivity can be understand as the ratio between the documents that match the user expectation and the total number of expected documents from the user. Intuitively speaking, recall and sensitivity are big indicators for how much good information that a system really misses, whereas precision is an indicator for how much good information in the search result the user can use. We can also cheat the recall/sensitivity score by getting the system to add up the number of returned documents, making the search result overwhelmingly abundant. Then guess who is going have a hard time to filtrate such load of information. Look at the figure below, we can see that by returning an overwhelming amount of information, the system can ensure that its recall/sensitivity is maximized. Mathematically, by doing this we have &lt;script type=&quot;math/tex&quot;&gt;TP \rightarrow 1&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;FN \rightarrow 0&lt;/script&gt;, therefore &lt;script type=&quot;math/tex&quot;&gt;\frac{TP}{TP + FN} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;div class=&quot;wp-caption aligncenter&quot;&gt;
  &lt;img src=&quot;/images/Venn-diagram-recall-optimized.jpg&quot; alt=&quot;We can cheat the precision result&quot; /&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    We can cheat the precision result
  &lt;/p&gt;
&lt;/div&gt;

&lt;h4 id=&quot;specificity&quot;&gt;Specificity&lt;/h4&gt;
&lt;p&gt;To understand the concept of specificity, as oppose to observing the Venn diagram, I found it much more easier to understand by checking the confusion matrix. First of all, one has to understand that by using specificity, he wants to steer the focus of his evaluation towards how good the system can predict negative outcomes( or how good the system can avoid returning irrelevant documents,. That’s the reason why TN is the numerator of the equation in the first place. One can also maximize the specificity score of the system by getting the system return no information at all (or predict all observations are negative). Mathematically, when return no information or predict all are negative, &lt;script type=&quot;math/tex&quot;&gt;FP \rightarrow 0&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;TN \rightarrow 1&lt;/script&gt;, then &lt;script type=&quot;math/tex&quot;&gt;\frac{FP}{TN + FP} \rightarrow 1&lt;/script&gt;.&lt;/p&gt;

&lt;h4 id=&quot;which-one-i-should-use&quot;&gt;Which one I should use?&lt;/h4&gt;
&lt;p&gt;We have four metrics to choose, so the question is which one I should. Well I can say that it depends on the problem domain. For example, if we have built a search system, in order to evaluate it, firstly we have to answer some questions: Which criteria we care about most? Do we care about the vast amount of information we ignore (which is TN)? Or do we care about the information we can return (FP + TP + FN)? My answer is I care about the relevant of information my system gives the user rather than the information I can’t return (there is countless amount of information, who cares in the first place). Thus, to evaluate the search system, I would use precision, recall and ignore specificity.&lt;/p&gt;

&lt;p&gt;In contrast, suppose we have  a diabetes predictive model to evaluate, which metrics should we care about? Do we care if a patient doesn’t have diabetes, but the system predicts positive (TN + FP)? Do we care about if a patient has diabetes, but the system predicts negative (TP + FP)? and so on. My answer is I would use all of three indicators (e.g., precision, recall, specificity) because I guess we have to take into account all of indicators TP, FP, TN and FN. Everything is crucial in this case.&lt;/p&gt;

&lt;p&gt;Lastly and interestingly, imagine if we had a system that can exploit all evidences and judge if a suspect guilty. In this case, what we care about most? Do we care if the suspect doesn’t commit the crime but the system judges him guilty (FP)? Or do we care if the suspect does commit the crime but the system judges him not guilty (FN)? Yeah, I guess in this case, we should put more emphasis on FP rather than FN, because putting an innocent person to the jail is also a crime, and letting a guilty suspect go is not as dire as putting innocent person to jail.&lt;/p&gt;

&lt;p&gt;** In statistic and relevant fields, usually we put more emphasis on Type I error (e.g., reject &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; incorrectly (FP)), rather than Type 2 error (e.g., retain a false &lt;script type=&quot;math/tex&quot;&gt;H_0&lt;/script&gt; (FN)).&lt;/p&gt;

&lt;p&gt;The end.&lt;/p&gt;
</description>
        <pubDate>Tue, 30 Aug 2016 00:00:00 +1000</pubDate>
        <link>http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</link>
        <guid isPermaLink="true">http://127.0.0.1:4000/2016/08/30/precision-recall-sensitivity-specificity/</guid>
      </item>
    
  </channel>
</rss>

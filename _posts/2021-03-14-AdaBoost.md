---
title: "AdaBoost"
author: "Ngoc Tran Trung"
date: "14 March 2021"
comments: yes
layout: post
---

## 1. Introduction

Adaptive Boosting (AdaBoost) is a boosting algorithm proposed by Yoav Freund and Robert Schapire. 

## 2. Algorithm

__Input:__ a training dataset ${(x_i, y_i)}_{i=1}^n$.

__Step 1:__ Initialize weights $w_{1,1}, ..., w_{n,1}$ with a distinct value $\frac{1}{n}$.

__Step 2:__ For $m=1$ to $M$ (maximum number of trees):

__(A)__ Make a stump $h_m$ from a feature that can yield the least __total error__ $\epsilon_m$ where:

\begin{equation}
\epsilon_m = \sum_{i=1}^n w_{i,m} \tag{where $h_m(x_i) \neq y_i$}
\end{equation}

__(B)__ Calculate __the influence of this stump__ $h_m$ on the final model:

\begin{equation}
\alpha_m = \frac{1}{2} log(\frac{1-\epsilon_t}{\epsilon_t})
\end{equation}

__(C)__ Update weights for samples depending on if they were correctly or incorrectly classified by $h_m$:

\begin{equation}
w_{incorrect} = w * e^{\alpha_m}
\end{equation}

\begin{equation}
w_{correct} = w * e^{-\alpha_m}
\end{equation}

__(D)__ Normalize the sample weights such that $\sum w_{i, t+1} = 1$.

__Output:__ The final model:

\begin{equation}
F(x) = \sum_{m=1}^M \alpha_m h_m(x)
\end{equation}

## 3. Algorithm Exposition

### 3.1. Step 1

At the beginning of the algorim, we have to initialize __an equal amount of weights__ $\frac{1}{n}$ for all training samples. 

Later, we will adjust weights such that incorrectly classified samples will receive a higher weight while correctly classified samples will receive a lower weights.

### 3.2. Step 2

#### (A)

In this step, for each feature in the training set, we make a stump. We will choose a stump that yield the least total error. The calculation of __the total error__ $\epsilon_m$ is straightforward: It is just the sum of weights of all __incorrectly classified__ training samples.

#### (B)

The total error $\epsilon_m$ is used to compute the "amount of say", *i.e.* how much the current stump $h_m$ influence the final model.

The calculate of $\alpha_m$ is also straightforward.

#### (C)

We adjust the weight for each training sample depending on how $h_m$ classified. 

As said above, we update weights such that we put __a greater emphasis on incorrectly classified samples__ and __a lesser on correctly classified samples__ .

#### (D)

After updating weights, we have to normalize all weights such that the sum of all weights equals to 1.

At the end of (E) step, we repeat Step 2 again for $h_{m+1}$.

#### Output

So how the final model $F(x)$ will output? Specifically, the final model takes into account the prediction of each stump as well as its amount of say. For example, suppose there are $12$ stumps that predict YES with the total amount of say $34$, while there are $15$ stumps that predict NO with the total amount of say $21$. The output of the final model will be YES.

